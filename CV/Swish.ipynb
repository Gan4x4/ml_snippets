{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO69qspDGDbsfIYc+L3Z5qb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"yNV64yfj0UQr"},"source":["### Swish"]},{"cell_type":"markdown","metadata":{"id":"EydD2gb-0UQr"},"source":["Эксперименты исследователей из Google показали, что функция активации [**Swish**](https://arxiv.org/pdf/1710.05941v1.pdf) улучшает точность глубоких моделей. Простая замена **ReLU** на **Swish** дала прирост 0.9% точности для **Mobile NASNet-A** (на ImageNet challenge).\n","\n"," Посмотрим, что из себя представляет активация **Swish**:\n"," $$f(x) = x*\\sigma(\\beta x)$$\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"cG4-WOvK0UQs"},"source":["Мы видим сигмоиду и некий параметр $ \\beta $, который может быть определен как константа или как обучаемый параметр. \n","\n","* Если $\\beta = 0$, **Swish** становится линейной функцией $f(x) = \\frac{x}{2} $\n","\n","* Если $\\beta\\to \\infty$ сигмоидная составляющая приближается к пороговой функции, поэтому **Swish** становится похожим на функцию **ReLU**. \n","\n","То есть, нелинейность функции **Swish** может контролироваться моделью, если $\\beta$ задан в качестве обучаемого параметра.\n","\n","Главное отличие **Swish** от **ReLU** — это ее немонотонность."]},{"cell_type":"markdown","metadata":{"id":"6TtfT7P10UQs"},"source":["Ниже на графике представлено распределение значений, которые принимает $ \\beta $ (являясь обучаемым параметром) при обучении **Mobile NASNet-A**. Видно, что значения распределены от 0 до 1.5, также виден резкий пик при значении = 1.  "]},{"cell_type":"markdown","metadata":{"id":"oFdrCA_L0UQs"},"source":["\n","<center><img src =\"https://edunet.kea.su/repo/EduNet-content/L09/out/swish_b_parameter.png\"  width=\"500\"></center"]},{"cell_type":"markdown","metadata":{"id":"3LsFPIL70UQt"},"source":["На практике, чтобы не увеличивать количество обучаемых параметров, используют **Swish** со значением $\\beta = 1$ . В PyTorch такая реализация вызывается `nn.SiLU()` (**Si**gmoid **L**inear **U**nit) "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wWzu1OnS0UQt"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","import torch\n","from torch import nn\n","\n","silu = nn.SiLU()\n","array = np.arange(-5, 5, 0.01)\n","activated = silu(torch.Tensor(array))\n","\n","plt.figure(figsize=(8, 4), dpi=100)\n","plt.plot(array, activated, label=\"$f(x)=x*\\sigma(x)$\")\n","plt.legend()\n","plt.grid()\n","plt.title(\"SiLU\")\n","ax = plt.gca()\n","ax.spines[\"top\"].set_color(\"none\")\n","ax.spines[\"bottom\"].set_position(\"zero\")\n","ax.spines[\"left\"].set_position(\"zero\")\n","ax.spines[\"right\"].set_color(\"none\")\n","plt.ylim(bottom=-2)\n","plt.axis()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sEtQJ47p0UQu"},"outputs":[],"source":["from torchvision import models\n","\n","mobilenet = models.mobilenet_v3_small(weights=None)\n","print(mobilenet)"]}]}