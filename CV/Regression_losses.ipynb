{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPo92k3B0bfsAx5rpeKMyow"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"q_2vsyYY0Wqc"},"source":["### Функции потерь для задач регрессии"]},{"cell_type":"markdown","metadata":{"id":"Jz0PFQw00Wqc"},"source":["**MAE vs MSE**\n","\n"," <img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/regression_loss.png\" width=\"700\">\n","\n","<center><p><em>Source: <a href=\"https://heartbeat.comet.ml/5-regression-loss-functions-all-machine-learners-should-know-4fb140e9d4b0\">5 Regression Loss Functions All Machine Learners Should Know</a></p> </em></center>"]},{"cell_type":"markdown","metadata":{"id":"upaWspzR0Wqc"},"source":["$\\displaystyle\\mathrm{MSE} = \\frac{\\sum^n_{i=1}\\left(y_i-y_i^p\\right)^2}{n}$ — L2/ MSE/ Mean Squared Error/ Среднеквадратичная ошибка\n","\n","\n","$\\displaystyle\\mathrm{MAE} = \\frac{\\sum^n_{i=1}\\left|y_i-y_i^p\\right|}{n}$ — L1/ MAE/ Mean Absolute Error/ Средняя ошибка"]},{"cell_type":"markdown","metadata":{"id":"mKC09grD0Wqc"},"source":["**Huber vs Log-cos**\n","\n","<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/mae_vs_mse.png\" width=\"700\">\n","\n","<center><p><em>Source: <a href=\"https://heartbeat.fritz.ai/5-regression-loss-functions-all-machine-learners-should-know-4fb140e9d4b0\"> 5 Regression Loss Functions All Machine Learners Should Know</a></p> </em></center>"]},{"cell_type":"markdown","metadata":{"id":"Q6f69d_S0Wqd"},"source":[" $\\displaystyle\\\n","    L_{\\delta}(y, f(x))=\\left\\{\n","                \\begin{array}{ll}\n","                  \\frac{1}{2}\\left(y-f\\left(x\\right)\\right)^2 \\qquad &\\mathrm{for}\\  |y-f(x)| \\leq \\delta\\\\\n","                  \\delta|y-f(x)|-\\frac{1}{2}\\delta^2 \\qquad &\\mathrm{otherwise}\n","                \\end{array}\n","              \\right.\n","  $ — Huber Loss/ Smooth Mean ABsolute Error/ Функция потерь Хьюбера\n","\n","\n","$\\displaystyle L\\left(y,y^p\\right)=\\sum_{i=1}^n log\\left(cosh\\left(y^p_i-y_i\\right)\\right)$ — Log-Cosh Loss/ Логарифм гиперболического косинуса"]}]}