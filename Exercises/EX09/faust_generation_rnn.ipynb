{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSl9c0icP2Mv"
      },
      "source": [
        "# –ó–∞–¥–∞–Ω–∏–µ 4*. –ü–æ—Å–∏–º–≤–æ–ª—å–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ (Solution)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbAknE3NP2Mv"
      },
      "source": [
        "–í–æ–∑—å–º–∏—Ç–µ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ –ì–µ—Ç–µ \"–§–∞—É—Å—Ç\" –∏ –æ–±—É—á–∏—Ç–µ –Ω–∞ –Ω–µ–º LSTM-–º–æ–¥–µ–ª—å –¥–ª—è –ø–æ—Å–∏–º–≤–æ–ª—å–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞. –í–º–µ—Å—Ç–æ one-hot –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ `nn.Embedding` [üõ†Ô∏è[doc]](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html). –ü—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –∏–≥–Ω–æ—Ä–∏—Ä—É–π—Ç–µ –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è –∏ –Ω–æ–º–µ—Ä–∞ —Å—Ç—Ä–∞–Ω–∏—Ü.\n",
        "\n",
        "[[doc] üõ†Ô∏è Word Embeddings Tutorial](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jG5RK1I-P2Mv"
      },
      "source": [
        "–ò–º–ø–æ—Ä—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v7pUuvggP2Mv"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0BGQ9E4P2Mv"
      },
      "source": [
        "–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rAmD_D8qP2Mv"
      },
      "outputs": [],
      "source": [
        "!wget -q https://edunet.kea.su/repo/EduNet-web_dependencies/datasets/Faust.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VKCnJqAeP2Mv"
      },
      "outputs": [],
      "source": [
        "with open(\"Faust.txt\") as text_file:\n",
        "    faust_text = \"\".join(text_file.readlines())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CDvW0HC4P2Mv"
      },
      "outputs": [],
      "source": [
        "# Your code here\n",
        "class FaustDataset(Dataset):\n",
        "    def __init__(self, data, seq_len=256, chars=\"–∞–±–≤–≥–¥–µ—ë–∂–∑–∏–π–∫–ª–º–Ω–æ–ø—Ä—Å—Ç—É—Ñ—Ö—Ü—á—à—â—ä—ã—å—ç—é—è \\n\"):\n",
        "        super().__init__()\n",
        "\n",
        "        self.seq_len = seq_len\n",
        "        # Creating a dictionary that maps integers to the characters\n",
        "        chars = list(chars)\n",
        "        self.int2char = dict(enumerate(chars))\n",
        "        # Creating another dictionary that maps characters to integers\n",
        "        self.char2int = {char: ind for ind, char in self.int2char.items()}\n",
        "\n",
        "        # encode data\n",
        "        self.data = self.encode(data)\n",
        "\n",
        "    def encode(self, char_string):\n",
        "        return [\n",
        "            self.char2int[character]\n",
        "            for character in char_string\n",
        "            if character in self.char2int\n",
        "        ]\n",
        "\n",
        "    def decode(self, int_string):\n",
        "        return [\n",
        "            self.int2chars[integer]\n",
        "            for integer in int_string\n",
        "            if integer in self.int2chars\n",
        "        ]\n",
        "\n",
        "    def __getitem__(self, n):\n",
        "        patch = self.data[n : n + self.seq_len]\n",
        "        x = torch.LongTensor(patch[:-1])\n",
        "        y = torch.LongTensor(patch[1:])\n",
        "        return x, y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data) - self.seq_len - 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PSIZx0pDP2Mv"
      },
      "outputs": [],
      "source": [
        "train_set = FaustDataset(faust_text)\n",
        "train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
        "dict_size = len(train_set.int2char)\n",
        "print(dict_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kftnJcbXP2Mw"
      },
      "outputs": [],
      "source": [
        "class TextRNN(nn.Module):\n",
        "    def __init__(self, dict_size, hidden_size, embedding_size, n_layers):\n",
        "        super(TextRNN, self).__init__()\n",
        "\n",
        "        self.dict_size = dict_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding_size = embedding_size\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.encoder = nn.Embedding(self.dict_size, self.embedding_size)\n",
        "        self.lstm = nn.LSTM(\n",
        "            self.embedding_size, self.hidden_size, self.n_layers, batch_first=True\n",
        "        )\n",
        "        # self.dropout = nn.Dropout(0.2) # can work without\n",
        "        self.fc = nn.Linear(self.hidden_size, self.dict_size)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        x = self.encoder(x)\n",
        "        out, (ht1, ct1) = self.lstm(x, hidden)\n",
        "        out = out.contiguous().view(-1, self.hidden_size)\n",
        "        # out = self.dropout(out) # can work without\n",
        "        x = self.fc(out)\n",
        "        return x, (ht1, ct1)\n",
        "\n",
        "    def init_hidden(self, batch_size=1):\n",
        "        return (\n",
        "            torch.zeros(\n",
        "                self.n_layers, batch_size, self.hidden_size, requires_grad=True\n",
        "            ).to(device),\n",
        "            torch.zeros(\n",
        "                self.n_layers, batch_size, self.hidden_size, requires_grad=True\n",
        "            ).to(device),\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TgkrUiSEP2Mw"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, char2int, int2char, start_text=\"\\n\", prediction_len=256):\n",
        "    hidden = model.init_hidden()\n",
        "    idx_input = [char2int[char] for char in start_text]\n",
        "    train = torch.LongTensor(idx_input).view(-1, 1).to(device)\n",
        "    predicted_text = start_text\n",
        "\n",
        "    _, hidden = model(train, hidden)\n",
        "\n",
        "    inp = train[-1].view(-1, 1)\n",
        "\n",
        "    for i in range(prediction_len):\n",
        "        output, hidden = model(inp.to(device), hidden)\n",
        "        output_logits = output.cpu().data.view(-1)\n",
        "        p_next = F.softmax(output_logits * 3, dim=-1).detach().cpu().data.numpy()\n",
        "        top_index = np.random.choice(len(char2int), p=p_next)\n",
        "        inp = torch.LongTensor([top_index]).view(-1, 1).to(device)\n",
        "        predicted_char = int2char[top_index]\n",
        "        predicted_text += predicted_char\n",
        "\n",
        "    return predicted_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E4lyMC4AP2Mw"
      },
      "outputs": [],
      "source": [
        "model = TextRNN(\n",
        "    dict_size=len(train_set.int2char), hidden_size=128, embedding_size=32, n_layers=3\n",
        ")\n",
        "model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
        "\n",
        "\n",
        "num_epochs = 1\n",
        "loss_avg = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    loss_list = []\n",
        "    i = 0\n",
        "    for train, target in tqdm(train_loader):\n",
        "        model.train()\n",
        "        hidden = model.init_hidden(train.shape[0])\n",
        "        train, target = train.to(device), target.to(device)\n",
        "\n",
        "        output, hidden = model(train, hidden)\n",
        "        loss = criterion(output, target.view(-1))\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        loss_list.append(loss.item())\n",
        "\n",
        "        i += 1\n",
        "        if i % 500 == 0:\n",
        "            model.eval()\n",
        "            mean_loss = np.mean(loss_list)\n",
        "            print(f\"Epoch: {epoch + 1}, batch: {i}, Loss: {mean_loss}\")\n",
        "            loss_list = []\n",
        "            print(evaluate(model, train_set.char2int, train_set.int2char))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "722EGVERP2Mw"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "print(evaluate(model, train_set.char2int, train_set.int2char))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hj4b34W9P2Mw"
      },
      "source": [
        "## –§–æ—Ä–º–∞—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X33MJtXMP2Mw"
      },
      "source": [
        "–°–≥–µ–Ω–µ—Ä–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç\n",
        "\n",
        "–ü—Ä–∏–º–µ—Ä —Ç–µ–∫—Å—Ç–∞:\n",
        "\n",
        "\"–≤—Å–µ –≤—Å–µ –æ—Ç –±–µ—Å—Å—Ç—ã–¥–Ω—ã–µ —Å—Ç–∞—Ä–æ–π\n",
        "\n",
        "–≤—Å–µ –≤ –Ω–µ–º –ø–æ–ª—É—á—à–µ –≤—Å–µ —Å—Ç—Ä–µ–º–ª–µ–Ω—å—è\n",
        "\n",
        "–ø–æ–¥–¥–µ—Ä–∂–∫–∏ —Å —Å–æ–±–æ–π –≤ —Å–µ—Ä–¥—Ü–µ –≤–æ–∑–¥—É—Ö —Å–≤–æ–µ–π\n",
        "\n",
        "–∏ –≤ –≤–µ—á–Ω–æ–π —Å—Ç—Ä–∞—Å—Ç–∏ –≤–æ—Å—Å—Ç–∞–Ω–µ—Ç —Å–≤–æ–π –ø—Ä–µ–¥–ª–æ–≥\n",
        "\n",
        "–ø—Ä–∏–≤–µ—Ç –≤–∞–º —Å–ª—É–≥–∞ –≤ —Å–ª–∞–¥–∫–æ–º —Å—Ç—Ä–∞—à–Ω–µ–π —Å—Ç—Ä–∞–Ω–µ\n",
        "\n",
        "–∏ –≤ –º–∏—Ä–µ –≤—Å–µ –≤—Ä–∞–∂–¥–∞ —Å—Ç–∞–Ω–µ—Ç —Å—Ç–∞–Ω–µ—Ç\n",
        "\n",
        "–≤ –ø–æ–ª–µ –Ω–∞ –ø–æ–ª—å–∑—É —Å–≤–æ–∏–º –≤–æ—Å–ø–æ–º–∏–Ω–∞–Ω—å—è\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4qgTTTgP2Mw"
      },
      "source": [
        "## –ü–∞–º—è—Ç–∫–∞ –¥–ª—è –ø—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª—è"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWNM0EmlP2Mw"
      },
      "source": [
        "1. –£–∫–∞–∂–∏—Ç–µ, —á—Ç–æ –Ω—É–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å 2 –≤—ã–≤–æ–¥–∞. –û–¥–∏–Ω –ø—Ä–∏ —Ä–∞–∑–Ω—ã—Ö —Å—Ç–∞—Ä—Ç–æ–≤—ã—Ö –±—É–∫–≤–∞—Ö, –¥—Ä—É–≥–æ–π ‚Äî —Ä–∞–∑–Ω—ã–µ —Ñ–∞–º–∏–ª–∏–∏ –Ω–∞ –æ–¥–Ω—É –∏ —Ç—É –∂–µ –±—É–∫–≤—É.\n",
        "2. –ü—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ `random.choise` —Å –≤–µ—Å–∞–º–∏-–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—è–º–∏ –º–æ–≥—É—Ç –≤–æ–∑–Ω–∏–∫–∞—Ç—å –æ—à–∏–±–∫–∏ –æ–∫—Ä—É–≥–ª–µ–Ω–∏—è —ç—Ç–∏—Ö –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π. –ï—Å–ª–∏ —Ç–∞–∫–æ–µ –≤–æ–∑–Ω–∏–∫–∞–µ—Ç, –º–æ–∂–Ω–æ –ø–µ—Ä–µ–π—Ç–∏ –æ—Ç —ç—Ç–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ –∫ –ª—é–±–æ–π –¥—Ä—É–≥–æ–π (–Ω–∞–ø—Ä–∏–º–µ—Ä, –±—Ä–∞—Ç—å —Ç–æ–ø-$k$ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –∏ –≤—ã–±–∏—Ä–∞—Ç—å –∏–∑ –Ω–∏—Ö —Å–ª—É—á–∞–π–Ω—ã–π).\n",
        "3. –ú–æ–∂–Ω–æ –ø—Ä–µ–¥–ª–æ–∂–∏—Ç—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —ç—Ç–æ—Ç –∂–µ —Ç–µ–∫—Å—Ç —Å –ø–æ–º–æ—â—å—é —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pjll5DZEWu8w"
      },
      "source": [
        "# –ó–∞–¥–∞–Ω–∏–µ 4*. –ü–æ—Å–∏–º–≤–æ–ª—å–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ (student version)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05qSZY_LWu8w"
      },
      "source": [
        "–í–æ–∑—å–º–∏—Ç–µ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ –ì–µ—Ç–µ \"–§–∞—É—Å—Ç\" –∏ –æ–±—É—á–∏—Ç–µ –Ω–∞ –Ω–µ–º LSTM-–º–æ–¥–µ–ª—å –¥–ª—è –ø–æ—Å–∏–º–≤–æ–ª—å–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞. –í–º–µ—Å—Ç–æ one-hot –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ `nn.Embedding` [üõ†Ô∏è[doc]](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html). –ü—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –∏–≥–Ω–æ—Ä–∏—Ä—É–π—Ç–µ –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è –∏ –Ω–æ–º–µ—Ä–∞ —Å—Ç—Ä–∞–Ω–∏—Ü.\n",
        "\n",
        "[[doc] üõ†Ô∏è Word Embeddings Tutorial](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zy8pATaUWu8w"
      },
      "source": [
        "–ò–º–ø–æ—Ä—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "thcsUay4Wu8w"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LIg9EJHWu8x"
      },
      "source": [
        "–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dWN2EBE3Wu8x"
      },
      "outputs": [],
      "source": [
        "!wget -q https://edunet.kea.su/repo/EduNet-web_dependencies/datasets/Faust.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rYMV6umkWu8x"
      },
      "outputs": [],
      "source": [
        "with open(\"Faust.txt\") as text_file:\n",
        "    faust_text = \"\".join(text_file.readlines())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OucQCc17Wu8x"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnZO6s9cWu8x"
      },
      "source": [
        "## –§–æ—Ä–º–∞—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTcOwgoXWu8x"
      },
      "source": [
        "–°–≥–µ–Ω–µ—Ä–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç\n",
        "\n",
        "–ü—Ä–∏–º–µ—Ä —Ç–µ–∫—Å—Ç–∞:\n",
        "\n",
        "\"–≤—Å–µ –≤—Å–µ –æ—Ç –±–µ—Å—Å—Ç—ã–¥–Ω—ã–µ —Å—Ç–∞—Ä–æ–π\n",
        "\n",
        "–≤—Å–µ –≤ –Ω–µ–º –ø–æ–ª—É—á—à–µ –≤—Å–µ —Å—Ç—Ä–µ–º–ª–µ–Ω—å—è\n",
        "\n",
        "–ø–æ–¥–¥–µ—Ä–∂–∫–∏ —Å —Å–æ–±–æ–π –≤ —Å–µ—Ä–¥—Ü–µ –≤–æ–∑–¥—É—Ö —Å–≤–æ–µ–π\n",
        "\n",
        "–∏ –≤ –≤–µ—á–Ω–æ–π —Å—Ç—Ä–∞—Å—Ç–∏ –≤–æ—Å—Å—Ç–∞–Ω–µ—Ç —Å–≤–æ–π –ø—Ä–µ–¥–ª–æ–≥\n",
        "\n",
        "–ø—Ä–∏–≤–µ—Ç –≤–∞–º —Å–ª—É–≥–∞ –≤ —Å–ª–∞–¥–∫–æ–º —Å—Ç—Ä–∞—à–Ω–µ–π —Å—Ç—Ä–∞–Ω–µ\n",
        "\n",
        "–∏ –≤ –º–∏—Ä–µ –≤—Å–µ –≤—Ä–∞–∂–¥–∞ —Å—Ç–∞–Ω–µ—Ç —Å—Ç–∞–Ω–µ—Ç\n",
        "\n",
        "–≤ –ø–æ–ª–µ –Ω–∞ –ø–æ–ª—å–∑—É —Å–≤–æ–∏–º –≤–æ—Å–ø–æ–º–∏–Ω–∞–Ω—å—è\"\n"
      ]
    }
  ]
}