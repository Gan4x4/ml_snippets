{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Solution"
      ],
      "metadata": {
        "id": "j4JKyDmHwQUh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TXb_iSfwnfh"
      },
      "source": [
        "# Задание 2. SHAP. Изображения: FashionMNIST\n",
        "Дана сверточная нейронная сеть, решающая задачу [FashionMNIST](https://pytorch.org/vision/stable/generated/torchvision.datasets.FashionMNIST.html#torchvision.datasets.FashionMNIST) (Классификация изображений одежды).\n",
        "\n",
        "Визуализируйте результат SHAP для картинок."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WFlvqAPwnfi"
      },
      "source": [
        "## Формат результата"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1Pw1qyXwnfi"
      },
      "source": [
        "Визуализация работы SHAP для FashionMNIST с помощью [shap.image_plot](https://shap.readthedocs.io/en/latest/example_notebooks/api_examples/plots/image.htmll)\n",
        "\n",
        "Пример визуализации:\n",
        "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/Exercises/EX14/result_2_task_ex14.png\" width=\"1000\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wWhjV3twnfj"
      },
      "source": [
        "## Модель"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_nNYYrJwnfk"
      },
      "source": [
        "Импорт библиотек:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t5kAX36swnfk"
      },
      "outputs": [],
      "source": [
        "!pip install -q shap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Za1SAkzvwnfl"
      },
      "outputs": [],
      "source": [
        "import shap\n",
        "from IPython.display import clear_output\n",
        "\n",
        "device = \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkT4fuqZwnfl"
      },
      "source": [
        "Описание модели:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7HmU9-Tywnfm"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(1, 10, kernel_size=5),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Conv2d(10, 20, kernel_size=5),\n",
        "            nn.Dropout(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.LeakyReLU(),\n",
        "        )\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Linear(320, 50),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(50, 10),  # ten classes\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layers(x)\n",
        "        x = x.view(-1, 320)\n",
        "        x = self.fc_layers(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ep_c3ehAwnfn"
      },
      "source": [
        "Функции для обучения модели:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1ig-TM4wnfn"
      },
      "outputs": [],
      "source": [
        "from torch.nn.functional import cross_entropy\n",
        "\n",
        "\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = cross_entropy(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(\n",
        "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
        "                    epoch,\n",
        "                    batch_idx * len(data),\n",
        "                    len(train_loader.dataset),\n",
        "                    100.0 * batch_idx / len(train_loader),\n",
        "                    loss.item(),\n",
        "                )\n",
        "            )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEW8XzKuwnfo"
      },
      "source": [
        "Загрузка данных и разбиение на train и test:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3wW9YyBGwnfo"
      },
      "outputs": [],
      "source": [
        "from torchvision.datasets import FashionMNIST\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "train_ds = FashionMNIST(\"fm\", train=True, download=True, transform=transform)\n",
        "test_ds = FashionMNIST(\"fm\", train=False, download=True, transform=transform)\n",
        "\n",
        "batch_size = 128\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTmGF9ydwnfp"
      },
      "source": [
        "Обучение модели:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OOj2D5Lqwnfp"
      },
      "outputs": [],
      "source": [
        "from torch import optim\n",
        "\n",
        "num_epochs = 2\n",
        "model = Net().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    train(model, device, train_loader, optimizer, epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27p6ncxDwnfq"
      },
      "source": [
        "## Визуализация SHAP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2EeEOI4Wwnfq"
      },
      "source": [
        "Выбираем из тестовых данных по одному элементу каждого класса (всего 10 классов)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wab8Nrvqwnfq"
      },
      "outputs": [],
      "source": [
        "print(\"Classes: \", \", \".join(train_ds.classes))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0N3MJhOxwnfr"
      },
      "source": [
        "Выберем по одному элементу каждого класса"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oVnZo8aswnfr"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "idx = []\n",
        "for class_num in range(len(train_ds.classes)):\n",
        "    matched_idx = np.where(test_ds.targets.numpy() == class_num)\n",
        "    idx.append(matched_idx[0][0])\n",
        "images = test_ds.data[idx]\n",
        "print(images.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b61noeCBwnfs"
      },
      "source": [
        "Убедимся, что картинки действительно из разных классов:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T6dYnYQHwnfs"
      },
      "outputs": [],
      "source": [
        "from torchvision.utils import make_grid\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "grid_img = make_grid(images.unsqueeze(1), nrow=10)\n",
        "plt.imshow(grid_img.permute(1, 2, 0))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VChnVu3Ewnft"
      },
      "source": [
        "Теперь инициализируем [shap.DeepExplainer](https://shap-lrjball.readthedocs.io/en/latest/generated/shap.DeepExplainer.html) так, как мы делали в лекции:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UP45riJEwnft"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Get background for DeepLift https://arxiv.org/abs/1704.02685\n",
        "# 1. generate 1000 random indexes\n",
        "inds = torch.randint(0, train_ds.data.shape[0], size=(1000,))\n",
        "# 2. Using indexes extract 1000 samples from dataset\n",
        "# because original data in uint8, convert it to float\n",
        "# and add channel dimension [1000, 28, 28] -> [1000, 1, 28, 28]\n",
        "background = train_ds.data[inds].float().unsqueeze(1)\n",
        "explainer = shap.DeepExplainer(model, background)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByQaxAHywnfu"
      },
      "source": [
        "Применяем SHAP к выбранным нами 10 случайным изображениям*.\n",
        "\n",
        "*Попиксельный SHAP очень тяжелый!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ckWVjLgLwnfu"
      },
      "outputs": [],
      "source": [
        "from warnings import simplefilter\n",
        "\n",
        "simplefilter(\"ignore\", category=Warning)\n",
        "\n",
        "# convert to float and add channel dimension [10, 28, 28] -> [10, 1, 28, 28]\n",
        "test_images = images.unsqueeze(1).float()\n",
        "shap_values = explainer.shap_values(test_images)  # shap magic here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgyalNHywnfu"
      },
      "source": [
        "Получили значения Шепли для каждого пикселя каждой картинки:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xxc1WAUhwnfv"
      },
      "outputs": [],
      "source": [
        "print(len(shap_values))\n",
        "print(shap_values[0].shape)\n",
        "print(test_images.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHWnPAE-wnfv"
      },
      "source": [
        "Задание: Визуализируйте результат работы SHAP, используя [shap.image_plot](https://shap.readthedocs.io/en/latest/generated/shap.plots.image.html?highlight=image_plot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bFjnLWM5wnfw"
      },
      "outputs": [],
      "source": [
        "# Your code here\n",
        "# shape of image for shap = [batch, 28, 28, 1]\n",
        "\n",
        "shap_list_numpy = []  # list of np.array\n",
        "\n",
        "for v in shap_values:\n",
        "    # because image_plot accept data in form(#samples x width x height x channels)\n",
        "    # move cannel to last dimension [10, 1, 28, 28] -> (10,  28, 28, 1)\n",
        "    shap_list_numpy.append(np.moveaxis(v, (1), (3)))\n",
        "\n",
        "    # Do the same for original images in pytorch format and convert it to numpy\n",
        "    test_numpy = test_images.permute(0, 2, 3, 1).numpy()\n",
        "\n",
        "# End of your code\n",
        "\n",
        "# Generate a matrix of labels for all cell. Because we want labels only in head\n",
        "# rest of table filled with  empty strings\n",
        "labels = np.full((10, 10), \" \" * 20)\n",
        "labels[0] = train_ds.classes\n",
        "\n",
        "# plot the feature attributions\n",
        "shap.image_plot(shap_list_numpy, -test_numpy, labels=labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJx6dTYUwnfw"
      },
      "source": [
        "Какие характерные особенности элементов одежды выделяет SHAP? (можно рассмотреть на примере T-shirt и Trouser).\n",
        "\n",
        "**Напишите вывод:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_honz4Lwnfx"
      },
      "source": [
        "## Памятка для преподавателя\n",
        "\n",
        "Цель задания — показать, что SHAP может работать с изображениями.\n",
        "\n",
        "Частая ошибка: shap_list_numpy для визуализации должен быть листом np.array, иначе SHAP его не съест.\n",
        "\n",
        "В выводе студент должен написать, что SHAP позволил выделить особенности элементов одежды, например, для штанов это пространство между штанинами, для футболки — короткие рукава.\n",
        "\n",
        "За отсутствие вывода баллы снижаются."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Версия для студентов"
      ],
      "metadata": {
        "id": "jjaT-iDlwUAP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pph1Lrx2uiHf"
      },
      "source": [
        "# Задание 2. SHAP. Изображения: FashionMNIST\n",
        "Дана сверточная нейронная сеть, решающая задачу [FashionMNIST](https://pytorch.org/vision/stable/generated/torchvision.datasets.FashionMNIST.html#torchvision.datasets.FashionMNIST) (Классификация изображений одежды).\n",
        "\n",
        "Визуализируйте результат SHAP для картинок."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kX3WPflwuiHf"
      },
      "source": [
        "## Формат результата"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDe1LauIuiHf"
      },
      "source": [
        "Визуализация работы SHAP для FashionMNIST с помощью [shap.image_plot](https://shap.readthedocs.io/en/latest/example_notebooks/api_examples/plots/image.htmll)\n",
        "\n",
        "Пример визуализации:\n",
        "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/Exercises/EX14/result_2_task_ex14.png\" width=\"1000\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzNF40HMuiHf"
      },
      "source": [
        "## Модель"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHQKGe96uiHg"
      },
      "source": [
        "Импорт библиотек:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8AshdP4guiHh"
      },
      "outputs": [],
      "source": [
        "!pip install -q shap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZqJ52FFNuiHh"
      },
      "outputs": [],
      "source": [
        "import shap\n",
        "from IPython.display import clear_output\n",
        "\n",
        "device = \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LP-wCvH0uiHh"
      },
      "source": [
        "Описание модели:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BU6gAaPXuiHi"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(1, 10, kernel_size=5),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Conv2d(10, 20, kernel_size=5),\n",
        "            nn.Dropout(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.LeakyReLU(),\n",
        "        )\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Linear(320, 50),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(50, 10),  # ten classes\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layers(x)\n",
        "        x = x.view(-1, 320)\n",
        "        x = self.fc_layers(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SoSH46n2uiHi"
      },
      "source": [
        "Функции для обучения модели:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_ASWrD3uiHi"
      },
      "outputs": [],
      "source": [
        "from torch.nn.functional import cross_entropy\n",
        "\n",
        "\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = cross_entropy(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(\n",
        "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
        "                    epoch,\n",
        "                    batch_idx * len(data),\n",
        "                    len(train_loader.dataset),\n",
        "                    100.0 * batch_idx / len(train_loader),\n",
        "                    loss.item(),\n",
        "                )\n",
        "            )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlJuqBUIuiHi"
      },
      "source": [
        "Загрузка данных и разбиение на train и test:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c-6w5I_kuiHj"
      },
      "outputs": [],
      "source": [
        "from torchvision.datasets import FashionMNIST\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "train_ds = FashionMNIST(\"fm\", train=True, download=True, transform=transform)\n",
        "test_ds = FashionMNIST(\"fm\", train=False, download=True, transform=transform)\n",
        "\n",
        "batch_size = 128\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-b48uevuiHj"
      },
      "source": [
        "Обучение модели:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7EHfZUBuiHj"
      },
      "outputs": [],
      "source": [
        "from torch import optim\n",
        "\n",
        "num_epochs = 2\n",
        "model = Net().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    train(model, device, train_loader, optimizer, epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wt0uuNzkuiHj"
      },
      "source": [
        "## Визуализация SHAP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEF_HSoTuiHk"
      },
      "source": [
        "Выбираем из тестовых данных по одному элементу каждого класса  (всего 10 классов)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0i2V0St0uiHk"
      },
      "outputs": [],
      "source": [
        "print(\"Classes: \", \", \".join(train_ds.classes))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYO4vxNauiHk"
      },
      "source": [
        "Выберем по одному элементу каждого класса"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZnWjMr_uiHk"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "idx = []\n",
        "for class_num in range(len(train_ds.classes)):\n",
        "    matched_idx = np.where(test_ds.targets.numpy() == class_num)\n",
        "    idx.append(matched_idx[0][0])\n",
        "images = test_ds.data[idx]\n",
        "print(images.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hrwj4kJQuiHl"
      },
      "source": [
        "Убедимся, что картинки действительно из разных классов:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "floTOYmpuiHl"
      },
      "outputs": [],
      "source": [
        "from torchvision.utils import make_grid\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "grid_img = make_grid(images.unsqueeze(1), nrow=10)\n",
        "plt.imshow(grid_img.permute(1, 2, 0))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46w_kHHZuiHl"
      },
      "source": [
        "Теперь инициализируем [shap.DeepExplainer](https://shap-lrjball.readthedocs.io/en/latest/generated/shap.DeepExplainer.html) так, как мы делали в лекции:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xwq8q7aJuiHl"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Get background for DeepLift https://arxiv.org/abs/1704.02685\n",
        "# 1. generate 1000 random indexes\n",
        "inds = torch.randint(0, train_ds.data.shape[0], size=(1000,))\n",
        "# 2. Using indexes extract 1000 samples from dataset\n",
        "# because original data in uint8, convert it to float\n",
        "# and add channel dimension [1000, 28, 28] -> [1000, 1, 28, 28]\n",
        "background = train_ds.data[inds].float().unsqueeze(1)\n",
        "explainer = shap.DeepExplainer(model, background)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhPFf3utuiHm"
      },
      "source": [
        "Применяем SHAP к выбранным нами 10 случайным изображениям*.\n",
        "\n",
        "*Попиксельный SHAP очень тяжелый!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rqefGzzouiHm"
      },
      "outputs": [],
      "source": [
        "from warnings import simplefilter\n",
        "\n",
        "simplefilter(\"ignore\", category=Warning)\n",
        "\n",
        "# convert to float and add channel dimension [10, 28, 28] -> [10, 1, 28, 28]\n",
        "test_images = images.unsqueeze(1).float()\n",
        "shap_values = explainer.shap_values(test_images)  # shap magic here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-llBWI-uiHm"
      },
      "source": [
        "Получили значения Шепли для каждого пикселя каждой картинки:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r0qrAV9NuiHm"
      },
      "outputs": [],
      "source": [
        "print(len(shap_values))\n",
        "print(shap_values[0].shape)\n",
        "print(test_images.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TduJZGxuiHn"
      },
      "source": [
        "Задание: Визуализируйте результат работы SHAP, используя [shap.image_plot](https://shap.readthedocs.io/en/latest/generated/shap.plots.image.html?highlight=image_plot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tcLTSPHFuiHn"
      },
      "outputs": [],
      "source": [
        "# shape of image for shap = [batch, 28, 28, 1]\n",
        "\n",
        "# Your code here\n",
        "shap_list_numpy = ...  # list of np.array\n",
        "test_numpy = ...\n",
        "\n",
        "# End of your code\n",
        "\n",
        "# Generate a matrix of labels for all cell. Because we want labels only in head\n",
        "# rest of table filled with  empty strings\n",
        "labels = np.full((10, 10), \" \" * 20)\n",
        "labels[0] = train_ds.classes\n",
        "\n",
        "# plot the feature attributions\n",
        "shap.image_plot(shap_list_numpy, -test_numpy, labels=labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BeB6zc4tuiHn"
      },
      "source": [
        "Какие характерные особенности элементов одежды выделяет SHAP? (можно рассмотреть на примере T-shirt и Trouser).\n",
        "\n",
        "**Напишите вывод:**"
      ]
    }
  ]
}