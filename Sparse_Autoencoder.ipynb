{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Разреженный автоэнкодер"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "До этого мы говорили о том, что построение эффективного автоэнкодера возможно только в случае, когда размер латентного слоя сильно меньше входных слоев. \n",
    "\n",
    "Что если мы сделаем автоэнкодер с размером латентного пространства больше входной размерности? \n",
    "\n",
    "При такой прямой реализации у нас ничего не получится, энкодер будет просто перемещать входные данные полностью в латентное пространство, без изменений. Наш автоэнкодер выучит, что ему нужно входные данные просто скопировать в латентное пространство, а потом перекопировать на вывод, т.е. он ничего не выучит."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Однако сделать латентный слой «узким местом» сети можно и иначе. Идея такая: мы добавим loss на латентный слой, который будет заключаться в том, что для каждого объекта использовалось ограниченное количество нейронов. \n",
    "\n",
    "Это позволяет модели самой выбирать размер латентного представления и использовать для разных групп объектов латентные представления разного размера, что может быть полезно для части задач и позволит получить более полезное внутреннее представление.\n",
    "\n",
    "Такой автоэнкодер называется **разреженным** (**sparse autoencoder**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/L14/out/sparse_autoencoder.png\" alt=\"alttext\" width = \"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала нам нужно задать понятие активированности нейрона. По логике это должна быть величина от 0 до 1.\n",
    "\n",
    "0 соответствует полное отсутствие активации, 1 соответствует полная активация нейрона. Нам не подходит решение — применить сигмоиду к активациям, так как они могут быть и отрицательны, а сигмоида на отрицательных активациях стремится к нулю. А нам нужно, чтобы 0 соответствовало изначально нулевое значение. \n",
    "\n",
    "Потому можем сделать следующее:\n",
    "\n",
    " 1. Берем латентный слой автоэнкодера и считаем абсолютные значения активаций. \n",
    " 2. Применяем к этим значениям сигмоиду. Теперь 0 соответствует 0.5. Полученные значения гарантированно не меньше 0.5\n",
    " 3. Вычтем из этих значений 0.5 и домножим результат на 2. Теперь они распределены так, как нам нужно. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_01_activation(latent):\n",
    "    activations = (torch.sigmoid(latent.abs()) - 0.5) * 2\n",
    "    return activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зададим то, как для нашего автоэнкодера должен подсчитываться лосс. Он будет состоять из двух частей, первая отвечает за реконструкцию (восстановление) изображения, а вторая штрафует за включение нейронов больше некоторого порога, который мы регулируем с помощью beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_ae_loss_handler(data, recon, latent, beta=0.1, *args, **kwargs):\n",
    "    activations = to_01_activation(latent)\n",
    "    return (\n",
    "        F.binary_cross_entropy(recon, data)\n",
    "        + F.l1_loss(activations, torch.zeros_like(activations)) * beta\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим разреженный автоэнкодер, с размером латентного пространства больше, чем размерность входных данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "latent_dim = 30 * 30\n",
    "\n",
    "learning_rate = 1e-4\n",
    "encoder = Encoder(latent_dim=latent_dim)\n",
    "decoder = Decoder(latent_dim=latent_dim)\n",
    "\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    chain(encoder.parameters(), decoder.parameters()), lr=learning_rate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "for i in range(1, 6):\n",
    "    train(\n",
    "        enc=encoder,\n",
    "        dec=decoder,\n",
    "        optimizer=optimizer,\n",
    "        loader=train_loader,\n",
    "        epoch=i,\n",
    "        single_pass_handler=ae_pass_handler,\n",
    "        loss_handler=partial(\n",
    "            sparse_ae_loss_handler, beta=0.01\n",
    "        ),  # regulize beta parameter\n",
    "        log_interval=450,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = encoder.eval()\n",
    "decoder = decoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_res = run_eval(encoder, decoder, test_noised_loader, ae_pass_handler)\n",
    "plot_digits(run_res[\"real\"][0:9], run_res[\"reconstr\"][0:9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что разреженный автоэнкодер, несмотря на большой размер латентного слоя, все равно эффективно убирает шум из изображений.\n",
    "\n",
    "Посмотрим, как активируются нейроны латентного слоя для каждого класса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_res = run_eval(encoder, decoder, test_loader, ae_pass_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axs = plt.subplots(nrows=2, ncols=5, figsize=(16, 5))\n",
    "activations = to_01_activation(torch.from_numpy(run_res[\"latent\"])).numpy()\n",
    "\n",
    "up_lim = activations.max()\n",
    "for label in range(0, 10):\n",
    "    figure = activations[run_res[\"labels\"] == label].mean(axis=0)\n",
    "    figure = figure.reshape(30, 30)\n",
    "    ax = axs[label % 2, label % 5]\n",
    "    ax.imshow(figure, cmap=\"Greys\", clim=(0, 0.5))\n",
    "    ax.grid(False)\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что нейронов активируются в среднем крайне мало, между классами активирующиеся нейроны отличаются."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "\n",
    "\n",
    "def plot_percent_hist(ax, data, bins):\n",
    "    counts, _ = np.histogram(data, bins=bins)\n",
    "    widths = bins[1:] - bins[:-1]\n",
    "    x = bins[:-1] + widths / 2\n",
    "    ax.bar(x, counts / len(data), width=widths * 0.8)\n",
    "    ax.xaxis.set_ticks(bins)\n",
    "    ax.yaxis.set_major_formatter(\n",
    "        mpl.ticker.FuncFormatter(\n",
    "            lambda y, position: \"{}%\".format(int(np.round(100 * y)))\n",
    "        )\n",
    "    )\n",
    "    ax.grid(True)\n",
    "\n",
    "\n",
    "def plot_activations_histogram(activations, height=1, n_bins=10):\n",
    "    activation_means = activations.mean(axis=0)\n",
    "\n",
    "    mean = activation_means.mean()\n",
    "    bins = np.linspace(0, 1, n_bins + 1)\n",
    "\n",
    "    fig, [ax1, ax2] = plt.subplots(figsize=(10, 3), nrows=1, ncols=2, sharey=True)\n",
    "    plot_percent_hist(ax1, activations.ravel(), bins)\n",
    "    ax1.plot(\n",
    "        [mean, mean], [0, height], \"k--\", label=\"Overall Mean = {:.2f}\".format(mean)\n",
    "    )\n",
    "    ax1.legend(loc=\"upper center\", fontsize=14)\n",
    "    ax1.set_xlabel(\"Activation\")\n",
    "    ax1.set_ylabel(\"% Activations\")\n",
    "    ax1.axis([0, 1, 0, height])\n",
    "    plot_percent_hist(ax2, activation_means, bins)\n",
    "    ax2.plot([mean, mean], [0, height], \"k--\")\n",
    "    ax2.set_xlabel(\"Neuron Mean Activation\")\n",
    "    ax2.set_ylabel(\"% Neurons\")\n",
    "    ax2.axis([0, 1, 0, height])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_activations_histogram(activations, height=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, что у нас наблюдается в целом. \n",
    "Видно, что средний размер активации — в районе 0.10. Причем часто активация равна 0. \n",
    "Для нейронов наблюдается похожая картина: в среднем активация нейрона по всему тестовому датасету расположена в районе 0.10 и нейроны очень редко отходят от этой области. \n",
    "\n",
    "Вообще говоря, нам нужна именно правая картина. Левая — лишь побочный результат применения нами L1-loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подход с L1-loss очень просто реализуется, но в то же время не совсем очевидно, как с помощью него задать условие вида: пусть в среднем на латентном слое активируется 1% нейронов. Понятно, что это косвенно задается величиной коэффициента $\\beta$, но хотелось бы задавать это в явном виде. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Дивергенция Кульбака-Лейблера"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для данной цели используется дивергенция Кульбака-Лейблера, которая считается по формуле: \n",
    "\n",
    "$$KL(P||Q) = \\int_X p(x)\\log \\dfrac {p(x)} {q(x)} dx$$\n",
    "\n",
    "В теории информации $p$ считается целевым (истинным) распределением, а $q$ — тем, с которым мы его сравниваем (проверяемым). \n",
    "Важно понимать, что $KL$ не является мерой расстояния, т.к. в общем случае $KL(P||Q) != KL(Q||P)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[Оказывается](https://math.stackexchange.com/questions/90537/what-is-the-motivation-of-the-kullback-leibler-divergence), в подобных задачах она, как правило, обеспечивает бОльшую сходимость к требуемому распределению, нежели та же L1-регуляризация.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KL-дивергенция для $p=0.1$\n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/L14/out/kl_plot.png\" alt=\"alttext\" width=\"350\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пусть мы хотим, чтобы на каждом слое для данного объекта активировалось в среднем p% нейронов.\n",
    "\n",
    "В нашем случае для каждой активации i-го нейрона латентного слоя $a_i^{latent}$ мы можем решить, активирован нейрон или нет. Мы можем посчитать для каждого объекта, какая доля нейронов активировалась в его случае. \n",
    "\n",
    "Или же мы можем усреднить активации нейронов, если активации распределены на отрезке от 0 до 1 (например, мы можем преобразовать активации, как это сделали выше). Получим величину $\\hat{p}$.\n",
    "\n",
    "Фактически мы сравниваем два бернулиевских распределения: то, которое хотим мы, с параметром p, и то, которое мы наблюдаем — с оценочным параметром $\\hat{p}$. \n",
    "\n",
    "$$KL(P||Q) =  p(x) \\log \\dfrac {p(x)} {\\hat{p}(x)} + (1 - p(x)) \\log \\dfrac {(1 - p(x))} {1 - \\hat{p}(x)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее лишь остается просуммировать данный loss по батчу.\n",
    "\n",
    "Можно делать и иначе — требовать, чтобы каждый нейрон в среднем активировался в p% случаев. В этом случае на первом шаге мы усредняем активации не по всему слою, а по батчам. А вот подсчитанный loss усредняем по всем нейронам слоя.\n",
    "\n",
    "Чем KL-loss лучше L1-loss и L2-loss? \n",
    "Он позволяет нам приближаться к решению более плавно и четко регулировать долю активирующихся нейронов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 7))\n",
    "p = 0.1\n",
    "q = np.linspace(0.001, 0.999, 500)\n",
    "kl_div = p * np.log(p / q) + (1 - p) * np.log((1 - p) / (1 - q))\n",
    "mse = (p - q) ** 2\n",
    "mae = np.abs(p - q)\n",
    "plt.plot([p, p], [0, 0.3], \"k:\")\n",
    "plt.text(0.05, 0.32, \"Target\\nsparsity\", fontsize=14)\n",
    "plt.plot(q, kl_div, \"b-\", label=\"KL divergence\")\n",
    "plt.plot(q, mae, \"g--\", label=r\"MAE ($\\ell_1$)\")\n",
    "plt.plot(q, mse, \"r--\", linewidth=1, label=r\"MSE ($\\ell_2$)\")\n",
    "plt.legend(loc=\"upper left\", fontsize=14)\n",
    "plt.xlabel(\"Actual sparsity\")\n",
    "plt.ylabel(\"Cost\", rotation=0)\n",
    "plt.axis([0, 1, 0, 0.95]);"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
