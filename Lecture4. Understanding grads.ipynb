{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"ToES8TOlYQHX"},"source":["# Grads in pytorch\n","\n"]},{"cell_type":"code","metadata":{"id":"SX2Ou8yMXmNM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1696067440549,"user_tz":-180,"elapsed":242,"user":{"displayName":"Антон Ганичев","userId":"03960392406657956647"}},"outputId":"53e395d2-b28e-4b37-9e12-fd5757d585bc"},"source":["import torch\n","\n","x = torch.tensor([2, 1.2, 0, 4],requires_grad= True)\n","\n","y = x*x\n","z = y.sum()\n","\n","print(\"x =\",x)\n","print(\"x.grad = \",x.grad) # grad is none\n","\n","z.backward()\n","print(\"y: \",y,\"y.grad = \", y.grad) # lead to warning\n","print(\"x: \",x,\"x.grad = \", x.grad)\n"],"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["x = tensor([2.0000, 1.2000, 0.0000, 4.0000], requires_grad=True)\n","x.grad =  None\n","y:  tensor([ 4.0000,  1.4400,  0.0000, 16.0000], grad_fn=<MulBackward0>) y.grad =  None\n","x:  tensor([2.0000, 1.2000, 0.0000, 4.0000], requires_grad=True) x.grad =  tensor([4.0000, 2.4000, 0.0000, 8.0000])\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-30-0114cc2a6f9f>:12: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:486.)\n","  print(\"y: \",y,\"y.grad = \", y.grad) # lead to warning\n"]}]},{"cell_type":"markdown","metadata":{"id":"iTiK8a5fYjk7"},"source":["loss.backward() computes dloss/dx for every parameter x which has requires_grad=True.\n","\n","These are accumulated into x.grad for every parameter x.\n","In pseudo-code:  x.grad += dloss/dx\n","\n","z = loss\n","\n","We want dz/dx\n","\n","dz/dx = dz/dy * dy/dx\n","\n","z = y0 + y1 + y2 + y3\n","\n","dz/dy = [1,1,1,1] dz equal to  loss\n","\n","y = x^2\n","\n","dy/dx = 2x"]},{"cell_type":"code","source":["!pip install torchviz"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z1OxC-0Sb_gS","executionInfo":{"status":"ok","timestamp":1696066453694,"user_tz":-180,"elapsed":9362,"user":{"displayName":"Антон Ганичев","userId":"03960392406657956647"}},"outputId":"7258813e-54f5-42a4-c6dc-657b6c4edb6c"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting torchviz\n","  Downloading torchviz-0.0.2.tar.gz (4.9 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torchviz) (2.0.1+cu118)\n","Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from torchviz) (0.20.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (3.12.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->torchviz) (3.27.4.1)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->torchviz) (16.0.6)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->torchviz) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->torchviz) (1.3.0)\n","Building wheels for collected packages: torchviz\n","  Building wheel for torchviz (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for torchviz: filename=torchviz-0.0.2-py3-none-any.whl size=4130 sha256=6721a20ff6abea7c1fb797fceca62498b096e23c77511cac5d849a0e03ea1dbf\n","  Stored in directory: /root/.cache/pip/wheels/4c/97/88/a02973217949e0db0c9f4346d154085f4725f99c4f15a87094\n","Successfully built torchviz\n","Installing collected packages: torchviz\n","Successfully installed torchviz-0.0.2\n"]}]},{"cell_type":"code","source":["from torchviz import make_dot\n","\n","y = x*x\n","z = y.sum()\n","make_dot(z, params={'x':x, 'y':y, 'z':z}, show_saved=True)#, show_attrs=True, show_saved=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":233},"id":"eifUz6pIcoOZ","executionInfo":{"status":"ok","timestamp":1696067328083,"user_tz":-180,"elapsed":273,"user":{"displayName":"Антон Ганичев","userId":"03960392406657956647"}},"outputId":"c5d227d5-b569-4a3b-835b-8170402e1e19"},"execution_count":27,"outputs":[{"output_type":"execute_result","data":{"image/svg+xml":"<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.43.0 (0)\n -->\n<!-- Title: %3 Pages: 1 -->\n<svg width=\"257pt\" height=\"159pt\"\n viewBox=\"0.00 0.00 257.00 159.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 155)\">\n<title>%3</title>\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-155 253,-155 253,4 -4,4\"/>\n<!-- 136820718706240 -->\n<g id=\"node1\" class=\"node\">\n<title>136820718706240</title>\n<polygon fill=\"#caff70\" stroke=\"black\" points=\"71.5,-30 17.5,-30 17.5,0 71.5,0 71.5,-30\"/>\n<text text-anchor=\"middle\" x=\"44.5\" y=\"-18\" font-family=\"monospace\" font-size=\"10.00\">z</text>\n<text text-anchor=\"middle\" x=\"44.5\" y=\"-7\" font-family=\"monospace\" font-size=\"10.00\"> ()</text>\n</g>\n<!-- 136820718621968 -->\n<g id=\"node2\" class=\"node\">\n<title>136820718621968</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"89,-90.5 0,-90.5 0,-71.5 89,-71.5 89,-90.5\"/>\n<text text-anchor=\"middle\" x=\"44.5\" y=\"-78.5\" font-family=\"monospace\" font-size=\"10.00\">SumBackward0</text>\n</g>\n<!-- 136820718621968&#45;&gt;136820718706240 -->\n<g id=\"edge7\" class=\"edge\">\n<title>136820718621968&#45;&gt;136820718706240</title>\n<path fill=\"none\" stroke=\"black\" d=\"M44.5,-71.37C44.5,-63.49 44.5,-51.45 44.5,-40.6\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"48,-40.36 44.5,-30.36 41,-40.36 48,-40.36\"/>\n</g>\n<!-- 136820718620432 -->\n<g id=\"node3\" class=\"node\">\n<title>136820718620432</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"179,-151 90,-151 90,-132 179,-132 179,-151\"/>\n<text text-anchor=\"middle\" x=\"134.5\" y=\"-139\" font-family=\"monospace\" font-size=\"10.00\">MulBackward0</text>\n</g>\n<!-- 136820718620432&#45;&gt;136820718621968 -->\n<g id=\"edge1\" class=\"edge\">\n<title>136820718620432&#45;&gt;136820718621968</title>\n<path fill=\"none\" stroke=\"black\" d=\"M121.22,-131.87C106.86,-122.54 83.74,-107.51 66.63,-96.38\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"68.13,-93.18 57.83,-90.67 64.31,-99.05 68.13,-93.18\"/>\n</g>\n<!-- 136820718590352 -->\n<g id=\"node4\" class=\"node\">\n<title>136820718590352</title>\n<polygon fill=\"lightblue\" stroke=\"black\" points=\"161.5,-96 107.5,-96 107.5,-66 161.5,-66 161.5,-96\"/>\n<text text-anchor=\"middle\" x=\"134.5\" y=\"-84\" font-family=\"monospace\" font-size=\"10.00\">x</text>\n<text text-anchor=\"middle\" x=\"134.5\" y=\"-73\" font-family=\"monospace\" font-size=\"10.00\"> (4)</text>\n</g>\n<!-- 136820718620432&#45;&gt;136820718590352 -->\n<g id=\"edge2\" class=\"edge\">\n<title>136820718620432&#45;&gt;136820718590352</title>\n<path fill=\"none\" stroke=\"black\" d=\"M129.54,-131.87C127.51,-122.46 127.21,-107.26 128.63,-96.11\"/>\n</g>\n<!-- 136820718620432&#45;&gt;136820718590352 -->\n<g id=\"edge3\" class=\"edge\">\n<title>136820718620432&#45;&gt;136820718590352</title>\n<path fill=\"none\" stroke=\"black\" d=\"M139.46,-131.87C141.49,-122.46 141.79,-107.26 140.37,-96.11\"/>\n</g>\n<!-- 136820718619856 -->\n<g id=\"node5\" class=\"node\">\n<title>136820718619856</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"249,-24.5 148,-24.5 148,-5.5 249,-5.5 249,-24.5\"/>\n<text text-anchor=\"middle\" x=\"198.5\" y=\"-12.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n</g>\n<!-- 136820718590352&#45;&gt;136820718619856 -->\n<g id=\"edge5\" class=\"edge\">\n<title>136820718590352&#45;&gt;136820718619856</title>\n<path fill=\"none\" stroke=\"black\" d=\"M148.7,-65.8C158.68,-55.82 172.04,-42.46 182.41,-32.09\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"185.06,-34.39 189.66,-24.84 180.11,-29.44 185.06,-34.39\"/>\n</g>\n<!-- 136820718619856&#45;&gt;136820718620432 -->\n<g id=\"edge4\" class=\"edge\">\n<title>136820718619856&#45;&gt;136820718620432</title>\n<path fill=\"none\" stroke=\"black\" d=\"M191.04,-24.82C183.76,-40.02 174.97,-71.84 161.5,-96 155.95,-105.96 148.13,-115.94 142.02,-123.91\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"139.2,-121.84 136.04,-131.96 144.82,-126.02 139.2,-121.84\"/>\n</g>\n<!-- 136820718619856&#45;&gt;136820718620432 -->\n<g id=\"edge6\" class=\"edge\">\n<title>136820718619856&#45;&gt;136820718620432</title>\n<path fill=\"none\" stroke=\"black\" d=\"M201.91,-24.82C201.76,-40.02 192.97,-71.84 179.5,-96 173.77,-106.29 165.63,-116.58 157.65,-124.68\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"154.76,-122.59 149.81,-131.96 159.52,-127.72 154.76,-122.59\"/>\n</g>\n</g>\n</svg>\n","text/plain":["<graphviz.graphs.Digraph at 0x7c700e54c160>"]},"metadata":{},"execution_count":27}]},{"cell_type":"markdown","metadata":{"id":"V8pIMjYPYuNp"},"source":["## Let's call backward second time\n","\n","it's produce an error\n"]},{"cell_type":"code","metadata":{"id":"MmxoVXnRbEn7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1696067444067,"user_tz":-180,"elapsed":263,"user":{"displayName":"Антон Ганичев","userId":"03960392406657956647"}},"outputId":"c1c2c59d-d0c5-4511-aea0-b8c79742b72f"},"source":["try:\n","  z.backward()\n","except Exception as e:\n","  print(\"ERROR\")\n","  print(e)"],"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["ERROR\n","Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.\n"]}]},{"cell_type":"code","metadata":{"id":"EVLOs_lMe1It","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1635097259301,"user_tz":-180,"elapsed":393,"user":{"displayName":"Антон Ганичев","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiBGUeRQ7Tx1PE7ge5LPPM4do9SVS7xoYoHEjDg=s64","userId":"03960392406657956647"}},"outputId":"f22c43f5-8fe8-4a4e-e241-3ee10546e5c4"},"source":["x = torch.tensor([2,1.2,0,4],requires_grad= True)\n","print (\"X = \",x)\n","\n","y = x*x\n","\n","# Now grads for intermediate tensor y stay in memory\n","y.retain_grad()\n","\n","z = y.sum()\n","print (\"z (loss) =\",z)\n","print (\"y =\",y)\n","print (\"x =\",x)\n","\n","print (\"dz/dx \",x.grad) # grad is None\n","\n","print(\"========== Backprop 1 ==============\")\n","z.backward(retain_graph=True)\n","print (\"dz/dy \",y.grad)\n","print (\"dz/dx \",x.grad)\n","print(\"========== Backprop 2 ==============\")\n","z.backward()\n","\n","# Grads are accumulated\n","print (\"dz/dy \",y.grad)\n","print (\"dz/dx \",x.grad)\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["X =  tensor([2.0000, 1.2000, 0.0000, 4.0000], requires_grad=True)\n","z (loss) = tensor(21.4400, grad_fn=<SumBackward0>)\n","y = tensor([ 4.0000,  1.4400,  0.0000, 16.0000], grad_fn=<MulBackward0>)\n","x = tensor([2.0000, 1.2000, 0.0000, 4.0000], requires_grad=True)\n","dz/dx  None\n","========== Backprop 1 ==============\n","dz/dy  tensor([1., 1., 1., 1.])\n","dz/dx  tensor([4.0000, 2.4000, 0.0000, 8.0000])\n","========== Backprop 2 ==============\n","dz/dy  tensor([2., 2., 2., 2.])\n","dz/dx  tensor([ 8.0000,  4.8000,  0.0000, 16.0000])\n"]}]},{"cell_type":"markdown","metadata":{"id":"sTVHAEYVZJ9J"},"source":["# Optimization in pytorch"]},{"cell_type":"markdown","metadata":{"id":"329jgCSmZPIO"},"source":["## manual\n","\n","https://discuss.pytorch.org/t/leaf-variable-was-used-in-an-inplace-operation/308"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nMtQh8qnZWDx","executionInfo":{"status":"ok","timestamp":1665396606797,"user_tz":-180,"elapsed":275,"user":{"displayName":"Антон Ганичев","userId":"03960392406657956647"}},"outputId":"e55849a7-5c93-44ed-a748-884289750dfa"},"source":["x = torch.tensor([2,1.2,-3,4],requires_grad= True)\n","w = torch.tensor([0.1,-0.2,-0.1,0.4],requires_grad= True)\n","g_true = torch.tensor([1,1,2,3])\n","print(\"X = \",x)\n","print(\"W = \",w)\n","\n","for i in range(100):\n","  y = x*w\n","  loss = torch.sum(torch.abs(y - g_true)) # L1 Loss must be a scalar\n","  loss.backward()\n","  if i ==0 :\n","    print(\"dW/dL\",w.grad) # stay the same\n","  with torch.no_grad():\n","    w =  w - 0.01* w.grad # update w with lr =0.01, note that w recreated here\n","    w.requires_grad = True\n","  if i % 10 == 0:\n","    print(f\"Loss {loss.item()} W {w} y {y}\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["X =  tensor([ 2.0000,  1.2000, -3.0000,  4.0000], requires_grad=True)\n","W =  tensor([ 0.1000, -0.2000, -0.1000,  0.4000], requires_grad=True)\n","dW/dL tensor([-2.0000, -1.2000,  3.0000, -4.0000])\n","Loss 5.139999866485596 W tensor([ 0.1200, -0.1880, -0.1300,  0.4400], requires_grad=True) y tensor([ 0.2000, -0.2400,  0.3000,  1.6000], grad_fn=<MulBackward0>)\n","Loss 2.4159996509552 W tensor([ 0.3200, -0.0680, -0.4300,  0.7600], requires_grad=True) y tensor([ 0.6000, -0.0960,  1.2000,  2.8800], grad_fn=<MulBackward0>)\n","Loss 1.1520003080368042 W tensor([ 0.4800,  0.0520, -0.6700,  0.7600], requires_grad=True) y tensor([1.0000, 0.0480, 1.9200, 2.8800], grad_fn=<MulBackward0>)\n","Loss 1.008000373840332 W tensor([ 0.4800,  0.1720, -0.6700,  0.7600], requires_grad=True) y tensor([1.0000, 0.1920, 1.9200, 2.8800], grad_fn=<MulBackward0>)\n","Loss 0.8640003204345703 W tensor([ 0.4800,  0.2920, -0.6700,  0.7600], requires_grad=True) y tensor([1.0000, 0.3360, 1.9200, 2.8800], grad_fn=<MulBackward0>)\n","Loss 0.7200003862380981 W tensor([ 0.4800,  0.4120, -0.6700,  0.7600], requires_grad=True) y tensor([1.0000, 0.4800, 1.9200, 2.8800], grad_fn=<MulBackward0>)\n","Loss 0.576000452041626 W tensor([ 0.4800,  0.5320, -0.6700,  0.7600], requires_grad=True) y tensor([1.0000, 0.6240, 1.9200, 2.8800], grad_fn=<MulBackward0>)\n","Loss 0.4320001006126404 W tensor([ 0.4800,  0.6520, -0.6700,  0.7600], requires_grad=True) y tensor([1.0000, 0.7680, 1.9200, 2.8800], grad_fn=<MulBackward0>)\n","Loss 0.28799980878829956 W tensor([ 0.4800,  0.7720, -0.6700,  0.7600], requires_grad=True) y tensor([1.0000, 0.9120, 1.9200, 2.8800], grad_fn=<MulBackward0>)\n","Loss 0.20159965753555298 W tensor([ 0.4800,  0.8440, -0.6700,  0.7600], requires_grad=True) y tensor([1.0000, 0.9984, 1.9200, 2.8800], grad_fn=<MulBackward0>)\n"]}]},{"cell_type":"markdown","metadata":{"id":"nntxBYExZWei"},"source":["## Via optimizer"]},{"cell_type":"code","metadata":{"id":"0HICqFWVgQ4D","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1665397032977,"user_tz":-180,"elapsed":282,"user":{"displayName":"Антон Ганичев","userId":"03960392406657956647"}},"outputId":"5550eee0-5cb4-4b7c-88de-b1c3781d1dbf"},"source":["import torch.optim as optim\n","\n","x = torch.tensor([2,1.2,-3,4],requires_grad= True)\n","w = torch.tensor([0.1,-0.2,-0.1,0.4],requires_grad= True)\n","\n","print(\"X = \",x)\n","print(\"W = \",w)\n","\n","y = x*w\n","L = y.sum() # loss stub\n","\n","print (\"dz/dy \",y.grad)\n","print (\"dz/dx \",x.grad)\n","\n","optimizer = optim.SGD([w],lr=0.01) # send params to optimizer\n","L.backward()\n","print(\"========== Backprop 1 ==============\")\n","print (\"dL/dy \",y.grad, \"  - because y is not a leaf\")\n","print (\"dL/dx \",x.grad)\n","print (\"dL/dw \",w.grad)\n","print(\"========== Optimize with LR = 0.01 ==============\")\n","optimizer.step()\n","print (\"X = \",x)\n","print (\"W = W-lr*(dL/dw) = \",w)\n","print(\"========== Zero grad ==============\")\n","optimizer.zero_grad()\n","print (\"dL/dx \",x.grad)\n","print (\"dL/dw \",w.grad,\" after zero_grad call\")\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["X =  tensor([ 2.0000,  1.2000, -3.0000,  4.0000], requires_grad=True)\n","W =  tensor([ 0.1000, -0.2000, -0.1000,  0.4000], requires_grad=True)\n","dz/dy  None\n","dz/dx  None\n","========== Backprop 1 ==============\n","dL/dy  None   - because y is not a leaf\n","dL/dx  tensor([ 0.1000, -0.2000, -0.1000,  0.4000])\n","dL/dw  tensor([ 2.0000,  1.2000, -3.0000,  4.0000])\n","========== Optimize with LR = 0.01 ==============\n","X =  tensor([ 2.0000,  1.2000, -3.0000,  4.0000], requires_grad=True)\n","W = W-lr*(dL/dw) =  tensor([ 0.0800, -0.2120, -0.0700,  0.3600], requires_grad=True)\n","========== Zero grad ==============\n","dL/dx  tensor([ 0.1000, -0.2000, -0.1000,  0.4000])\n","dL/dw  tensor([0., 0., 0., 0.])  after zero_grad call\n"]}]},{"cell_type":"markdown","metadata":{"id":"9Ol2b8Mci9bt"},"source":["## Typical train loop"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JuPhYahTi849","executionInfo":{"status":"ok","timestamp":1665397059923,"user_tz":-180,"elapsed":274,"user":{"displayName":"Антон Ганичев","userId":"03960392406657956647"}},"outputId":"239a4191-90f9-412d-cc30-54977c7546f4"},"source":["x = torch.tensor([2,1.2,-3,4],requires_grad= True)\n","w = torch.tensor([0.1,-0.2,-0.01,0.4],requires_grad= True)\n","print(\"X = \",x)\n","print(\"W = \",w)\n","\n","optimizer = optim.SGD([w],lr=0.01)\n","\n","for i in range(100):\n","  y = x*w\n","  loss = L = torch.sum(torch.abs(y - g_true)) # L1 Loss must be a scalar\n","  loss.backward()\n","  optimizer.step()\n","  optimizer.zero_grad()\n","  if i % 10 == 0:\n","    print(f\"Loss {loss.item()} W {w} y {y}\")\n",""],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["X =  tensor([ 2.0000,  1.2000, -3.0000,  4.0000], requires_grad=True)\n","W =  tensor([ 0.1000, -0.2000, -0.0100,  0.4000], requires_grad=True)\n","Loss 5.410000324249268 W tensor([ 0.1200, -0.1880, -0.0400,  0.4400], requires_grad=True) y tensor([ 0.2000, -0.2400,  0.0300,  1.6000], grad_fn=<MulBackward0>)\n","Loss 2.685999631881714 W tensor([ 0.3200, -0.0680, -0.3400,  0.7600], requires_grad=True) y tensor([ 0.6000, -0.0960,  0.9300,  2.8800], grad_fn=<MulBackward0>)\n","Loss 1.2420002222061157 W tensor([ 0.4800,  0.0520, -0.6400,  0.7600], requires_grad=True) y tensor([1.0000, 0.0480, 1.8300, 2.8800], grad_fn=<MulBackward0>)\n","Loss 0.937999427318573 W tensor([ 0.4800,  0.1720, -0.6400,  0.7600], requires_grad=True) y tensor([1.0000, 0.1920, 2.0100, 2.8800], grad_fn=<MulBackward0>)\n","Loss 0.793999433517456 W tensor([ 0.4800,  0.2920, -0.6400,  0.7600], requires_grad=True) y tensor([1.0000, 0.3360, 2.0100, 2.8800], grad_fn=<MulBackward0>)\n","Loss 0.6499994993209839 W tensor([ 0.4800,  0.4120, -0.6400,  0.7600], requires_grad=True) y tensor([1.0000, 0.4800, 2.0100, 2.8800], grad_fn=<MulBackward0>)\n","Loss 0.5059995651245117 W tensor([ 0.4800,  0.5320, -0.6400,  0.7600], requires_grad=True) y tensor([1.0000, 0.6240, 2.0100, 2.8800], grad_fn=<MulBackward0>)\n","Loss 0.3619992136955261 W tensor([ 0.4800,  0.6520, -0.6400,  0.7600], requires_grad=True) y tensor([1.0000, 0.7680, 2.0100, 2.8800], grad_fn=<MulBackward0>)\n","Loss 0.2179989218711853 W tensor([ 0.4800,  0.7720, -0.6400,  0.7600], requires_grad=True) y tensor([1.0000, 0.9120, 2.0100, 2.8800], grad_fn=<MulBackward0>)\n","Loss 0.13159877061843872 W tensor([ 0.4800,  0.8440, -0.6400,  0.7600], requires_grad=True) y tensor([1.0000, 0.9984, 2.0100, 2.8800], grad_fn=<MulBackward0>)\n"]}]}]}