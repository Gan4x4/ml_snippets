{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grads in pytorch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor([2, 1.2, 0, 4],requires_grad= True)\n",
    "\n",
    "y = x*x\n",
    "z = y.sum()\n",
    "\n",
    "print(\"x =\",x)\n",
    "print(\"x.grad = \",x.grad) # grad is none\n",
    "\n",
    "z.backward()\n",
    "print(\"y: \",y,\"y.grad = \", y.grad) # lead to warning\n",
    "print(\"x: \",x,\"x.grad = \", x.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss.backward() computes dloss/dx for every parameter x which has requires_grad=True.\n",
    "\n",
    "These are accumulated into x.grad for every parameter x.\n",
    "In pseudo-code:  x.grad += dloss/dx\n",
    "\n",
    "z = loss\n",
    "\n",
    "We want dz/dx\n",
    "\n",
    "dz/dx = dz/dy * dy/dx\n",
    "\n",
    "z = y0 + y1 + y2 + y3\n",
    "\n",
    "dz/dy = [1,1,1,1] dz equal to  loss\n",
    "\n",
    "y = x^2\n",
    "\n",
    "dy/dx = 2x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchviz import make_dot\n",
    "\n",
    "y = x*x\n",
    "z = y.sum()\n",
    "make_dot(z, params={'x':x, 'y':y, 'z':z}, show_saved=True)#, show_attrs=True, show_saved=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's call backward second time\n",
    "\n",
    "it's produce an error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  z.backward()\n",
    "except Exception as e:\n",
    "  print(\"ERROR\")\n",
    "  print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([2,1.2,0,4],requires_grad= True)\n",
    "print (\"X = \",x)\n",
    "\n",
    "y = x*x\n",
    "\n",
    "# Now grads for intermediate tensor y stay in memory\n",
    "y.retain_grad()\n",
    "\n",
    "z = y.sum()\n",
    "print (\"z (loss) =\",z)\n",
    "print (\"y =\",y)\n",
    "print (\"x =\",x)\n",
    "\n",
    "print (\"dz/dx \",x.grad) # grad is None\n",
    "\n",
    "print(\"========== Backprop 1 ==============\")\n",
    "z.backward(retain_graph=True)\n",
    "print (\"dz/dy \",y.grad)\n",
    "print (\"dz/dx \",x.grad)\n",
    "print(\"========== Backprop 2 ==============\")\n",
    "z.backward()\n",
    "\n",
    "# Grads are accumulated\n",
    "print (\"dz/dy \",y.grad)\n",
    "print (\"dz/dx \",x.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization in pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## manual\n",
    "\n",
    "https://discuss.pytorch.org/t/leaf-variable-was-used-in-an-inplace-operation/308"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([2,1.2,-3,4],requires_grad= True)\n",
    "w = torch.tensor([0.1,-0.2,-0.1,0.4],requires_grad= True)\n",
    "g_true = torch.tensor([1,1,2,3])\n",
    "print(\"X = \",x)\n",
    "print(\"W = \",w)\n",
    "\n",
    "for i in range(100):\n",
    "  y = x*w\n",
    "  loss = torch.sum(torch.abs(y - g_true)) # L1 Loss must be a scalar\n",
    "  loss.backward()\n",
    "  if i ==0 :\n",
    "    print(\"dW/dL\",w.grad) # stay the same\n",
    "  with torch.no_grad():\n",
    "    w =  w - 0.01* w.grad # update w with lr =0.01, note that w recreated here\n",
    "    w.requires_grad = True\n",
    "  if i % 10 == 0:\n",
    "    print(f\"Loss {loss.item()} W {w} y {y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Via optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "x = torch.tensor([2,1.2,-3,4],requires_grad= True)\n",
    "w = torch.tensor([0.1,-0.2,-0.1,0.4],requires_grad= True)\n",
    "\n",
    "print(\"X = \",x)\n",
    "print(\"W = \",w)\n",
    "\n",
    "y = x*w\n",
    "L = y.sum() # loss stub\n",
    "\n",
    "print (\"dz/dy \",y.grad)\n",
    "print (\"dz/dx \",x.grad)\n",
    "\n",
    "optimizer = optim.SGD([w],lr=0.01) # send params to optimizer\n",
    "L.backward()\n",
    "print(\"========== Backprop 1 ==============\")\n",
    "print (\"dL/dy \",y.grad, \"  - because y is not a leaf\")\n",
    "print (\"dL/dx \",x.grad)\n",
    "print (\"dL/dw \",w.grad)\n",
    "print(\"========== Optimize with LR = 0.01 ==============\")\n",
    "optimizer.step()\n",
    "print (\"X = \",x)\n",
    "print (\"W = W-lr*(dL/dw) = \",w)\n",
    "print(\"========== Zero grad ==============\")\n",
    "optimizer.zero_grad()\n",
    "print (\"dL/dx \",x.grad)\n",
    "print (\"dL/dw \",w.grad,\" after zero_grad call\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Typical train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([2,1.2,-3,4],requires_grad= True)\n",
    "w = torch.tensor([0.1,-0.2,-0.01,0.4],requires_grad= True)\n",
    "print(\"X = \",x)\n",
    "print(\"W = \",w)\n",
    "\n",
    "optimizer = optim.SGD([w],lr=0.01)\n",
    "\n",
    "for i in range(100):\n",
    "  y = x*w\n",
    "  loss = L = torch.sum(torch.abs(y - g_true)) # L1 Loss must be a scalar\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "  optimizer.zero_grad()\n",
    "  if i % 10 == 0:\n",
    "    print(f\"Loss {loss.item()} W {w} y {y}\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
