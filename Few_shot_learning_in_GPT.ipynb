{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few-shot learning in GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Языковые модели, такие как GPT-3, тренируются решать одну задачу: предсказывать следующее слово (токен) для текста. На первый взгляд, применений им немного: можно генерировать псведо-осмысленный текст для увеличения объема курсовой работы.\n",
    "\n",
    "Языковые модели содержат в своих весах знания об обучающем датасете. Чем больше весов, тем больше знаний. Огромные модели, такие как GPT-3 (175 млрд. весов), знают очень многое. Рассмотрим, как использовать эти знания для решения реальных задач."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L11/out/gpt_train_inference.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обученную языковую модель можно **обусловить на решение конкретной задачи** путем встраивания этой задачи в текстовый запрос на входе модели. Для этого необходимо сформулировать запрос таким образом, чтобы ожидаемый ответ был органичным продолжением запроса в среднестатистической книге или статье на соответствующую тему из обучающего датасета.\n",
    "\n",
    "Например, если нужен рецепт яблочного пирога, то запрос \"модель, прошу, выдай мне рецепт яблочного пирога\" вряд ли поможет. Надо сформировать запрос так, чтобы он был похож на начало абзаца поваренной книги, например: \"Рецепт яблочного пирога. Нам потребуются\".\n",
    "\n",
    "При таком использовании модели не происходит ее обучения или дообучения для определенной задачи: **веса не обновляются**, модель используется \"как есть\" и распознает задачу как часть входного запроса."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-shot learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zero-shot learning — это способ получения от модели ответа на задачу без дополнительных примеров ее решения. Фактически, мы просто ожидаем, что внутреннее представление мира позволит модели сразу дать ответ на наш исходный запрос.\n",
    "\n",
    "Этот принцип использован выше в примере с рецептом яблочного пирога. Мы просто рассчитываем, что модель из контекста запроса поймет, что мы от нее хотим, и сможет использовать свое представление мира для выдачи корректного ответа."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L11/out/gpt_zero_shot.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем поставить модели задачу **перефразировать** первое предложение из оригинальной статьи о GPT-3 [Language Models are Few-Shot Learners (Brown et al., 2020)](https://arxiv.org/abs/2005.14165). Для этого используем простую конструкцию запроса:\n",
    "\n",
    "> Original: ...\n",
    "> \n",
    "> Paraphrase:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оригинальная GPT-3 недоступна для свободного использования. Поэтому будем использовать [open-source версию GPT-J](https://textsynth.com/playground.html) от сообщества ElutherAI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее жирным шрифтом выделен запрос, а курсивом — сгенерированный текст."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Original: Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task.**\n",
    "> \n",
    "> **Paraphrase:** *I recently saw a post on stackoverflow that said that it's possible to pre-train a deep neural network (DNN) on a large body of training data, which it is commonly done in a “fast and loose” fashion. And, then a fine-tuning on a new task. This is the “pre-training” paradigm.*\n",
    ">\n",
    "> *If you pre-train a network on a large body of training data, it becomes more robust to input, and it is easier to transfer a new task. The pre-trained network is more robust to input because it has been forced to capture the distribution of a large body of training data.*\n",
    ">\n",
    "> *This is in contrast to training a neural network from scratch, where the only distribution is the data that you provided to the network.*\n",
    ">\n",
    "> *While pre-training a network on a large body of data is an elegant way to approach “task” specific training, it also causes a problem*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель сгенерировала связный текст на тему, соответствующую запросу. Но ответ нельзя назвать правильным: мы хотели получить перефразированное предложение, а получили свободное рассуждение.\n",
    "\n",
    "Это случилось потому, что мы не дали модели информацию о желаемом формате ответа. При такой нечеткой формулировке вопроса человек мог бы ошибиться так же, как и модель."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zero-shot learning может успешно применяться для решения задач, которые требуют лишь извлечения знаний из модели без смысловой обработки. Например, для получения рецепта или какого-то известного факта из истории."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-shot learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-shot learning — это формулировка задачи для модели с единственным примером верного решения аналогичной задачи. Добавим в начало запроса пример удачного перефразирования."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L11/out/gpt_one_shot.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Original: Giraffes like Acacia leaves and hay, and they can consume 75 pounds of food a day.**\n",
    ">\n",
    ">**Paraphrase: A giraffe can eat up to 75 pounds of Acacia leaves and hay daily.**\n",
    ">\n",
    ">**Original: Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task.**\n",
    ">\n",
    ">**Paraphrase:** *Research recently demonstrated large performance improvements on many NLP tasks and benchmarks by pre-training a deep network from scratch, before fine-tuning on the task of interest.*\n",
    ">\n",
    ">*Origenation note from authors: Some source-sensitivity was lost in this article, as there were some previous iterations of annotations based on the contributors' original source text.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Единственного примера хватило модели для того, чтобы сгенерировать удачную перефразировку. Но у модели не хватило сведений для того, чтобы понять, на каком месте следует закончить генерацию."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few-shot learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Few-shot learning — это формулировка задачи для модели с несколькими примерами верного решения аналогичных задач."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L11/out/gpt_few_shot.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Original: Her life spanned years of incredible change for women as they gained more rights than ever before.**\n",
    ">\n",
    ">**Paraphrase: She lived through the exciting era of women's liberation.**\n",
    ">\n",
    ">**Original: Giraffes like Acacia leaves and hay, and they can consume 75 pounds of food a day.**\n",
    ">\n",
    ">**Paraphrase: A giraffe can eat up to 75 pounds of Acacia leaves and hay daily.**\n",
    ">\n",
    ">**Original: Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task.**\n",
    ">\n",
    ">**Paraphrase:** *Recent work has demonstrated that training on a large corpus of text followed by fine-tuning on a specific task has led to considerable gains.*\n",
    ">\n",
    ">*Original: In her book, The Hands-On Teacher, Koehler provides suggestions for the following instructional strategies to help students gain insight into the material:*\n",
    ">\n",
    ">*Paraphrase: In her book, The Hands-On Teacher, Koehler provides suggestions for the following instructional strategies to help students gain insight into the material:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По двум примерам модель уловила закономерности:\n",
    "- в тексте есть структура \"вопрос-ответ\", которую надо продолжить;\n",
    "- ответ должен быть близок по смыслу и по длине к вопросу.\n",
    "\n",
    "Модель сгенерировала хороший ответ на заданный вопрос, а потом сама придумала следующий вопрос."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель может работать даже с несложными числовыми рядами."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Sequence: 2, 4, 6, 8**\n",
    ">\n",
    ">**Continuation: 16, 32, 64, 128**\n",
    ">\n",
    ">**Sequence: 3, 9, 27, 81**\n",
    ">\n",
    ">**Continuation: 243, 729, 2187, 6561**\n",
    ">\n",
    ">**Sequence: 4, 16, 64, 256**\n",
    ">\n",
    ">**Continuation:** *1024, 4096, 16384, 65536*\n",
    ">\n",
    ">*Sequence: 5, 25, 125, 625*\n",
    ">\n",
    ">*Continuation: 15625, 31250, 78125, 1953125*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данном случае модель уловила, что от нее требуется продолжить последовательность степеней четверки и справилась с этим. Далее она попыталась сгенерировать последовательность степеней пятерки, но в Continuation уже стала допускать ошибки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ограничения языковых моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предобученные мощные языковые модели можно использовать для быстрого решения разнообразных сложных задач. Но полученным ответам нельзя слепо доверять: часто модели используют совершенно не те закономерности, которые ожидает пользователь.\n",
    "\n",
    "Кроме того, ответы имеют стохастический характер. Поэтому иногда даже при качественных запросах результаты будут неудовлетворительными из-за случайности. Например, при предсказании числового ряда правильный результат был получен не с первого раза."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
