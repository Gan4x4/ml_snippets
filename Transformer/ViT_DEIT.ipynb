{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNj/esNkrSVXE+JA4RmiPo/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Y_hlZPMJFF5k"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"1quAsyIoELR_"},"source":["# Self Attention (ViT 2020)"]},{"cell_type":"markdown","metadata":{"id":"Xy2AjIP8ELR_"},"source":["[Visual Transformers: Token-based Image Representation and Processing for Computer Vision (Wu et al., 2020)](https://arxiv.org/abs/2006.03677)\n","\n","[Реализация](https://github.com/lucidrains/vit-pytorch)\n","\n","[Блог-пост разбор  ViT](https://towardsdatascience.com/implementing-visualttransformer-in-pytorch-184f9f16f632)\n","\n","\n","**Vision Transformer** — это модель для классификации изображений, которая использует архитектуру трансформера. Попробуем разобраться, как она работает.\n","\n","В 2020 году стали появляться работы, где модели на базе архитектур трансформер смогли показать результаты лучше, чем у **CNN** моделей."]},{"cell_type":"markdown","metadata":{"id":"esVezivyELR_"},"source":["<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L08/cited_vit_accuracy.png\"  width=\"650\"></center>\n","\n","\n","<center><em>Source: <a href=\"https://arxiv.org/abs/2010.11929\">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (Dosovitskiy et al., 2020</a></em></center>\n","\n","BiT — это baseline модель на базе **ResNet**, ViT — **Visual Transformer**\n","\n"]},{"cell_type":"markdown","metadata":{"id":"-XnU7HQLELSA"},"source":["### Недостатки сверточного слоя"]},{"cell_type":"markdown","metadata":{"id":"8zzDy9JUELSA"},"source":["Авторы практически полностью отказались от использования сверток,  заменив их слоями **self-attention**.  Попробуем понять, почему это сработало.\n","\n","Добавляя в модель свёрточный слой, мы руководствуемся резонным предположением: чем ближе пиксели на изображении, тем больше будет их взаимное влияние.\n","\n","В большинстве случаев это работает:"]},{"cell_type":"markdown","metadata":{"id":"aDCcnCKjELSB"},"source":["<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L08/cnn_ok.png\" width=\"700\"></center>"]},{"cell_type":"markdown","metadata":{"id":"C_xSoRy1ELSB"},"source":[" - На слое n (красный) активируются нейроны, которые реагируют на морду и на хвост кота.\n","\n"," - В карте активаций их выходы оказываются рядом, и в слое n + 1 (синий) они попадают в одну свертку, которая активируется на объектах типа \"кот\".\n","\n","Так случается часто, но не всегда:"]},{"cell_type":"markdown","metadata":{"id":"UjyW5OG5ELSC"},"source":["<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L08/cnn_fail.jpg\"  width=\"700\"></center>"]},{"cell_type":"markdown","metadata":{"id":"7npvaFjcELSC"},"source":["На этом изображении активации нейронов, реагирующих на морду и хвост, не попадут в одну свертку на следующем слое. Это может привести к тому, что нейрон, обучившийся реагировать на кошек, не активируется.\n","\n","Причиной этого является допущение ([Inductive bias](https://en.wikipedia.org/wiki/Inductive_bias)) о взаимном влиянии соседних пикселей."]},{"cell_type":"markdown","metadata":{"id":"5qi9Uw_BELSC"},"source":["### Self-attention"]},{"cell_type":"markdown","metadata":{"id":"QYXmEXiDELSC"},"source":["<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L08/global_attention.png\"  width=\"900\"></center>"]},{"cell_type":"markdown","metadata":{"id":"szKtHPJuELSD"},"source":["**Self-attention** слой лишен этого недостатка. Он обучается оценивать взаимное влияние входов друг на друга. Но как применить его к изображениям?\n","\n","В статье [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (Dosovitskiy et al., 2020)](https://arxiv.org/pdf/2010.11929.pdf) предлагается разбивать картинки на кусочки (patches) размером 16x16 пикселей и подавать их на вход модели.\n","\n","Проделаем это:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P-_spPcbELSD"},"outputs":[],"source":["URL = \"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L08/cat.jpeg\"\n","!wget -q $URL -O image.jpg"]},{"cell_type":"markdown","metadata":{"id":"ZQiI0WyJELSD"},"source":["Преобразуем изображение в тензор, порежем на фрагменты и отобразим их, используя image_grid"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"439kdYjBELSE"},"outputs":[],"source":["from torchvision import utils, transforms\n","import matplotlib.pyplot as plt\n","import torch\n","from PIL import Image\n","\n","img = Image.open(\"image.jpg\")\n","\n","transform = transforms.Compose([transforms.Resize((256, 256)), transforms.ToTensor()])\n","\n","img = transform(img)\n","patches = []\n","sz = 64\n","for r in range(0, img.shape[1], sz):\n","    for c in range(0, img.shape[2], sz):\n","        patches.append(img[:, r : r + sz, c : c + sz])\n","\n","patches = torch.stack(patches).type(torch.float)\n","\n","img_grid = utils.make_grid(patches, pad_value=10, normalize=True, nrow=4)\n","plt.imshow(transforms.ToPILImage()(img_grid).convert(\"RGB\"))\n","plt.axis(\"off\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"Rjdfi5GOELSE"},"source":["На вход модели они поступят в виде вектора:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zqau4NLRELSE"},"outputs":[],"source":["plt.figure(figsize=(18, 6))\n","img_grid = utils.make_grid(patches, pad_value=10, normalize=True, nrow=256 // 16)\n","plt.imshow(transforms.ToPILImage()(img_grid).convert(\"RGB\"))\n","plt.axis(\"off\");"]},{"cell_type":"markdown","metadata":{"id":"_kjpRfwGELSF"},"source":["Затем последовательность из фрагментов изображения передается в модель, где после ряда преобразований попадает на вход слоя **self-attention**:"]},{"cell_type":"markdown","metadata":{"id":"3IAS3GkIELSG"},"source":["<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L08/self_attention.png\"  width=\"900\"></center>"]},{"cell_type":"markdown","metadata":{"id":"ydt2lpU7ELSG"},"source":["Картинки приведены исключительно для наглядности, в действительности слой работает с векторами признаков, которые не визуализируются столь очевидно. Однако кэффициенты, с которыми складываются вектора-признаков, отражают важность каждого с учетом всех остальных входов.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"sMZv0NUwELSG"},"source":["## Сравнение со сверткой"]},{"cell_type":"markdown","metadata":{"id":"iVwIWl8FELSH"},"source":["<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L08/conv_vs_self_attention1.png\" width=\"400\">\n","\n","При свертке каждый признак умножается на свой вес, и затем они суммируются. Важно что вклад взвешенных признаков в сумму не зависит от контекста.\n","\n","То есть ягода клубники, лежащая на столе (где рядом с ней может быть все, что угодно), даст такой же вклад в сумму, как и ягода с клубничного куста.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"tFbI2PpAELSI"},"source":["<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L08/conv_vs_self_attention2.png\"  width=\"900\"></center>"]},{"cell_type":"markdown","metadata":{"id":"mrP7ufDzELSI"},"source":["Слой self-attention выполняет ту же задачу, что и свертка: получает на вход вектор признаков и возвращает другой, более информативный.  Но делает это более умно:\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"nkVKFq7ZELSJ"},"source":["<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L08/conv_vs_self_attention3.png\"  width=\"900\"></center>\n","\n","*Вместо чисел здесь вектора, но принципильно это ничего не меняет, можно применить self-attention и к отдельным признакам (яркостям, пикселям), просто для это потребуется очень много ресурсов.*"]},{"cell_type":"markdown","metadata":{"id":"KbP9jpxCELSK"},"source":["**Каждый признак** участвует в каждой сумме, а не только те, что попали в рецептивное поле фильтра.\n","Кроме этого, суммируются они с коэффициентами $a$, которые **зависят от входов** и различны для каждой суммы."]},{"cell_type":"markdown","metadata":{"id":"8zTrbU4BELSK"},"source":["Для получения этих коэффициентов и нужна большая часть слоя self-attention. На рисунке выделено красным."]},{"cell_type":"markdown","metadata":{"id":"nrCpJpDDELSK"},"source":["<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L08/conv_vs_self_attention5.png\"  width=\"900\"></center>"]},{"cell_type":"markdown","metadata":{"id":"HuXLmEfAELSL"},"source":["#### Как получить веса внимания?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MsKv6sxxELSM"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","\n","\n","class SelfAttention(nn.Module):\n","    def __init__(self, input_dim):\n","        super().__init__()\n","        self.input_dim = input_dim\n","        self.query = nn.Linear(input_dim, input_dim)\n","        self.key = nn.Linear(input_dim, input_dim)\n","        self.value = nn.Linear(input_dim, input_dim)\n","\n","    def forward(self, x):\n","        queries = self.query(x)\n","        keys = self.key(x)\n","        values = self.value(x)\n","        scores = torch.bmm(queries, keys.transpose(1, 2)) / (self.input_dim**0.5)\n","        attention = scores.softmax(dim=2)\n","        print(\"Scores shape\", scores.shape)\n","        weighted = torch.bmm(attention, values)\n","        return weighted"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bG9sGS8oELSM"},"outputs":[],"source":["embed_dim = 256\n","self_attention_layer = SelfAttention(embed_dim)\n","dummy_x = torch.randn(1, 4 * 4, embed_dim)  # Batch_size x Sequence_len x Embedding_size\n","out = self_attention_layer(dummy_x)\n","print(out.shape)"]},{"cell_type":"markdown","metadata":{"id":"8sMpqOnuELSM"},"source":["#### Соображения относительно размера patch"]},{"cell_type":"markdown","metadata":{"id":"xfq0lcgCELSM"},"source":["Трансформеры работают с последовательностями за счёт механизма внимания (**self-attention**). И чтобы подать на вход изображение, требуется превратить его в последовательность.\n","\n","Сделать это можно разными способами, например, составить последовательность из всех пикселей изображения. Её длина $n =  H*W$ (высота на ширину)\n","\n","[Сложность вычисления](https://www.researchgate.net/figure/Compare-the-computational-complexity-for-self-attention-where-n-is-the-length-of-input_tbl7_347999026) одноголового слоя **self-attention** $O(n^2 d )$,  где $n$ — число токенов и $d$ — размерность входа (embedding)  (для любознательных расчеты [тут](https://stackoverflow.com/questions/65703260/computational-complexity-of-self-attention-in-the-transformer-model)).\n","\n","То есть для квадратных изображений $(H==W)$ получим $O(H^3 d )$\n","\n","1. Такой подход будет очень вычислительно сложен.\n","\n","2. Интуитивно понятно, что кодировать каждый пиксель относительно большим embedding-ом не очень осмысленно.\n","\n","\n","*Для тех, кто забыл, напомним что $O()$ — это Big O notation, которая отражает ресурсы, требуемые для вычисления. Так для $O(1)$ — время вычисления будет постоянным, вне зависимости от количества данных, а для $O(N)$ — расти пропорционально количеству данных.*\n"]},{"cell_type":"markdown","metadata":{"id":"DCQm1n8aELSN"},"source":["Разберём на примере: Допустим, мы используем трансформер для предложения длиной в 4 слова — \"Мама мылом мыла раму\" => у нас есть `4 токена`. Закодируем их в *embeddings* с размерностью `256`. Потребуется порядка $4^2*256 = 4096$ операций.\n","\n","А теперь попробуем провернуть то же самое для картинки размерами 256 на 256.\n","Количество токенов\n","\n"," $256^3*256  = 256^4 =  4 294 967 296 $. Упс... Кажется, нам так никаких ресурсов не хватит — трансформеры с картинками использовать."]},{"cell_type":"markdown","metadata":{"id":"vPNxWeJaELSN"},"source":["\n","\n","Посчитаем сложность для картинки размером 256x256, разбитой на кусочки по 16px. при том же размере токена (256) $n = 16$.\n","$16^2*256 = 256^2 = 65536 $. И впрямь! ~65000 раз меньше ресурсов требуется."]},{"cell_type":"markdown","metadata":{"id":"wMoLhoonELSN"},"source":["[Как устроен  self-attention](https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a)\n","\n","[Self-attention слой в PyTorch](https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html)"]},{"cell_type":"markdown","metadata":{"id":"zHdNvvBuELSO"},"source":["### Position embedding"]},{"cell_type":"markdown","metadata":{"id":"1nSUDfzjELSO"},"source":["Не теряем ли мы важной информации, разбивая изображение на фрагменты? На первый взгляд кажется, что модель сможет научиться восстанавливать порядок, в котором фрагменты шли в исходном изображении.\n","\n","Всегда ли?\n","\n","Рассмотрим пример изображения, где нет ярко выраженной текстуры:"]},{"cell_type":"markdown","metadata":{"id":"Wl64bhf2ELSO"},"source":["<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L08/positional_transformer_explanation.png\"  width=\"500\"></center>"]},{"cell_type":"markdown","metadata":{"id":"_RMo3bNkELSO"},"source":["На рисунке а) наковальня падает на ребенка, на рисунке б) ребенок прыгает на наковальне.\n","\n","Суть принципиально отличается, но что будет, если составить из фрагментов любого изображения набор патчей:"]},{"cell_type":"markdown","metadata":{"id":"-aQaDeECELSP"},"source":["<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L08/positional_vec_transformer_explanation.png\"  width=\"500\"></center>"]},{"cell_type":"markdown","metadata":{"id":"ZL1pGIHBELSP"},"source":["Восстановить по нему можно будет любой из вариантов!\n","\n","Так как **self-attention** блок никак не кодирует позицию элемента на входе, то важная информация потеряется.\n","\n","Чтобы избежать таких потерь, информацию, кодирующую позицию фрагмента (patch),  добавляют к входным данным **self-attention** слоя в явном виде."]},{"cell_type":"markdown","metadata":{"id":"h4V57etIELSP"},"source":["<center><img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L08/out/linear_projection_of_flattened_patches.png\"  width=\"600\"></center>"]},{"cell_type":"markdown","metadata":{"id":"y3OZPSynELSQ"},"source":["[Методы для кодирования позиции](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/)"]},{"cell_type":"markdown","metadata":{"id":"8iR2S1viELSQ"},"source":["## Архитектура ViT"]},{"cell_type":"markdown","metadata":{"id":"ugQgJq_AELSQ"},"source":["Теперь мы можем грузить наши изображения в **Vi**sual **T**ransformer.\n","\n","**Self-attention** блок мы разобрали, остальные блоки модели нам знакомы:\n","\n","> **MLP** (Multi layer perceptron) — Блок из одного или нескольких линейных слоев\n","\n","> **Norm** — Layer Normalization"]},{"cell_type":"markdown","metadata":{"id":"GIbkL8KrELSR"},"source":["<center><img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L08/out/visual_transformer_architecture.png\"  width=\"1000\"></center>\n","<center><em>Архитектура Visual Transformer </em></center>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"bZ04yJBmELSR"},"source":["1.   Изображение режется на фрагменты (patch).\n","2.   Фрагменты (patch) подвергаются линейной проекции с помощью **MLP**.\n","3.   С полученными на выходе **MLP** векторами конкатенируются **positional embeddings** (кодирующие информацию о позиции path, как и в обычном трансформере для текста).\n","4. К полученным векторам добавляют еще один **0***, который называют **class embedding**.\n","\n","Любопытно, что для предсказания класса используется только выход. Он соответствует дополнительному **class embedding**.  Остальные выходы (а для каждего токена в трансформере есть свой выход) отбрасываются за ненадобностью.\n","\n","В финале этот специальный токен **0*** прогоняют через **MLP** и предсказывают классы."]},{"cell_type":"markdown","metadata":{"id":"0LuekbpCELSS"},"source":["Попробуем провести аналогию со свертками. Если рассмотреть сверточную сеть без слоев пулинга и других способов изменения пространственных размеров карт признаков, то можно считать такую сеть механизмом постепенно улучшаюшем качество признаков."]},{"cell_type":"markdown","metadata":{"id":"oP47s5xcELSS"},"source":["<center><img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L08/out/selfattention_feature_flow.png\"  width=\"900\"></center>"]},{"cell_type":"markdown","metadata":{"id":"0y9oZkFvELSS"},"source":["## Предсказание с помощью ViT\n","\n","\n","Используем пакет [ViT PyTorch](https://pypi.org/project/pytorch-pretrained-vit/)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qxVax7KFELST"},"outputs":[],"source":["!pip install -q pytorch_pretrained_vit"]},{"cell_type":"markdown","metadata":{"id":"SR38lqlZELST"},"source":["В пакете доступны несколько [предобученных моделей](https://github.com/lukemelas/PyTorch-Pretrained-ViT#loading-pretrained-models):\n","\n","B_16, B_32, B_16_imagenet1k, ...\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WVmvnp6WELST"},"outputs":[],"source":["from pytorch_pretrained_vit import ViT\n","from torchvision import transforms\n","\n","model = ViT(\"B_16_imagenet1k\", pretrained=True)\n","model.eval()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DBNSs-HpELSU"},"outputs":[],"source":["# Load image\n","!wget  https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L08/capybara.jpg"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CApFlWSPELSV"},"outputs":[],"source":["capybara_in_pil = Image.open(\"capybara.jpg\")\n","transforms = transforms.Compose(\n","    [\n","        transforms.Resize((384, 384)),\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n","    ]\n",")\n","capybara_in_tensor = transforms(capybara_in_pil)\n","print(capybara_in_tensor.shape)  # torch.Size([1, 3, 384, 384])\n","\n","# Classify\n","with torch.no_grad():\n","    outputs = model(capybara_in_tensor.unsqueeze(0))\n","print(outputs.shape)  # (1, 1000)"]},{"cell_type":"markdown","metadata":{"id":"QNGC4eTCELSV"},"source":["Давайте посмотрим, что нам предсказывает ViT. Для этого подгрузим dict с переводом индексов в человеческие названия:"]},{"cell_type":"markdown","metadata":{"id":"gqPjGrHEELSV"},"source":["И, собственно, переведем индекс в название:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pY2L4_6TELSW"},"outputs":[],"source":["top3 = outputs[0].topk(3).indices\n","top3 = top3.tolist()\n","\n","\n","print(\"Top 3 predictions:\")\n","for class_num in top3:\n","    print(class_num, classes[class_num])\n","display(capybara_in_pil.resize((384, 384)))"]},{"cell_type":"markdown","metadata":{"id":"IZyYxaiBELSW"},"source":["Ну что ж, почти (капибар в классах ImageNet 1k, как вы могли догадаться, просто нет)."]},{"cell_type":"markdown","metadata":{"id":"dTR-q1MFELSW"},"source":["## Обучение ViT"]},{"cell_type":"markdown","metadata":{"id":"OyP43GDKELSX"},"source":["### Объем данных и ресурсов\n","\n","Как следует из текста [статьи](https://arxiv.org/abs/2010.11929), **ViT**, обученный на **ImageNet**, уступал baseline CNN-модели\n","на базе сверточной сети (**ResNet**). И только при увеличении датасетов больше, чем **ImageNet**, преимущество стало заметным."]},{"cell_type":"markdown","metadata":{"id":"UeSOWYXXELSX"},"source":["<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L08/cited_vit_accuracy.png\"  width=\"400\"></center>\n","\n","<center><em>Source: <a href=\"https://arxiv.org/abs/2010.11929\">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (Dosovitskiy et al., 2020)</a></em></center>\n"]},{"cell_type":"markdown","metadata":{"id":"Pm4-G43CELSX"},"source":["Вряд ли в вашем распоряжении окажется датасет, сравнимый с [JFT-300M](https://paperswithcode.com/dataset/jft-300m) (300 миллионов изображений),\n","и GPU/TPU ресурсы, необходимые для обучения с нуля (*it could be trained using a standard cloud TPUv3 with 8 cores in approximately 30 days*)\n","\n","Поэтому для работы с пользовательскими данными используется техника дообучения ранее обученной модели на пользовательских данных (**fine-tuning**)."]},{"cell_type":"markdown","metadata":{"id":"LbDS07FIELSY"},"source":["## DeiT: Data-efficient Image Transformers"]},{"cell_type":"markdown","metadata":{"id":"SYcjVVaaELSY"},"source":["Для практических задач рекомендуем использовать эту реализацию. Авторы предлагают подход, благодаря которому становится возможным обучить модель на стандартном **ImageNet** (ImageNet1k) на одной рабочей станции за 3 дня.\n","\n","*We train them on a single computer in less than 3 days. Our reference vision transformer (86M parameters) achieves top-1 accuracy of 83.1% (single-crop evaluation) on ImageNet with no external data.*"]},{"cell_type":"markdown","metadata":{"id":"LhhumLagELSY"},"source":["<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L08/cited_deit_vit.png\"  width=\"700\"></center>\n","\n","<center><em>Source: <a href=\"https://arxiv.org/abs/2012.12877\">Training data-efficient image transformers & distillation through attention</a></em></center>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"gI5LOzYpELSZ"},"source":["Разбор этого материала уже не входит в наш курс и рекомендуется к самостоятельному изучению.\n","\n","Дополнительно:\n","[Distilling Transformers: (DeiT) Data-efficient Image Transformers](https://towardsdatascience.com/distilling-transformers-deit-data-efficient-image-transformers-61f6cd276a03)\n","\n","Статьи, предшествовавшие появлению **ViT**:\n","\n","[Non-local Neural Networks](https://arxiv.org/abs/1711.07971)\n","\n","[CCNet: Criss-Cross Attention for Semantic Segmentation](https://arxiv.org/abs/1811.11721)\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"TF3IDYXVELSZ"},"source":["### Использование ViT с собственным датасетом"]},{"cell_type":"markdown","metadata":{"id":"nQ7qkds1ELSZ"},"source":["Для использования **ViT** с собственными данными рекомендуем не обучать собственную модель с нуля, а использовать уже предобученную.\n","\n","Рассмотрим этот процесс на примере. Есть предобученный на **ImageNet** **Visual Transformer**, например: [deit_tiny_patch16_224](https://github.com/facebookresearch/deit)\n","\n","И мы хотим использовать ее со своим датасетом, который может сильно отличаться от **ImageNet**.\n","\n","Для примера возьмем **CIFAR-10**.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"S8wMth_-ELSa"},"source":["Загрузим модель. Как указано на [github](https://github.com/facebookresearch/deit), модель зависит от библиотеки [timm](https://fastai.github.io/timmdocs/), которую нужно установить."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YrVuyRnSELSa"},"outputs":[],"source":["!pip install -q timm"]},{"cell_type":"markdown","metadata":{"id":"N-jVzHJXELSa"},"source":["Теперь загружаем модель с [pytorch-hub](https://pytorch.org/hub/):"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4Drz-qnoELSb"},"outputs":[],"source":["import torch\n","\n","model = torch.hub.load(\n","    \"facebookresearch/deit:main\", \"deit_tiny_patch16_224\", pretrained=True\n",")"]},{"cell_type":"markdown","metadata":{"id":"DULhezvJELSb"},"source":["Убедимся, что модель запускается.\n","Загрузим изображение:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P9fV77AzELSb"},"outputs":[],"source":["!wget  https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L08/capybara.jpg"]},{"cell_type":"markdown","metadata":{"id":"yQc1XlxLELSc"},"source":["И подадим его на вход трансформеру:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"91tVX9bwELSd"},"outputs":[],"source":["from timm.data.constants import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n","import torchvision.transforms as T\n","from PIL import Image\n","\n","pil = Image.open(\"capybara.jpg\")\n","\n","# create the data transform that DeiT expects\n","imagenet_transform = T.Compose(\n","    [\n","        T.Resize((224, 224)),\n","        T.ToTensor(),\n","        T.Normalize(IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD),\n","    ]\n",")\n","\n","out = model(imagenet_transform(pil).unsqueeze(0))\n","print(out.shape)\n","pil.resize((224, 224))"]},{"cell_type":"markdown","metadata":{"id":"n3Jg0re6ELSd"},"source":["Чтобы использовать модель с **CIFAR-10**, нужно поменять количество выходов слоя, отвечающих за классификацию. Так как в **CIFAR-10** десять классов, а в **ImageNet** — тысяча.\n","\n","Чтобы понять, как получить доступ к последнему слою, выведем структуру модели:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ldSm8svVELSd"},"outputs":[],"source":["print(model)"]},{"cell_type":"markdown","metadata":{"id":"VaUBULBHELSe"},"source":["Видим, что последний слой называется head и, судя по количеству параметров на выходе (1000), которое совпадает с количеством классов **ImageNet**, именно он отвечает за классификацию."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YDXeRdcXELSe"},"outputs":[],"source":["print(model.head)"]},{"cell_type":"markdown","metadata":{"id":"yY325QQzELSe"},"source":["Заменим его слоем с 10-ю выходами по количеству классов в CIFAR-10."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-fgCmioEELSf"},"outputs":[],"source":["model.head = torch.nn.Linear(192, 10, bias=True)"]},{"cell_type":"markdown","metadata":{"id":"zYteoFlAELSf"},"source":["Убедимся, что модель не сломалась."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5hMGA1_uELSf"},"outputs":[],"source":["out = model(imagenet_transform(pil).unsqueeze(0))\n","print(out.shape)"]},{"cell_type":"markdown","metadata":{"id":"lqno8MhNELSg"},"source":["Теперь загрузим **CIFAR-10** и проверим, как дообучится модель"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1LSmrgKlELSg"},"outputs":[],"source":["from torchvision.datasets import CIFAR10\n","from torch.utils.data import DataLoader\n","\n","cifar10 = CIFAR10(root=\"./\", train=True, download=True, transform=imagenet_transform)\n","\n","# We use only part of CIFAR10 to reduce training time\n","trainset, _ = torch.utils.data.random_split(cifar10, [10000, 40000])\n","train_loader = DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n","\n","testset = CIFAR10(root=\"./\", train=False, download=True, transform=imagenet_transform)\n","test_loader = DataLoader(testset, batch_size=128, shuffle=False, num_workers=2)"]},{"cell_type":"markdown","metadata":{"id":"9VaIAe0gELSh"},"source":[" Проведем стандартный цикл обучения."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NlKDc4B4ELSi"},"outputs":[],"source":["from torch import nn\n","from tqdm.notebook import tqdm_notebook\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","\n","def train(model, train_loader, optimizer, num_epochs=1):\n","    model.to(device)\n","    model.train()\n","    criterion = nn.CrossEntropyLoss()\n","\n","    for epoch in range(num_epochs):\n","        for batch in tqdm_notebook(train_loader):\n","            inputs, labels = batch\n","            optimizer.zero_grad()\n","            outputs = model(inputs.to(device))\n","            loss = criterion(outputs, labels.to(device))\n","            loss.backward()\n","            optimizer.step()"]},{"cell_type":"markdown","metadata":{"id":"-wLEW2EMELSi"},"source":["Дообучаем (**fine tune**) только последний слой модели, который мы изменили."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yBBi3f8pELSj"},"outputs":[],"source":["import torch.optim as optim\n","\n","model.to(device)\n","optimizer = optim.SGD(model.head.parameters(), lr=0.001, momentum=0.9)\n","train(model, train_loader, optimizer)"]},{"cell_type":"markdown","metadata":{"id":"MP0hHg0sELSj"},"source":["Проверим точность, на всей тестовой подвыборке **CIFAR-10**."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M3_YnEpmELSj"},"outputs":[],"source":["@torch.inference_mode()\n","def accuracy(model, testloader):\n","    correct = 0\n","    total = 0\n","    for batch in testloader:\n","        images, labels = batch\n","        outputs = model(images.to(device))\n","        # the class with the highest energy is what we choose as prediction\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels.to(device)).sum().item()\n","    return correct / total"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iF1NtlSqELSk"},"outputs":[],"source":["print(f\"Accuracy of fine-tuned network : {accuracy(model, test_loader):.2f} \")"]},{"cell_type":"markdown","metadata":{"id":"Vys8dp-dELSk"},"source":["Дообучив последний слой на одной эпохе с использованием 20% данных, мы получили точность ~0.75\n","\n","Если дообучить все слои на 2-х эпохах, можно получить точность порядка 0.95.\n","\n","Это результат намного лучше чем тот, что мы получали на семинарах.\n","\n","Для этого потребуется порядка 10 мин (на GPU). Сейчас мы этого делать не будем.\n"]},{"cell_type":"markdown","metadata":{"id":"X_drN5KKELSl"},"source":["И одной из причин того, что обучение идет относительно медленно, является увеличение изображений размером 32x32 до 224x224.\n","\n","Если бы мы использовали изображения **CIFAR-10** в их родном размере, мы бы не потеряли никакой информации, но могли бы в разы ускорить обучение.\n"]},{"cell_type":"markdown","metadata":{"id":"E8tyh16fELSm"},"source":["### Изменение размеров входа ViT\n","\n","На первый взгляд, ничего не мешает это сделать: **self-attention** слой работает с произвольным количеством входов.\n","\n","Давайте посмотрим, что будет, если подать на вход модели изображение, отличное по размерам от 224x224.\n","\n","Для этого перезагрузим модель:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"02p2nNqtELSm"},"outputs":[],"source":["def get_model():\n","    model = torch.hub.load(\n","        \"facebookresearch/deit:main\", \"deit_tiny_patch16_224\", pretrained=True\n","    )\n","    model.head = torch.nn.Linear(192, 10, bias=True)\n","    return model\n","\n","\n","model = get_model()"]},{"cell_type":"markdown","metadata":{"id":"0CeQ9Hk5ELSn"},"source":["И уберем из трансформаций Resize:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zoOdlqeXELSn"},"outputs":[],"source":["cifar_transform = T.Compose(\n","    [\n","        # T.Resize((224, 224)),    don't remove this line\n","        T.ToTensor(),\n","        T.Normalize(IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD),\n","    ]\n",")\n","\n","# Change transformation in base dataset\n","cifar10.transform = cifar_transform\n","first_img = trainset[0][0]\n","\n","model.to(torch.device(\"cpu\"))\n","try:\n","    out = model(first_img.unsqueeze(0))\n","except Exception as e:\n","    print(\"Exception:\", e)"]},{"cell_type":"markdown","metadata":{"id":"euPKON7mELSo"},"source":["Получаем ошибку.\n","\n","Ошибка возникает в объекте [PatchEmbed](https://huggingface.co/spaces/Andy1621/uniformer_image_demo/blob/main/uniformer.py#L169), который превращает изображение в набор эмбеддингов.\n","\n","У объекта есть свойство `img_size`, попробуем просто поменять его:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nMbY-8QXELSo"},"outputs":[],"source":["model.patch_embed.img_size = (32, 32)\n","try:\n","    out = model(first_img.unsqueeze(0))\n","except Exception as e:\n","    print(\"Exception:\", e)"]},{"cell_type":"markdown","metadata":{"id":"VcL5iiRfELS-"},"source":["Получаем новую ошибку.\n","\n","И возникает она в строке\n","`x = self.pos_drop(x + self.pos_embed)`\n","\n","x — это наши новые эмбеддинги для CIFAR-10 картинок\n","\n","Откуда взялось число 5?\n","\n","4 — это закодированные фрагменты (patch) для картинки 32х32, их всего 4 (16x16) + один embedding для предсказываемого класса(class embedding).\n","\n","А 197 — это positional encoding — эмбеддинги, кодирующие позицию элемента. Они остались от **ImageNet**.\n","\n","Так как в ImageNet картинки размера 224x224, то в каждой помещалось 14x14 = 196 фрагментов и еще embedding для класса, итого 197 позиций.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"h1sDQnHCELS_"},"source":["Эмбеддинги для позиций доступны через свойство:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-TUuRTJCELS_"},"outputs":[],"source":["model.pos_embed.data.shape"]},{"cell_type":"markdown","metadata":{"id":"N-JLvjasELTA"},"source":["Теперь нам надо изменить количество pos embeddings так, чтобы оно было равно 5  (количество patch + 1).\n","Возьмем 5 первых:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RlbgwC68ELTA"},"outputs":[],"source":["model.pos_embed.data = model.pos_embed.data[:, :5, :]\n","out = model(first_img.unsqueeze(0))\n","print(out.shape)"]},{"cell_type":"markdown","metadata":{"id":"IJclA-jDELTB"},"source":["Заработало!\n","\n","Теперь обучим модель. Так как изображения стали намного меньше, то мы можем увеличить размер batch и использовать весь датасет. Также будем обучать все слои, а не только последний."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yCW2LvRPELTC"},"outputs":[],"source":["cifar10.transform = cifar_transform\n","train_loader = DataLoader(cifar10, batch_size=512, shuffle=True, num_workers=2)\n","\n","# Now we train all parameters because model altered\n","optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n","train(model, train_loader, optimizer)"]},{"cell_type":"markdown","metadata":{"id":"7mWwq_W8ELTC"},"source":["Сильно быстрее.\n","Посмотрим на результат:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6tguKpKxELTD"},"outputs":[],"source":["testset.transform = cifar_transform\n","print(f\"Accuracy of altered network : {accuracy(model,test_loader):.2f} \")"]},{"cell_type":"markdown","metadata":{"id":"mNjS5U_9ELTD"},"source":["Сильно хуже.\n","\n","Это можно объяснить тем, что  маленькие patch  ImageNet(1/196) семантически сильно отличаются от четвертинок картинок из CIFAR-10 (1/4).\n","\n","Но есть и другая причина: мы взяли лишь первые 4 pos_embedding а остальные отбросили. В итоге модель вынуждена практически заново обучаться работать с малым pos_embedding, и двух эпох для этого мало.\n","\n","Зато теперь мы можем использовать модель с изображениями любого размера."]}]}