{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R437ysKgzXtA"
      },
      "source": [
        "# –ú–µ—Ç–æ–¥—ã, —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –¥–ª—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_IGUKRhzXtA"
      },
      "source": [
        "–í —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞—Ö –µ—Å—Ç—å –º–µ—Ö–∞–Ω–∏–∑–º **self-attention**, –∫–æ—Ç–æ—Ä—ã–π –∫–∞–∂–µ—Ç—Å—è –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–º —Å–ø–æ—Å–æ–±–æ–º –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å, –∫–∞–∫–∏–µ —Ç–æ–∫–µ–Ω—ã —Ç–µ–∫—Å—Ç–∞/–∫—É—Å–æ—á–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–º–µ—é—Ç –±–æ–ª—å—à—É—é –≤–∞–∂–Ω–æ—Å—Ç—å –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3J3vEMbOzXtA"
      },
      "source": [
        "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.1/L14/self_attention.png\"  width=\"900\"></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSC5fo-ozXtB"
      },
      "source": [
        "–≠—Ç–æ –±—ã –æ—Ç–ª–∏—á–Ω–æ —Ä–∞–±–æ—Ç–∞–ª–æ, –±—É–¥—å —É –Ω–∞—Å —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω –±–ª–æ–∫ **self-attention**, –Ω–æ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ **–Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –±–ª–æ–∫–æ–≤ –∫–æ–¥–µ—Ä–∞/–¥–µ–∫–æ–¥–µ—Ä–∞**, —Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω–Ω—ã—Ö –¥—Ä—É–≥ –∑–∞ –¥—Ä—É–≥–æ–º. –û—Ç —Å–ª–æ—è –∫ —Å–ª–æ—é –∑–∞ —Å—á–µ—Ç —Ç–æ–≥–æ –∂–µ **self-attention** **–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –≤ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞—Ö –ø–µ—Ä–µ–º–µ—à–∏–≤–∞–µ—Ç—Å—è** —Å–∏–ª—å–Ω–µ–µ –∏ —Å–∏–ª—å–Ω–µ–µ. –¢–∞–∫, –Ω–∞–ø—Ä–∏–º–µ—Ä, –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ –º—ã –º–æ–∂–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥ –Ω–∞ –≤—ã—Ö–æ–¥–µ **BERT**, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π `[CLS]` —Ç–æ–∫–µ–Ω—É, –≤ –∫–æ—Ç–æ—Ä–æ–º –Ω–∞ –≤—Ö–æ–¥–µ **BERT** –Ω–µ –±—ã–ª–æ –Ω–∏–∫–∞–∫–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –ø–æ—Å–ª–µ–¥—É—é—â–µ–º —Ç–µ–∫—Å—Ç–µ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biqSMdWxzXtB"
      },
      "source": [
        "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.1/L14/transformer_text_translation_example.png\" width=\"800\"></center>\n",
        "\n",
        "<center><em>Source: <a href=\"https://jalammar.github.io/illustrated-bert/\">The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)</a></em></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QW5jtqH5zXtB"
      },
      "source": [
        "–ö—Ä–æ–º–µ —Ç–æ–≥–æ, –≤ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–º –±–ª–æ–∫–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ –µ—Å—Ç—å **residual —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è**. –ò–∑-–∑–∞ —ç—Ç–æ–≥–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ç–æ–∫–µ–Ω–∞—Ö/–ø–∞—Ç—á–∞—Ö –ø—Ä–æ—Ö–æ–¥–∏—Ç –Ω–µ —Ç–æ–ª—å–∫–æ —á–µ—Ä–µ–∑ **attention**, –Ω–æ –∏ —á–µ—Ä–µ–∑ **residual —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRlLn4FAzXtB"
      },
      "source": [
        "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L14/out/transformer_architecture.png\" width=\"450\"></center>\n",
        "\n",
        "<center><em>–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞</em></center>\n",
        "\n",
        "<center><em>Source: <a href=\"https://arxiv.org/pdf/1706.03762.pdf\"> Attention Is All You Need</a></em></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5naEvAlzXtB"
      },
      "source": [
        "–ü–æ —ç—Ç–∏–º –ø—Ä–∏—á–∏–Ω–∞–º –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ —á–µ—Ä–µ–∑ attention ‚Äî –Ω–µ–ø—Ä–æ—Å—Ç–∞—è –∑–∞–¥–∞—á–∞.\n",
        "\n",
        "**–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ:** –º–µ—Ç–æ–¥—ã –æ–±—ä—è—Å–Ω–µ–Ω–∏—è **attention** —Ç–æ–ª—å–∫–æ —á–∞—Å—Ç–∏—á–Ω–æ –æ–±—ä—è—Å–Ω—è—é—Ç —Ä–∞–±–æ—Ç—É —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞, –æ–Ω–∏  —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã –∏ –º–æ–≥—É—Ç –¥–∞–≤–∞—Ç—å –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã.\n",
        "\n",
        "–î–∞–≤–∞–π—Ç–µ –¥–ª—è –Ω–∞—á–∞–ª–∞ –Ω–µ–º–Ω–æ–≥–æ –ø–æ–∏–∑—É—á–∞–µ–º, –∫–∞–∫ –≤—ã–≥–ª—è–¥—è—Ç –∑–Ω–∞—á–µ–Ω–∏—è self-attention –≤ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞—Ö. –ü–æ–¥–≥—Ä—É–∂–∞–µ–º –±–∏–±–ª–∏–æ—Ç–µ–∫—É."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZB0Vy_rzXtB"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONKLgx6ozXtB"
      },
      "source": [
        "–í–æ–∑—å–º–µ–º –±–∞–∑–æ–≤—ã–π —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã–π —Ä–∞–∑–≥–æ–≤–æ—Ä–Ω—ã–π [BERT –æ—Ç Deep Pavlov üõ†Ô∏è[doc]](https://huggingface.co/DeepPavlov/rubert-base-cased-conversational), [–æ–±—É—á–µ–Ω–Ω—ã–π üõ†Ô∏è[doc]](https://huggingface.co/blanchefort/rubert-base-cased-sentiment) –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–π –æ–∫—Ä–∞—Å–∫–∏ –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ä—É—Å—Å–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤. –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏ –º–æ–¥–µ–ª—å. –°—Ç–∞–≤–∏–º —Ñ–ª–∞–≥ `output_attentions=True`, —á—Ç–æ–±—ã –º–æ–¥–µ–ª—å –≤–æ–∑–≤—Ä–∞—â–∞–ª–∞ –∑–Ω–∞—á–µ–Ω–∏—è attention."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eNfZ_RjlzXtB"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "from transformers import BertTokenizerFast, AutoModelForSequenceClassification\n",
        "from IPython.display import clear_output\n",
        "\n",
        "tokenizer = BertTokenizerFast.from_pretrained(\n",
        "    \"blanchefort/rubert-base-cased-sentiment\",\n",
        ")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"blanchefort/rubert-base-cased-sentiment\",\n",
        "    output_attentions=True,  # for save attention\n",
        ")\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7G1aHgz1zXtB"
      },
      "source": [
        "–ì–æ—Ç–æ–≤–∏–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5oYRCqazXtB"
      },
      "outputs": [],
      "source": [
        "sentences = [\n",
        "    \"–ú–∞–º–∞ –º—ã–ª–∞ —Ä–∞–º—É\",\n",
        "    \"–§–∏–ª—å–º —Å–¥–µ–ª–∞–Ω –æ—Ç–∫—Ä–æ–≤–µ–Ω–Ω–æ –ø–ª–æ—Ö–æ\",\n",
        "    \"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —Å–∫—É—á–Ω—ã–π —Å–µ—Ä–∏–∞–ª, –≥–¥–µ —Å—é–∂–µ—Ç –≤—ã—Å–æ—Å–∞–Ω –∏–∑ –ø–∞–ª—å—Ü–∞\",\n",
        "    \"–Ø –±—ã–ª –≤ –≤–æ—Å—Ç–æ—Ä–≥–µ\",\n",
        "    \"–í –æ–±—â–µ–º, –∫–∏–Ω–æ —Ö–æ—Ä–æ—à–µ–µ –∏ –µ—Å—Ç—å –º–Ω–æ–≥–æ —á—Ç–æ –ø–æ–æ–±—Å—É–∂–¥–∞—Ç—å\",\n",
        "]\n",
        "\n",
        "tokens = [\n",
        "    [\"[cls]\"] + tokenizer.tokenize(sentence) + [\"[sep]\"] for sentence in sentences\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqMMrnj-zXtB"
      },
      "source": [
        "–ü–æ—Å–º–æ—Ç—Ä–∏–º, –∫–∞–∫ —Ä–∞–∑–±–∏–≤–∞–µ—Ç—Å—è –Ω–∞ —Ç–æ–∫–µ–Ω—ã –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ. –ù–∞ –≤—ã—Ö–æ–¥–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –Ω–æ–º–µ—Ä–∞ —Ç–æ–∫–µ–Ω–æ–≤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r_2foaCNzXtC"
      },
      "outputs": [],
      "source": [
        "item = 0\n",
        "print(f\"Tokens: {tokens[item]}\")\n",
        "token_ids = [tokenizer.encode(sentence) for sentence in sentences]\n",
        "print(f\"Token ids: {token_ids[item]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZAhD0BIzXtC"
      },
      "source": [
        "–ü–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –º–æ–¥–µ–ª–∏, —á—Ç–æ–±—ã –ø—Ä–æ–≤–µ—Ä–∏—Ç—å, –Ω–∞—Å–∫–æ–ª—å–∫–æ –æ–Ω–∞ –∞–¥–µ–∫–≤–∞—Ç–Ω–∞."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sutw_ktKzXtC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "ans = {0: \"NEUTRAL\", 1: \"POSITIVE\", 2: \"NEGATIVE\"}\n",
        "\n",
        "for item in range(5):\n",
        "    input_ids = torch.tensor([token_ids[item]])\n",
        "    model_output = model(input_ids)\n",
        "    predicted = torch.argmax(model_output.logits, dim=1).numpy()\n",
        "    print(f\"Text: {sentences[item]}\")\n",
        "    print(f\"Predict lable = {predicted}, {ans[predicted.item()]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tXwtSm2zXtC"
      },
      "source": [
        "–í –¥–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ 12 —Å–ª–æ–µ–≤ (–±–ª–æ–∫–æ–≤ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤), –ø–æ—ç—Ç–æ–º—É –º–æ–¥–µ–ª—å –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ—Ä—Ç–µ–∂ –∏–∑ 12 —Ç–µ–Ω–∑–æ—Ä–æ–≤. –ö–∞–∂–¥—ã–π —Å–ª–æ–π –∏–º–µ–µ—Ç 12 –≥–æ–ª–æ–≤ self-attention."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ixbpxeWdzXtC"
      },
      "outputs": [],
      "source": [
        "item = 1\n",
        "input_ids = torch.tensor([token_ids[item]])\n",
        "model_output = model(input_ids)\n",
        "\n",
        "attentions = model_output.attentions\n",
        "print(f\"Text: {sentences[item]}\")\n",
        "print(f\"Tokens: {tokens[item]}\")\n",
        "print(f\"Number of layers: {len(attentions)}\")\n",
        "print(\n",
        "    f\"Attention size: {attentions[0].shape} \"\n",
        "    \"[batch x attention_heads x seq_size x seq_size]\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25cprXKDzXtC"
      },
      "source": [
        "–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ –æ–¥–Ω–æ—Ä–æ–¥–Ω—ã–π –º–∞—Å—Å–∏–≤ –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞ –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–π.\n",
        "\n",
        "–ö–æ–¥ —ç—Ç–æ–π —á–∞—Å—Ç–∏ –ª–µ–∫—Ü–∏–∏ –æ—Å–Ω–æ–≤–∞–Ω–∞ –Ω–∞:\n",
        "* [[git] üêæ Attention flow](https://github.com/samiraabnar/attention_flow),\n",
        "* [[arxiv] üéì Quantifying Attention Flow in Transformers (Abnar, Zuidema, 2020)](https://arxiv.org/pdf/2005.00928.pdf).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9WAkOnmVzXtC"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def to_array(attentions):\n",
        "    attentions_arr = [attention.detach().numpy() for attention in attentions]\n",
        "    return np.asarray(attentions_arr)[:, 0]\n",
        "\n",
        "\n",
        "attentions_arr = to_array(attentions)\n",
        "print(\n",
        "    f\"Shape: {attentions_arr.shape} \" \"[layers x attention_heads x seq_size x seq_size]\"\n",
        ")\n",
        "print(f\"Type: {type(attentions_arr)}, Dtype: {attentions_arr.dtype}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKQ9hExizXtC"
      },
      "source": [
        "–ü–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ **–Ω—É–ª–µ–≤—É—é –≥–æ–ª–æ–≤—É –Ω—É–ª–µ–≤–æ–≥–æ —Å–ª–æ—è**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EJlD47STzXtC"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x_ticks = tokens[item]\n",
        "y_ticks = tokens[item]\n",
        "\n",
        "sns.heatmap(\n",
        "    data=attentions_arr[0][0],\n",
        "    vmin=0,\n",
        "    vmax=1,\n",
        "    xticklabels=x_ticks,\n",
        "    yticklabels=y_ticks,\n",
        "    cmap=\"YlOrRd\",\n",
        ")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txyFw5ozzXtD"
      },
      "source": [
        "–¢—É—Ç –ø–æ **–æ—Å–∏ x** ‚Äî —Ç–æ–∫–µ–Ω—ã, **–Ω–∞** –∫–æ—Ç–æ—Ä—ã–µ —Å–º–æ—Ç—Ä–∏—Ç –≤–Ω–∏–º–∞–Ω–∏–µ, –ø–æ **–æ—Å–∏ y** ‚Äî —Ç–æ–∫–µ–Ω—ã, **–∫—É–¥–∞** –∑–∞–ø–∏—Å—ã–≤–∞–µ—Ç—Å—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–æ—Ö–æ–∂–¥–µ–Ω–∏—è —Å–ª–æ—è.\n",
        "\n",
        "–¢–∞–∫ –¥–ª—è —Å–ª–æ–≤–∞ ‚Äú–ø–ª–æ—Ö–æ‚Äù –ø–µ—Ä–≤–∞—è –≥–æ–ª–æ–≤–∞ –≤–Ω–∏–º–∞–Ω–∏—è –ø–µ—Ä–≤–æ–≥–æ —Å–ª–æ—è –±–æ–ª—å—à–µ –≤—Å–µ–≥–æ —Å–º–æ—Ç—Ä–∏—Ç –Ω–∞ —Å–ª–æ–≤–∞ ‚Äú—Ñ–∏–ª—å–º‚Äù –∏ ‚Äú –æ—Ç–∫—Ä–æ–≤–µ–Ω–Ω–æ‚Äù."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1T7jtnjYzXtD"
      },
      "source": [
        "–î–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –≤–Ω–∏–º–∞–Ω–∏—è –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–∏–±–ª–∏–æ—Ç–µ–∫—É [BertViz üõ†Ô∏è[doc]](https://pypi.org/project/bertviz/):\n",
        "* [[arxiv] üéì Visualizing Attention in Transformer-Based Language Representation Models (Vig, 2019)](https://arxiv.org/pdf/1904.02679.pdf),\n",
        "* [[colab] ü•® BertViz Interactive Tutorial](https://colab.research.google.com/drive/1hXIQ77A4TYS4y3UthWF-Ci7V7vVUoxmQ?usp=sharing)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SqKeBR-pzXtD"
      },
      "outputs": [],
      "source": [
        "!pip install -q bertviz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEE_yxvczXtD"
      },
      "source": [
        "–¢—É—Ç Layer ‚Äî —ç—Ç–æ –≤—ã–±–æ—Ä —Å–ª–æ—è, —Ü–≤–µ—Ç–∞ ‚Äî –≥–æ–ª–æ–≤—ã self-attention, —Å–ª–µ–≤–∞ ‚Äî –∫—É–¥–∞ –∑–∞–ø–∏—Å—ã–≤–∞–µ—Ç—Å—è, —Å–ø—Ä–∞–≤–∞ ‚Äî –Ω–∞ –∫–∞–∫–∏–µ —Ç–æ–∫–µ–Ω—ã —Å–º–æ—Ç—Ä–∏—Ç. –Ø—Ä–∫–æ—Å—Ç—å —Å–æ–µ–¥–∏–Ω—è—é—â–∏—Ö –ª–∏–Ω–∏–π ‚Äî –≤–µ–ª–∏—á–∏–Ω–∞ attention (—á–µ–º —è—Ä—á–µ, —Ç–µ–º –±–æ–ª—å—à–µ). –ö–∞—Ä—Ç–∏–Ω—É, –∞–Ω–∞–ª–æ–≥–∏—á–Ω—É—é –∫–∞—Ä—Ç–∏–Ω–µ –≤—ã—à–µ, –º–æ–∂–Ω–æ –ø–æ–ª—É—á–∏—Ç—å, *–¥–≤–∞–∂–¥—ã —â–µ–ª–∫–Ω—É–≤ –Ω–∞ –ø–µ—Ä–≤—ã–π —Å–∏–Ω–∏–π –∫–≤–∞–¥—Ä–∞—Ç*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FqdzsowizXtD"
      },
      "outputs": [],
      "source": [
        "import bertviz\n",
        "from bertviz import head_view\n",
        "\n",
        "head_view(model_output.attentions, tokens[item])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dh6zHq-pzXtD"
      },
      "source": [
        "–£—Å—Ä–µ–¥–Ω–∏–º –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ –≥–æ–ª–æ–≤–∞–º:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-6_fZiYMzXtD"
      },
      "outputs": [],
      "source": [
        "attention_head_mean = attentions_arr.mean(axis=1)\n",
        "print(f\"{attention_head_mean.shape} [layers x seq_size x seq_size]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lRC4jdYzXtD"
      },
      "source": [
        "–ü–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ —É—Å—Ä–µ–¥–Ω–µ–Ω–Ω–æ–µ –ø–æ –≥–æ–ª–æ–≤–∞–º –≤–Ω–∏–º–∞–Ω–∏–µ **–Ω–∞ –ø–µ—Ä–≤–æ–º —Å–ª–æ–µ**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "31BVgKrRzXtD"
      },
      "outputs": [],
      "source": [
        "x_ticks = tokens[item]\n",
        "y_ticks = tokens[item]\n",
        "\n",
        "sns.heatmap(\n",
        "    data=attention_head_mean[0],\n",
        "    vmin=0,\n",
        "    vmax=1,\n",
        "    xticklabels=x_ticks,\n",
        "    yticklabels=y_ticks,\n",
        "    cmap=\"YlOrRd\",\n",
        ")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Td7GystzXtE"
      },
      "source": [
        "–ò –Ω–∞ **–ø–æ—Å–ª–µ–¥–Ω–µ–º —Å–ª–æ–µ**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "flcANcLrzXtE"
      },
      "outputs": [],
      "source": [
        "x_ticks = tokens[item]\n",
        "y_ticks = tokens[item]\n",
        "\n",
        "sns.heatmap(\n",
        "    data=attention_head_mean[-1],\n",
        "    vmin=0,\n",
        "    vmax=1,\n",
        "    xticklabels=x_ticks,\n",
        "    yticklabels=y_ticks,\n",
        "    cmap=\"YlOrRd\",\n",
        ")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qk2DasfzzXtE"
      },
      "source": [
        "–í–∏–¥–Ω–æ, —á—Ç–æ –Ω–∞ –ø–æ—Å–ª–µ–¥–Ω–µ–º —Å–ª–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–æ –º–µ–∂–¥—É —Ç–æ–∫–µ–Ω–∞–º–∏ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏ —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2mmzCghzXtE"
      },
      "source": [
        "–ü–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ **–∑–Ω–∞—á–µ–Ω–∏—è –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è –∑–∞–ø–∏—Å—ã–≤–∞–µ–º–æ–≥–æ –≤ —ç–º–±–µ–¥–¥–∏–Ω–≥ [CLS] —Ç–æ–∫–µ–Ω–∞** (—ç–º–±–µ–¥–¥–∏–Ω–≥ —Å –Ω–µ–≥–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZXOojn5CzXtE"
      },
      "outputs": [],
      "source": [
        "x_ticks = tokens[item][1:-1]\n",
        "y_ticks = [i for i in range(12, 0, -1)]\n",
        "\n",
        "sns.heatmap(\n",
        "    data=np.flip(attention_head_mean[:, 0, 1:-1], axis=0),\n",
        "    xticklabels=x_ticks,\n",
        "    yticklabels=y_ticks,\n",
        "    cmap=\"YlOrRd\",\n",
        ")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqedFztkzXtE"
      },
      "source": [
        "–í–∏–¥–Ω–æ, —á—Ç–æ –ø–æ—Å–ª–µ 6-–≥–æ —Å–ª–æ—è –∑–Ω–∞—á–µ–Ω–∏—è –≤–Ω–∏–º–∞–Ω–∏—è –≤—ã—Ä–∞–≤–Ω–∏–≤–∞—é—Ç—Å—è. –≠—Ç–æ —Å–≤—è–∑–∞–Ω–æ —Å —Ç–µ–º, —á—Ç–æ –º–µ—Ö–∞–Ω–∏–∑–º **self-attention** —Å–º–µ—à–∏–≤–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–æ–∫–µ–Ω–∞—Ö."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQNPxsb9zXtE"
      },
      "source": [
        "<font size=\"5\">Residual connection</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHayfdx4zXtE"
      },
      "source": [
        "–î–∞–≤–∞–π—Ç–µ —Å–Ω–∞—á–∞–ª–∞ –æ–ø—Ä–µ–¥–µ–ª–∏–º—Å—è, —á—Ç–æ –¥–µ–ª–∞—Ç—å —Å **residual —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è–º–∏**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59XZb_67zXtE"
      },
      "source": [
        "**Residual —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ** –º–æ–∂–Ω–æ –∑–∞–ø–∏—Å–∞—Ç—å –∫–∞–∫\n",
        "\n",
        "$$ \\large V_{l+1} = V_l + W_{att}V_l,$$\n",
        "\n",
        "–≥–¥–µ $W_{att}$ ‚Äî –º–∞—Ç—Ä–∏—Ü–∞ –≤–Ω–∏–º–∞–Ω–∏—è, –∞ $V_l$ ‚Äî —ç–º–±–µ–¥–¥–∏–Ω–≥–∏.\n",
        "\n",
        "–ü–æ—Å–ª–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ —ç—Ç–æ –º–æ–∂–Ω–æ –ø–µ—Ä–µ–ø–∏—Å–∞—Ç—å –∫–∞–∫\n",
        "\n",
        "$$\\large A=0.5W_{att}+0.5I,$$\n",
        "\n",
        "–≥–¥–µ $I$ ‚Äî –µ–¥–∏–Ω–∏—á–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞.\n",
        "\n",
        "[[arxiv] üéì Quantifying Attention Flow in Transformers (Abnar, Zuidema, 2020)](https://arxiv.org/pdf/2005.00928.pdf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B1cSBMz2zXtF"
      },
      "outputs": [],
      "source": [
        "def residual(attention_head_mean):\n",
        "    attention_residual = (\n",
        "        0.5 * attention_head_mean\n",
        "        + 0.5 * np.eye(attention_head_mean.shape[1])[None, ...]\n",
        "    )\n",
        "    return attention_residual\n",
        "\n",
        "\n",
        "attention_res = residual(attention_head_mean)\n",
        "\n",
        "x_ticks = tokens[item][1:-1]\n",
        "y_ticks = [i for i in range(12, 0, -1)]\n",
        "\n",
        "sns.heatmap(\n",
        "    data=np.flip(attention_res[:, 0, 1:-1], axis=0),\n",
        "    xticklabels=x_ticks,\n",
        "    yticklabels=y_ticks,\n",
        "    cmap=\"YlOrRd\",\n",
        ")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ivkg_Y5RzXtF"
      },
      "source": [
        "## Attention rollout\n",
        "‚Äú–†–∞–∑–≤–æ—Ä–∞—á–∏–≤–∞–Ω–∏–µ –≤–Ω–∏–º–∞–Ω–∏—è‚Äù (**Attention rollout**) ‚Äî –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –≤ [—Å—Ç–∞—Ç—å–µ üéì[arxiv]](https://arxiv.org/pdf/2005.00928.pdf) —Å–ø–æ—Å–æ–± –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω—è–µ–º–æ–π –æ—Ç –≤—Ö–æ–¥–Ω–æ–≥–æ –∫ –≤—ã—Ö–æ–¥–Ω–æ–º—É –±–ª–æ–∫—É, –≤ –∫–æ—Ç–æ—Ä–æ–º –∑–Ω–∞—á–µ–Ω–∏–µ –≤–Ω–∏–º–∞–Ω–∏—è —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –∫–∞–∫ –¥–æ–ª—è –ø—Ä–æ–ø—É—Å–∫–∞–µ–º–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏.  –î–æ–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø–µ—Ä–µ–º–Ω–æ–∂–∞—é—Ç—Å—è –∏ —Å—É–º–º–∏—Ä—É—é—Ç—Å—è. –ò—Ç–æ–≥–æ–≤–∞—è —Ñ–æ—Ä–º—É–ª–∞ ‚Äî —Ä–µ–∫—É—Ä—Å–∏–≤–Ω–æ–µ –º–∞—Ç—Ä–∏—á–Ω–æ–µ –ø–µ—Ä–µ–º–Ω–æ–∂–µ–Ω–∏–µ.\n",
        "\n",
        "\\begin{align}\n",
        "\\widetilde{A}(l_i) = \\left\\{\n",
        "\\begin{array}{cl}\n",
        "A(l_i)\\widetilde{A}(l_{i-1}) & i>0 \\\\\n",
        "A(l_i) & i = 0.\n",
        "\\end{array}\n",
        "\\right.\n",
        "\\end{align}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aeTtwz4vzXtF"
      },
      "outputs": [],
      "source": [
        "def rollout(attention_res):\n",
        "    rollout_attention = np.zeros(attention_res.shape)\n",
        "    rollout_attention[0] = attention_res[0]\n",
        "    n_layers = attention_res.shape[0]\n",
        "    for i in range(1, n_layers):\n",
        "        rollout_attention[i] = attention_res[i].dot(rollout_attention[i - 1])\n",
        "    return rollout_attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tk1RKnqBzXtF"
      },
      "outputs": [],
      "source": [
        "rollout_attention = rollout(attention_res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o_QOpZyPzXtF"
      },
      "outputs": [],
      "source": [
        "x_ticks = tokens[item][1:-1]\n",
        "y_ticks = [i for i in range(12, 0, -1)]\n",
        "\n",
        "sns.heatmap(\n",
        "    data=np.flip(rollout_attention[:, 0, 1:-1], axis=0),\n",
        "    xticklabels=x_ticks,\n",
        "    yticklabels=y_ticks,\n",
        "    cmap=\"YlOrRd\",\n",
        ")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEuwtuJUzXtF"
      },
      "source": [
        "–†–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –æ—Ç HuggingFace:\n",
        "* [[git] üêæ –î–ª—è ViT –∏ –∫–∞—Ä—Ç–∏–Ω–æ–∫](https://huggingface.co/spaces/probing-vits/attention-rollout/tree/main)\n",
        "* [[git] üêæ –ù–∞ PyTorch Rollout –¥–ª—è BERT –∏ –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö –¥—Ä—É–≥–∏—Ö –º–µ—Ç–æ–¥–æ–≤](https://huggingface.co/spaces/amsterdamNLP/attention-rollout/tree/main)\n",
        "* [[demo] üéÆ Attention Rollout ‚Äî RoBERTa](https://huggingface.co/spaces/amsterdamNLP/attention-rollout)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNdYoPX_zXtF"
      },
      "source": [
        "## Attention Flow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLPFfF9RzXtF"
      },
      "source": [
        "–î—Ä—É–≥–∏–º –≤–∞—Ä–∏–∞–Ω—Ç–æ–º —Ä–∞—Å—Å–º–æ—Ç—Ä–µ–Ω–∏—è —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è –≤–Ω–∏–º–∞–Ω–∏—è —è–≤–ª—è–µ—Ç—Å—è **attention flow**. –í –Ω–µ–º —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç –≤ –≤–∏–¥–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –≥—Ä–∞—Ñ–∞, **—É–∑–ª—ã** –∫–æ—Ç–æ—Ä–æ–≥–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç —Å–æ–±–æ–π **—ç–º–±–µ–¥–¥–∏–Ω–≥–∏** –º–µ–∂–¥—É —Å–ª–æ—è–º–∏, –∞ **—Ä–µ–±—Ä–∞** ‚Äî —Å–≤—è–∑–∏ –≤ –≤–∏–¥–µ **attention** —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–π –µ–º–∫–æ—Å—Ç—å—é (–ø–µ—Ä–µ–¥–∞—é—â–µ–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å—é)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qerQt9FqzXtG"
      },
      "source": [
        "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.1/L14/attention_flow.png\" width=\"700\">\n",
        "\n",
        "<em>Source: <a href=\"https://github.com/samiraabnar/attention_flow/blob/master/bert_example.ipynb\">Bert Example\n",
        "</a></em></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Bll_a64zXtG"
      },
      "source": [
        "–í —Ç–∞–∫–æ–π –ø–æ—Å—Ç–∞–Ω–æ–≤–∫–µ –∑–∞–¥–∞—á–∞ –Ω–∞—Ö–æ–∂–¥–µ–Ω–∏—è —Ä–æ–ª–∏ —Ç–æ–∫–µ–Ω–æ–≤/—á–∞—Å—Ç–µ–π –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ —Ä–µ–∑—É–ª—å—Ç–∏—Ä—É—é—â–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–µ —Å–≤–æ–¥–∏—Ç—Å—è –∫ [–∑–∞–¥–∞—á–µ –æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ üìö[wiki]](https://ru.wikipedia.org/wiki/%D0%97%D0%B0%D0%B4%D0%B0%D1%87%D0%B0_%D0%BE_%D0%BC%D0%B0%D0%BA%D1%81%D0%B8%D0%BC%D0%B0%D0%BB%D1%8C%D0%BD%D0%BE%D0%BC_%D0%BF%D0%BE%D1%82%D0%BE%D0%BA%D0%B5) (–∑–∞–¥–∞—á–∞ –Ω–∞—Ö–æ–∂–¥–µ–Ω–∏—è —Ç–∞–∫–æ–≥–æ –ø–æ—Ç–æ–∫–∞ –ø–æ —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–Ω–æ–π —Å–µ—Ç–∏, —á—Ç–æ —Å—É–º–º–∞ –ø–æ—Ç–æ–∫–æ–≤ –≤ –ø—É–Ω–∫—Ç –Ω–∞–∑–Ω–∞—á–µ–Ω–∏—è –±—ã–ª–∞ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞). –≠—Ç–æ –∏–∑–≤–µ—Å—Ç–Ω–∞—è –∞–ª–≥–æ—Ä–∏—Ç–º–∏—á–µ—Å–∫–∞—è –∑–∞–¥–∞—á–∞, –∫–æ—Ç–æ—Ä—É—é –º—ã –Ω–µ –±—É–¥–µ–º —Ä–∞–∑–±–∏—Ä–∞—Ç—å –≤ —Ä–∞–º–∫–∞—Ö —ç—Ç–æ–≥–æ –∫—É—Ä—Å–∞.\n",
        "\n",
        "* [[git] üêæ Attention flow](https://github.com/samiraabnar/attention_flow)\n",
        "* [[arxiv] üéì Quantifying Attention Flow in Transformers (Abnar, Zuidema, 2020)](https://arxiv.org/pdf/2005.00928.pdf)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aActgHQwzXtG"
      },
      "source": [
        "## Gradient-weighted Attention Rollout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRATmUr2zXtG"
      },
      "source": [
        "–ú–µ—Ç–æ–¥, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∏–π GradCam –∏ Attention Rollout, –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ—Ü–µ–Ω–∏—Ç—å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π –∏ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–π –≤–∫–ª–∞–¥ —Ç–æ–∫–µ–Ω–æ–≤ / —á–∞—Å—Ç–µ–π –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ –∏—Ç–æ–≥–æ–≤—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç.\n",
        "* [[article] üéì Transformer Interpretability Beyond Attention Visualization (Chefer et.al., 2021)](https://openaccess.thecvf.com/content/CVPR2021/papers/Chefer_Transformer_Interpretability_Beyond_Attention_Visualization_CVPR_2021_paper.pdf)\n",
        "* [[git] üêæ Attention Rollout](https://huggingface.co/spaces/amsterdamNLP/attention-rollout/tree/main)\n",
        "* [[demo] üéÆ Attention Rollout ‚Äî RoBERTa](https://huggingface.co/spaces/amsterdamNLP/attention-rollout)\n",
        "\n",
        "–ú–µ—Ç–æ–¥—ã, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞ –º–µ—Ö–∞–Ω–∏–∑–º–µ –≤–Ω–∏–º–∞–Ω–∏—è, —Ç–æ–ª—å–∫–æ —á–∞—Å—Ç–∏—á–Ω–æ –æ–±—ä—è—Å–Ω—è—é—Ç —Ä–∞–±–æ—Ç—É —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ –∏ –Ω–∞ —Å–µ–≥–æ–¥–Ω—è—à–Ω–∏–π –¥–µ–Ω—å –Ω–µ —è–≤–ª—è—é—Ç—Å—è –Ω–∞–¥–µ–∂–Ω—ã–º –º–µ—Ç–æ–¥–æ–º –æ—Ü–µ–Ω–∫–∏ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤."
      ]
    }
  ]
}