{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/github.com/Gan4x4/ml_snippets/blob/main/Training/Lightning.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обычный цикл обучения в Torch\n",
    "При работе с Pytorch базовый pipeline обучения выглядит примерно так"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms, utils\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.13), (0.3))]\n",
    ")\n",
    "\n",
    "mnist = datasets.MNIST(root=\"./\", train=True, download=True, transform=transform)\n",
    "\n",
    "# Reduce size of dataset to speedup training\n",
    "train_set, val_set, _ = torch.utils.data.random_split(mnist, [10000, 3000, 47000])\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_set, batch_size=256, shuffle=False, num_workers=2\n",
    ")\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set, batch_size=256, shuffle=True, num_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создание модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.core = nn.Sequential(\n",
    "            nn.Flatten(), nn.Linear(28 * 28, 256), nn.ReLU(), nn.Linear(256, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.core(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Код для валидации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "@torch.inference_mode()  # this annotation disable grad computation\n",
    "def validate(model, test_loader, device):\n",
    "    correct, total = 0, 0\n",
    "    for imgs, labels in test_loader:\n",
    "        pred = model(imgs.to(device))\n",
    "        total += labels.size(0)\n",
    "        _, predicted = torch.max(pred.data, 1)  # shape = batch_size, class_count\n",
    "        correct_predictions = (predicted.cpu() == labels.cpu()).sum()\n",
    "        correct += correct_predictions.sum().item()\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение (train loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# managing device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = SimpleModel()\n",
    "model.to(device)\n",
    "\n",
    "# define optimizer and loss\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)  # Weight update\n",
    "criterion = nn.CrossEntropyLoss()  # Loss function\n",
    "epochs = 3\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for batch in train_loader:\n",
    "        # Processing one batch\n",
    "        imgs, labels = batch\n",
    "        optimizer.zero_grad()\n",
    "        out = model(imgs)\n",
    "        loss = criterion(out, labels.to(device))\n",
    "        loss.backward()\n",
    "\n",
    "        # Calclulate metrics: TODO\n",
    "        # Save metrics to logs:  TODO\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation step\n",
    "    print(f\"Epoch {epoch} accuracy: {validate(model,val_loader,device):.2f}\")\n",
    "    # Save checkpoint: TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.13), (0.3))]\n",
    ")\n",
    "testset = datasets.MNIST(\n",
    "    root=\"./\", train=False, download=True, transform=test_transform\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=512, shuffle=False)\n",
    "accuracy = validate(model, test_loader, device)\n",
    "\n",
    "print(f\"Accuracy on TEST {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lightning\n",
    "\n",
    "При обучении моделей в pytorch нам часто приходиться переписовать цикл обучения (train loop) это дублирование кода, которое нарушает принцип [DRY](https://en.wikipedia.org/wiki/Don%27t_repeat_yourself).\n",
    "\n",
    "Кроме того нам нужно следить за процессом обучения модели, например если loss взрываться или выходит на плато как правило есть смысл остановить обучение. Чтобы контролировать этот процесс приходиться добавлять дополнительный код для вывода и/или логгирования метрик.\n",
    "\n",
    "При проведении реальных экспериментов логирование результатов станет необходимым. Фреймворк ([Lightning](https://lightning.ai/)) облегчает написание tain loop, логирование результатов, и выполняет за нас ряд других задач."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Первым делом зафиксируем все seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "\n",
    "L.seed_everything(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train loop в Lightning\n",
    "Базовая задача которую решает Lightning это реализация train loop.\n",
    "\n",
    "Типичный цикл обучения разбит на фрагменты каждый из которых помещен в соответствующий метод класса LightningModule. Посмотрим на послдовательность их вызовов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitDemo(L.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        print(\"#\\t configure_optimizers\")\n",
    "        # return optimizer\n",
    "\n",
    "    def on_train_epoch_start(self):\n",
    "        print(\"#\\t on_train_epoch_start\")\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # print(\"#\\t training_step\")\n",
    "        pass\n",
    "        # return loss\n",
    "\n",
    "    def on_validation_epoch_start(self):\n",
    "        # called only if validation_step implemented\n",
    "        print(\"#\\t on_validation_epoch_start\")\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # print(\"#\\t validation_step\")\n",
    "        pass\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        print(\"#\\t on_validation_epoch_end\")\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        print(\"#\\t on_train_epoch_end\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что бы воспользоваться таким модулем надо передать его в объект класса Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L.seed_everything(42)\n",
    "lit_model = LitDemo()\n",
    "trainer = L.Trainer(max_epochs=1)\n",
    "trainer.fit(model=lit_model, train_dataloaders=train_loader, val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, один цикл валидации запускается до до начала эпохи обучения, а затем повторяеттся внутри каждой эпохи.\n",
    "\n",
    "Отключить первый вызов валидации можно инициализировав Trainer c параметром num_sanity_val_steps=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_model = LitDemo()\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=1, num_sanity_val_steps=0  # disable vlidation before first epoch\n",
    ")\n",
    "trainer.fit(model=lit_model, train_dataloaders=train_loader, val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Перепишем цикл обучения из на Lightning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель мы можем не менять, достаточно сохранить на нее ссылку при инициализации.\n",
    "Также инициализировать оптимизатор и перенести чать кода в train_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitMinimal(L.LightningModule):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.SGD(self.parameters(), lr=0.01, momentum=0.9)\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        out = self.model(x)\n",
    "        loss = self.criterion(out, y)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При создании оптимизатора мы передаем ему не параметры модели, а параметры всего модуля, поэтом не важно как будет называться свойство содержащее ссылку на модель.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_model = LitMinimal(model)\n",
    "trainer = L.Trainer(max_epochs=1)\n",
    "trainer.fit(model=lit_model, train_dataloaders=train_loader, val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Код выше минимально необходимый для обучения, он незначительно упрощает создание train_loop\n",
    "\n",
    "При этом фреймворк самостоятельно:\n",
    "- обновляет веса модели\n",
    "- сохраняет checkpoints на диск\n",
    "\n",
    " в нем нет методов для оценки точности или вывода графика loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torchmetrics\n",
    "\n",
    "Для выисления метрик установим пакет [torchmetrics](https://torchmetrics.readthedocs.io/en/stable/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchmetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метрики это объекты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmetrics\n",
    "\n",
    "accuracy_metric = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10)\n",
    "# Basic usage\n",
    "preds = torch.tensor([1.0, 2.0, 3.0])\n",
    "labels = torch.tensor([1, 2, 9])\n",
    "print(\"Accuracy\", accuracy_metric(preds, labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Они могут накапливать данные а потом вычислять заначение метрики."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy\", accuracy_metric.compute())  # old values stored in memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если они не нужны следует их очистить"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_metric.reset()  # lear old values\n",
    "print(\"Accuracy\", accuracy_metric.compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обычно нужно делать это в конце эпохи, так как многие метрики считаются за эпоху"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    preds = torch.randint(0, 10, (256, 10)).float()  # batch predictions\n",
    "    labels = torch.randint(0, 10, (256,))  # batch labels\n",
    "    accuracy_metric.update(preds, labels)\n",
    "\n",
    "print(\"Accuracy\", accuracy_metric.compute())\n",
    "accuracy_metric.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Логиррование в Lightning\n",
    "\n",
    "Добавим подсчет метрики в lightning модуль.\n",
    "\n",
    "\n",
    "Будем добавлять значения в метрику при обработке каждого batch.\n",
    "\n",
    "А считать значение метрики будем в конце каждой эпохи обучения.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Метод log\n",
    "Для сохранения значений (метрик и любых других) в lightning модуле реализован метод `log`. Используем его так же для согранения loss на каждом batch.\n",
    "\n",
    "Что бы последнее значение отображалось в progress bar установим параметр `prog_bar = True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitWithMetric(LitMinimal):\n",
    "    def __init__(self, model):\n",
    "        super().__init__(model)\n",
    "        self.metric = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        out = self.model(x)\n",
    "        loss = self.criterion(out, y)\n",
    "        self.metric.update(out, y)\n",
    "        self.log(\"loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        self.log(\"accuracy/train\", self.metric.compute(), prog_bar=True)\n",
    "        self.metric.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleModel()\n",
    "lit_model = LitWithMetric(model)\n",
    "trainer = L.Trainer(max_epochs=3)  # def on_validation_epoch_start(self):\n",
    "trainer.fit(model=lit_model, train_dataloaders=train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Просмотр логов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На диске должен был появиться каталог `lightning_logs` в нем Lightning сохранил значения метрик которые мы передавали в метод log.\n",
    "\n",
    "По умолчанию используется формат логов [Tensorboard](https://github.com/Gan4x4/ml_snippets/blob/main/Training/Tensorboard.ipynb) но можно использовать и другой logger, например [WandB](https://lightning.ai/docs/pytorch/stable/extensions/generated/lightning.pytorch.loggers.WandbLogger.html#lightning.pytorch.loggers.WandbLogger)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colab magic command, run only once\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При запуске Tensorboad мы должны указать путь к каталогу с логами:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %reload_ext tensorboard\n",
    "\n",
    "%tensorboard --logdir lightning_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation\n",
    "\n",
    "Тепперь добавим методы для валидации. Метрику можно было бы оставить и одну, но что бы не запутаться и не забыть ее очистить сосдадим для валидации новую метрику."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitWithVal(LitWithMetric):\n",
    "    def __init__(self, model):\n",
    "        super().__init__(model)\n",
    "        self.val_metric = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        out = self.model(x)\n",
    "        self.val_metric.update(out, y)\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        self.log(\"accuracy/val\", self.val_metric.compute(), prog_bar=True)\n",
    "        self.val_metric.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так же можно использовать функцию для фиксации seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L.seed_everything(42)\n",
    "model = SimpleModel()\n",
    "lit_model = LitWithVal(model)\n",
    "trainer = L.Trainer(max_epochs=3)  # def on_validation_epoch_start(self):\n",
    "trainer.fit(model=lit_model, train_dataloaders=train_loader, val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test\n",
    "\n",
    "Аналогично добавляются методы для прогона на тестовом датасете. Так как тестовый прогон запускается независимо от обучающего, можем использовать уже имеющуюся в классе метрику.\n",
    "\n",
    "https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.core.LightningModule.html#lightning.pytorch.core.LightningModule.test_step\n",
    "\n",
    "https://pytorch-lightning.readthedocs.io/en/1.4.9/common/test_set.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitWithTest(LitWithVal):\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        out = self.model(x)\n",
    "        self.metric.update(out, y)\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        self.log(\"accuracy/test\", self.metric.compute(), prog_bar=True)\n",
    "        self.metric.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сначала обучим:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleModel()\n",
    "lit_model = LitWithTest(model)\n",
    "trainer = L.Trainer(max_epochs=3)  # def on_validation_epoch_start(self):\n",
    "trainer.fit(model=lit_model, train_dataloaders=train_loader, val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Затем протестируем вызвав метод `test` у объекта trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(model=lit_model, dataloaders=test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment naming\n",
    "\n",
    "Что вы различать эксперименты, нужно инициализировать logger вручную.\n",
    "\n",
    "\n",
    "https://lightning.ai/docs/pytorch/stable/extensions/generated/lightning.pytorch.loggers.TensorBoardLogger.html#lightning.pytorch.loggers.TensorBoardLogger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorBoardLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "\n",
    "logger = TensorBoardLogger(\"lightning_logs\", name=\"MNIST\")\n",
    "trainer = L.Trainer(logger=logger, max_epochs=4, num_sanity_val_steps=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log two variable in one axxis\n",
    "\n",
    "Используя объект логгера можно получить доступ к его специфическим методам, например отображать два графика в одних осях."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/66287075/pytorch-lightning-multiple-scalars-e-g-train-and-valid-loss-in-same-tensorbo\n",
    "\n",
    "\n",
    "class LitTB(LitWithVal):\n",
    "    def on_validation_epoch_end(self):\n",
    "        # do nothing\n",
    "        pass\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        train_acc = self.metric.compute()\n",
    "        val_acc = self.val_metric.compute()\n",
    "        # self.log(\"accuracy/test\", self.metric.compute(), prog_bar=True)\n",
    "        self.logger.experiment.add_scalars(\n",
    "            \"Accuracy\", {\"val\": val_acc, \"train\": train_acc}, self.global_step\n",
    "        )\n",
    "        self.metric.reset()\n",
    "        self.val_metric.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleModel()\n",
    "lit_model = LitTB(model)\n",
    "trainer.fit(model=lit_model, train_dataloaders=train_loader, val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для WanDB\n",
    "https://lightning.ai/docs/pytorch/stable/extensions/generated/lightning.pytorch.loggers.WandbLogger.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from lightning.pytorch.loggers import WandbLogger\n",
    "\n",
    "# wandb_logger = WandbLogger(project=\"MNIST\")\n",
    "# trainer = Trainer(logger=wandb_logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "           log_every_n_steps = 1,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint\n",
    "\n",
    "По умолчанию lightning периодически сохраняет веса модели и [ряд свойств](https://lightning.ai/docs/pytorch/stable/common/checkpointing_basic.html) `LightningModule` таких как номер эпохи или веса оптимизатора на диск.\n",
    "\n",
    "По умолчанию данные сохраняются в каталог `lightning_logs/version_xx/checkpoints/`\n",
    "\n",
    "Что бы загружать сохраненные состояния **необходимо** добавить в конструктор модуля вызов метода `self.save_hyperparameters()`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitCkpt(LitWithTest):\n",
    "    def __init__(self, model):\n",
    "        super().__init__(model)\n",
    "        self.save_hyperparameters(logger=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleModel()\n",
    "lit_model = LitCkpt(model)\n",
    "trainer = L.Trainer(max_epochs=4, num_sanity_val_steps=0)\n",
    "trainer.fit(model=lit_model, train_dataloaders=train_loader, val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При помощи этих файлов можно восстановить как состояние всего lightning модуля, так и модели.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "# Find last checkpoint\n",
    "ckpt_list = glob(\"lightning_logs/version_*/checkpoints/*\")\n",
    "ckpt_list.sort()\n",
    "ckpt_path = ckpt_list[-1]\n",
    "print(ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleModel()\n",
    "lit = LitTB.load_from_checkpoint(ckpt_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "так и torch модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dict = torch.load(ckpt_path)\n",
    "model = checkpoint_dict[\"hyper_parameters\"][\"model\"]\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "smoke test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(torch.randn(1, 1, 28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно продолжить обучение с определенной точки передав путь к к каталогу с checkpoint в `trainer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleModel()\n",
    "lit_model = LitCkpt(model)\n",
    "trainer = L.Trainer(max_epochs=4 + 4, num_sanity_val_steps=0)\n",
    "# trainer.fit(model=lit_model, train_dataloaders=train_loader, val_dataloaders= val_loader)\n",
    "# automatically restores model, epoch, step, LR schedulers, etc...\n",
    "trainer.fit(\n",
    "    lit_model,\n",
    "    ckpt_path=ckpt_path,\n",
    "    train_dataloaders=train_loader,\n",
    "    val_dataloaders=val_loader,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save best model\n",
    "\n",
    "Можно сохранять только определенные состояния модели например те в которых значение метрики было лучшим. Для этого используется механизм `callback` с совместно с классом `ModelCheckpoint`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from L.pytorch.callbacks import ModelCheckpoint #- don't work\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    # dirpath='best_ckp/'\n",
    "    save_last=True,\n",
    "    every_n_epochs=1,\n",
    "    save_top_k=1,\n",
    "    save_weights_only=True,\n",
    "    monitor=\"accuracy/val\",\n",
    "    filename=\"model\",\n",
    "    mode=\"max\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем сначала переобучить модель:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleModel()\n",
    "lit_model = LitCkpt(model)\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=20,\n",
    "    callbacks=[checkpoint_callback],\n",
    "    num_sanity_val_steps=0,\n",
    "    log_every_n_steps=4,\n",
    ")\n",
    "trainer.fit(model=lit_model, train_dataloaders=train_loader, val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А затем протестируем, передав в trainer параметр `ckpt_path=\"best\"`  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(model=lit_model, dataloaders=val_loader, ckpt_path=\"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shadow work\n",
    "\n",
    "\n",
    "* managing devices\n",
    "* creating checkpoints\n",
    "* finding LR\n",
    "\n",
    "Standartize\n",
    "\n",
    "* train loop creation\n",
    "* logging\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- переименовывать ключи\n",
    "https://lightning.ai/docs/pytorch/stable/common/checkpointing_basic.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Learning rate\n",
    "\n",
    "https://lightning.ai/docs/pytorch/2.1.0/advanced/training_tricks.html#learning-rate-finder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дополнительно"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
