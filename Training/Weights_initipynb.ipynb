{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Apvv_ehT21Kk"
      },
      "source": [
        "# Инициализация весов"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGX6NzDD21Kl"
      },
      "source": [
        "Одним из способов борьбы с затухающим градиентом является правильная инициализация весов. Как это сделать?\n",
        "\n",
        "**Идея 1:** инициализировать все веса константой. \n",
        "\n",
        "Проблема: градиент во всем весам будет одинаков, как и обновление весов. Все нейроны в слое будут учить одно и то же, или, в случае $const = 0$, [не будут учиться вообще](https://habr.com/ru/post/592711/).\n",
        "\n",
        "\n",
        "Вывод: в качестве начальных весов нужно выбирать различные значения.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHxkJrtw21Kl"
      },
      "source": [
        "**Идея 2:** инициализировать веса нормальным (Гауссовским) шумом с матожиданием 0 и маленькой дисперсией. \n",
        "\n",
        "Маленькая дисперсия нужна, чтобы не получить огромные градиенты за большие изначальные ошибки в предсказании."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9ObwD7621Km"
      },
      "outputs": [],
      "source": [
        "# Normal distribution: mu = 0, sigma = 1\n",
        "\n",
        "x = np.arange(-4, 4.1, 0.1)\n",
        "y = np.exp(-np.square(x)/2) / np.sqrt(2*np.pi)\n",
        "\n",
        "plt.style.use('seaborn-whitegrid')\n",
        "plt.title('Normal distribution: mu = 0, sigma = 1', size=15)\n",
        "plt.plot(x, y)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twK3UiwJ21Km"
      },
      "source": [
        "Проблема: инициализация нормальным шумом не гарантирует отсутствие взрыва или затухания градиета."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSv4za1021Kn"
      },
      "source": [
        "**Идея 3:** формализуем условия при которых не будет происходить взрыв градиентов.\n",
        "\n",
        "Нам бы хотелось, чтобы \n",
        "дисперсии признаков, получаемых на каждом слое были бы одинаковы: \n",
        "\n",
        "$$Dz^i = Dz^j. \\tag{1}$$\n",
        "\n",
        "И чтобы  начальные дисперсии градиентов для разных слоев были бы одинаковы:\n",
        " \n",
        "$$D\\dfrac {\\delta L} {\\delta z^i} = D\\dfrac {\\delta L} {\\delta z^j}. \\tag{2}$$\n",
        "\n",
        "При выполнении этих условий градиент не затухает и не взрывается.\n",
        "\n",
        "Попытаемся выполнить эти условия."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgzjHj1_21Ko"
      },
      "source": [
        "### Вывод Xavier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYA1B-y121Ko"
      },
      "source": [
        "Рассмотрим функцию активации гиперболический тангенс (tanh).\n",
        "<img src =\"https://edunet.kea.su/repo/EduNet-content/L07/out/simple_nn_with_tanh.png\" width=\"600\">\n",
        "\n",
        "Это - [нечетная функция](https://ru.wikipedia.org/wiki/%D0%A7%D1%91%D1%82%D0%BD%D0%BE%D1%81%D1%82%D1%8C_%D1%84%D1%83%D0%BD%D0%BA%D1%86%D0%B8%D0%B8) с единичной производной в нуле. Функция и ее производная изображены ниже."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DoywHpgv21Ko"
      },
      "outputs": [],
      "source": [
        "x = np.arange(-10, 10.1, 0.1)\n",
        "y = np.tanh(x)\n",
        "dy = 1 / np.cosh(x)\n",
        "\n",
        "plt.style.use('seaborn-whitegrid')\n",
        "fig, (im1, im2) = plt.subplots(2, 1, figsize=(7, 7))\n",
        "im1.set(title = \"tanh(x)\")\n",
        "im1.plot(x[0:51], y[0:51], 'r', x[50:96], y[50:96], 'b', \n",
        "         x[95:106], y[95:106], 'g', x[105:151], y[105:151], 'b', \n",
        "         x[150:201], y[150:201], 'r') \n",
        "im2.set(title = \"tanh'(x)\")\n",
        "im2.plot(x[0:51], dy[0:51], 'r', x[50:96], dy[50:96], 'b',\n",
        "         x[95:106], dy[95:106], 'g', x[105:151], dy[105:151], 'b',\n",
        "         x[150:201], dy[150:201], 'r')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKyfE40B21Kp"
      },
      "source": [
        "При выборе весов нам важно не попасть в красные зоны с почти нулевой производной, т.к. в этих областях градиент затухает. Мы хотим инициализировать веса таким образом, чтобы признаки, поступающие на слой активации, находились в зеленой области в окрестности нуля. Матожидание признаков, поступающих на слой активации, будет равно нулю\n",
        "$$E(z^i_t w_{kt})=0 \\tag{3}.$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1juUVHt21Kp"
      },
      "source": [
        "Выход слоя активации на текущем слое будет зависеть от выхода слоя активации на предыдущем: $$z^{i+1} = f(z^iW^i). \\tag{4}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wN8VODTF21Kp"
      },
      "source": [
        "В окрестности нуля функцию гиперболический тангенс можно считать [линейной](https://ru.wikipedia.org/wiki/%D0%A0%D1%8F%D0%B4_%D0%A2%D0%B5%D0%B9%D0%BB%D0%BE%D1%80%D0%B0):\n",
        "$$z^{i+1} \\approx z^i W^i \\tag{5}.$$\n",
        "Матожидаение выхода функции активации также равно нулю:\n",
        "$$E(z^i_t)=0 \\tag{6}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDLne9cz21Kq"
      },
      "source": [
        "Значения весов в слоях будут генерироваться генератором случайных чисел, поэтому веса в различных слоях можно считать независимыми случайными величинами. Для независимых случайных величин выполняются формулы: \n",
        "1. Дисперсии суммы двух независимых величин:\n",
        "\n",
        "$$D(\\eta + \\gamma) = D\\eta + D\\gamma \\tag{7}.$$\n",
        "\n",
        "2. Дисперсии произведения двух независимых величин:\n",
        "\n",
        "$$D\\eta\\gamma = E(\\eta\\gamma)^2 - (E\\eta\\gamma)^2 = E\\eta^2E\\gamma^2 - (E\\eta)^2(E\\gamma)^2 \\tag{8}.$$ \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hZg0CdJ21Kq"
      },
      "source": [
        "Распишем условие $(1)$ с использованием $(5)$ и $(7)$:\n",
        "\n",
        "$$D(z^{i+1}_{k}) = D(\\sum_t z^i_t w_{kt}) = \\sum_t D(z^i_t w_{kt}) \\tag{9}$$\n",
        "\n",
        "Дисперсии признаков на входе функции активации берем одинаковые:\n",
        "\n",
        "$$D(z^{i+1}_{k}) = n D(z^i_0 w_{k0}),\\tag{10}$$\n",
        "\n",
        "где $n$ - размерность выхода слоя.\n",
        "\n",
        "Применяем нашу формулу $(8)$ и получаем:\n",
        "\n",
        "$$D(z^{i+1}_{k}) = n [E(z^i_0)^2E(w_{k0})^2 - (Ez^i_0)^2(Ew_{k0})^2]. \\tag{11}$$\n",
        "\n",
        "Используем $(6)$:\n",
        "\n",
        "$$D(z^{i+1}_{k}) =   n E(z^i_0)^2E(w_{k0})^2. \\tag{12}$$\n",
        "\n",
        "Матожидание выходов активаций и весов равны 0. Из этого: \n",
        "\n",
        "$$D(z^{i}_{0}) = E(z^{i}_{0})^2 - (Ez^{i}_{0})^2 = E(z^{i}_{0})^2, \\tag{13}$$\n",
        "\n",
        "$$D(w_{k0}) = E(w_{k0})^2 - (Ew_{k0})^2 = E(w_{k0})^2.$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHXAgooK21Kr"
      },
      "source": [
        "Подставляем в $(12)$:\n",
        "\n",
        "$$D(z^{i+1}_{k}) = n D(z^i_0)D(w_{k0}). \\tag{14}$$\n",
        "\n",
        "Из $(14)$ следует формула для зависимости выхода активаций любого слоя от весов предыдущих слоев и дисперсии исходных данных:\n",
        "\n",
        "$$Dz^i = Dx \\prod_{p=0}^{i-1}n_pDW^p, \\tag{15}$$\n",
        "\n",
        "где $n_p$ - размерность выхода слоя p-го слоя.\n",
        "\n",
        "Аналогично можно вывести формулу для градиентов по активациям\n",
        "\n",
        "$$D(\\dfrac {\\delta L} {\\delta z^i}) = D(\\dfrac {\\delta L} {\\delta z^d} ) \\prod_{p=i}^{d}n_{p+1}DW^p. \\tag{16}$$\n",
        "\n",
        "Вспоминаем условия $(1)$, $(2)$:\n",
        "\n",
        "$$Dz^i = Dz^j \\tag{1},$$\n",
        "\n",
        "$$D\\dfrac {\\delta L} {\\delta z^i} = D\\dfrac {\\delta L} {\\delta z^j} \\tag{2}.$$\n",
        "\n",
        "С учетом $(15)$, $(16)$ они эквивалентны условиям:\n",
        "\n",
        "$$n_iDW^i = 1 \\tag{17}$$\n",
        "\n",
        "$$n_{i+1}DW^i = 1 \\tag{18}$$\n",
        "\n",
        "Условия могут быть невыполнимы одновременно:\n",
        "\n",
        " $$n_i \\ne n_{i+1}. $$\n",
        "\n",
        "Возьмем компромисс - среднее гармоническое решений первого и второго уравнения:\n",
        "\n",
        "$$DW^i = \\dfrac 2 {n_i + n_{i+1}} \\tag{19}.$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXe3iE1021Ks"
      },
      "source": [
        "**Итого:** нам нужно инициализировать веса нейронов случайными величинами со следующим матожиданием и дисперсией:\n",
        "\n",
        "$$ EW^i = 0,$$\n",
        "\n",
        "$$DW^i = \\dfrac 2 {n_i + n_{i+1}}.$$\n",
        "\n",
        "При инициализации Xavier используется [равномерное распределение](https://ru.wikipedia.org/wiki/%D0%9D%D0%B5%D0%BF%D1%80%D0%B5%D1%80%D1%8B%D0%B2%D0%BD%D0%BE%D0%B5_%D1%80%D0%B0%D0%B2%D0%BD%D0%BE%D0%BC%D0%B5%D1%80%D0%BD%D0%BE%D0%B5_%D1%80%D0%B0%D1%81%D0%BF%D1%80%D0%B5%D0%B4%D0%B5%D0%BB%D0%B5%D0%BD%D0%B8%D0%B5): \n",
        "\n",
        "$$W_i \\sim U[a, b ],$$\n",
        "\n",
        "где $a=-b$, так как матожидание равно 0. \n",
        "\n",
        "Дисперсия которого выражается формулой: \n",
        "$$D(U[a, b]) = \\dfrac 1 {12} (b -a)^2 = \\dfrac 4 {12} b^2 = \\dfrac 1 {3} b^2.$$\n",
        "\n",
        "Из $(19)$ получим:\n",
        "\n",
        "$$ b = \\sqrt{\\dfrac {6} {n_i + n_{i + 1}}}$$\n",
        "\n",
        "**Инициализация  Xavier** - это инициализация весов случайной величиной с распределением:\n",
        "\n",
        "$$W_i \\sim U[-\\sqrt{\\dfrac {6} {n_i + n_{i + 1}}}, \\sqrt{\\dfrac {6} {n_i + n_{i + 1}}}],$$ \n",
        "\n",
        "где $n_i$ - размерность выхода слоя n-го слоя.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNLi8HZV21Kt"
      },
      "source": [
        "Чтобы понять, что происходит с выходами слоя активации при использовании инициализации Xavier, рассмотрим картинку из оригинальной статьи [Xavier, Yoshua, \"Understanding the difficulty of training deep feedforward neural networks\", Aistats, 2010](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf):\n",
        "\n",
        "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L07/xavier_procentile_and_deviation_with_and_without_init.png\" width=\"600\">\n",
        "\n",
        "На картинке изображена зависимость 98-[процентиля](https://en.wikipedia.org/wiki/Percentile) (отдельные маркеры) и стандартного отклонения (соединенные маркеры) значений на выходе слоя активации $tanh$ от эпохи обучения для различных слоев нейросети. \n",
        "\n",
        "Верхнее изображение - инициализация весов с помощью нормального распределения $W_i \\sim U[-\\dfrac {1} {\\sqrt{n_i}}, \\dfrac {1} {\\sqrt{n_i}} ]$, нижнее с использованием инициализации Xavier.\n",
        "\n",
        "На верхнем изображении видно, как значения 98-процентиля уходят в значения +1 и -1 (сначала на выходе первого слоя, потом на выходе второго и т.д.). Это значит, что для части нейронов происходит затухание градиентов (они переходят в область, отмеченную на графиках $tanh(x)$, $tanh’(x)$ красным). На нижней картинке такого не происходит. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCqUAvul21Kt"
      },
      "source": [
        "## Хи-инициализация (Kaiming He)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VuJAh9K721Ku"
      },
      "source": [
        "### Вывод Kaiming"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGXadXFF21Ku"
      },
      "source": [
        "Для функции активации  ReLU и ее модификаций (PReLU, Leaky ReLU и т.д.) аналогично инициализации Xavier можно расписать условия $(1)$, $(2)$. Так вводится He-инициализация. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUqJjBbj21Ku"
      },
      "source": [
        "Аналогично выводу для Xavier получаем выражения:\n",
        "\n",
        "$$Dz^i = Dx \\prod_{p=0}^{i-1}\\dfrac 1 2 n_pDW^p $$\n",
        "\n",
        "$$D(\\dfrac {\\delta L} {\\delta z^i}) = D(\\dfrac {\\delta L} {\\delta z^d} ) \\prod_{p=i}^{d}\\dfrac 1 2 n_{p+1}DW^p $$\n",
        "\n",
        "где $n_p$ - размерность выхода слоя p-го слоя.\n",
        "\n",
        "Условия $(1)$, $(2)$ эквивалентны условиям: \n",
        "\n",
        "$$  \\dfrac {n_iDW^i} {2}  = 1, $$\n",
        "\n",
        "$$\\dfrac {n_{i+1}DW^i} {2} = 1.$$\n",
        "\n",
        "Можно опять взять среднее гармоническое. Но на практике берут либо $ \\frac 2 {n_i}$, либо $\\frac 2 {n_i + 1}$\n",
        "\n",
        "Итого получим:\n",
        "\n",
        "$$W^i \\sim N(0, sd=\\sqrt{\\frac 2 n_i})$$\n",
        "\n",
        "Более подробно с выводом Хи-инициализации можно ознакомиться в оригинальной [статье](https://arxiv.org/pdf/1502.01852v1.pdf)."
      ]
    }
  ]
}