{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Блок кода для обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def get_correct_count(pred, labels):\n",
    "    _, predicted = torch.max(pred.data, 1)\n",
    "    return (predicted.cpu() == labels.cpu()).sum().item()\n",
    "\n",
    "\n",
    "@torch.inference_mode()  # this annotation disable grad computation\n",
    "def validate(model, test_loader, device=\"cpu\"):\n",
    "    correct, total = 0, 0\n",
    "    for imgs, labels in test_loader:\n",
    "        pred = model(imgs.to(device))\n",
    "        total += labels.size(0)\n",
    "        correct += get_correct_count(pred, labels)\n",
    "    return correct / total\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, plotter=None, lr=0.03):\n",
    "        self.model = model\n",
    "        self.optimizer = torch.optim.SGD(\n",
    "            self.model.parameters(), lr=0.03\n",
    "        )  # Weight update\n",
    "        self.criterion = nn.CrossEntropyLoss()  # Loss function\n",
    "        self.plotter = ProgressPlotter() if plotter is None else plotter\n",
    "        self.epochs = 25\n",
    "        self.loss_hist = []\n",
    "\n",
    "    def __call__(self, train_loader, val_loader, epochs=10):\n",
    "        global device\n",
    "        print(\"Using device:\", device)\n",
    "        self.model.to(device)\n",
    "        self.model.train()\n",
    "        for epoch in tqdm(range(self.epochs)):\n",
    "            self.loss_hist = []\n",
    "            correct, total = 0, 0\n",
    "            for imgs, labels in train_loader:\n",
    "                correct += self.process_batch(imgs, labels)\n",
    "                total += len(labels)\n",
    "            self.plotter.add_scalar(\"Loss/train\", np.mean(self.loss_hist))\n",
    "            self.plotter.add_scalar(\n",
    "                \"Accuracy/val\", validate(self.model, val_loader, device=device)\n",
    "            )\n",
    "            self.plotter.add_scalar(\"Accuracy/train\", correct / total)\n",
    "            self.plotter.display([\"Loss/train\", \"Accuracy/val\"])\n",
    "\n",
    "    def process_batch(self, imgs, labels):\n",
    "        self.optimizer.zero_grad()\n",
    "        out = self.model(imgs.to(device))\n",
    "        loss = self.criterion(out, labels.to(device))\n",
    "        loss.backward()\n",
    "        self.loss_hist.append(loss.item())\n",
    "        self.optimizer.step()\n",
    "        return get_correct_count(out.cpu(), labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В силу нашей ненависти к TB копируем костыль для визуализации из прошлых заданий."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class ProgressPlotter:\n",
    "    def __init__(self, title=\"default\", groups=None) -> None:\n",
    "        self._history_dict = defaultdict(dict)\n",
    "        self.set_title(title)\n",
    "        self.groups = self.get_groups(groups)\n",
    "\n",
    "    def get_groups(self, groups):\n",
    "        if groups is not None:\n",
    "            return self._history_dict.keys()\n",
    "        if type(groups) is str:\n",
    "            groups = [groups]\n",
    "        return groups\n",
    "\n",
    "    def set_title(self, title):\n",
    "        for g in self._history_dict.keys():\n",
    "            self._history_dict[g][title] = []  # reset data\n",
    "        self.title = title\n",
    "\n",
    "    # group e.g. \"loss_val\" tag e.g. \"experiment_1\"\n",
    "    def add_scalar(self, group: str, value, tag=None) -> None:\n",
    "        tag = self.title if tag is None else tag\n",
    "\n",
    "        if not tag in self._history_dict[group]:\n",
    "            self._history_dict[group][tag] = []\n",
    "        self._history_dict[group][tag].append(value)\n",
    "\n",
    "    def add_row(self, group: str, value, tag=None) -> None:\n",
    "        tag = self.title if tag is None else tag\n",
    "        self._history_dict[group][tag] = value\n",
    "\n",
    "    def display_keys(self, ax, data):\n",
    "        history_len = 0\n",
    "        ax.grid()\n",
    "        for key in data:\n",
    "            ax.plot(data[key], label=key)\n",
    "            history_len = max(history_len, len(data[key]))\n",
    "        if len(data) > 1:\n",
    "            ax.legend(loc=\"upper right\")\n",
    "        if history_len < 50:\n",
    "            ax.set_xlabel(\"step\")\n",
    "            ax.set_xticks(np.arange(history_len))\n",
    "            ax.set_xticklabels(np.arange(history_len))\n",
    "\n",
    "    \"\"\"\n",
    "     groups list of keys like [['loss_train','loss_val'],['accuracy']]\n",
    "     All charts within a group will be plot in the same axis\n",
    "  \"\"\"\n",
    "\n",
    "    def display(self, groups=None):\n",
    "        clear_output()\n",
    "        if groups is None:\n",
    "            groups = self.groups\n",
    "        n_groups = len(groups)\n",
    "        fig, ax = plt.subplots(1, n_groups, figsize=(48 // n_groups, 3))\n",
    "        if n_groups == 1:\n",
    "            ax = [ax]\n",
    "        for i, g in enumerate(groups):\n",
    "            ax[i].set_ylabel(g)\n",
    "            self.display_keys(ax[i], self.history_dict[g])\n",
    "        fig.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    @property\n",
    "    def history_dict(self):\n",
    "        return dict(self._history_dict)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
