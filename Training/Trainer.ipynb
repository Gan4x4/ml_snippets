{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Зачем помещать код обучения в класс?"
      ],
      "metadata": {
        "id": "jBoQuTClH4Gk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Предположим что мы снова решили классифицировать CIFAR10"
      ],
      "metadata": {
        "id": "HcM7xUeERw1u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.transforms.transforms import ToTensor\n",
        "from torchvision.datasets import FakeData\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "trainset = FakeData(128, (3, 32, 32), 10, transform=ToTensor(),random_offset=43)\n",
        "valset = FakeData(16, (3, 32, 32), 10, transform=ToTensor(),random_offset=42)\n",
        "trainloader = DataLoader(trainset, batch_size=8, shuffle=True)\n",
        "valloader = DataLoader(valset, batch_size=8, shuffle=True)\n"
      ],
      "metadata": {
        "id": "VhUfRcjjR6Qf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Вероятно сначала наш код будет выглядеть так же как в примерах с сайта Pytorch:\n",
        "\n",
        "https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html"
      ],
      "metadata": {
        "id": "FbTqPBW8tKBc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch \n",
        "from torchvision.models import resnet18\n",
        "from torch import nn\n",
        "\n",
        "model = resnet18(False)\n",
        "optimizer = torch.optim.SGD(model.parameters(),lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(2):  # loop over the dataset multiple times    \n",
        "    for data in trainloader:\n",
        "        inputs, labels = data # data is a list of [inputs, labels]\n",
        "        optimizer.zero_grad() # zero the parameter gradients\n",
        "        outputs = model(inputs)# forward\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward() # get grad for weight \n",
        "        optimizer.step() # optimize"
      ],
      "metadata": {
        "id": "NbvCL2Y0tfUb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Так как обычно модель обучают несколько раз с различными параметрами, то логично поместить этот код в функцию:"
      ],
      "metadata": {
        "id": "R7AT7VhLuHS5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(net, trainloader, epochs,  optimizer, criterion ):\n",
        "  for epoch in range(epochs):  # loop over the dataset multiple times    \n",
        "      for i, data in enumerate(trainloader, 0):\n",
        "          inputs, labels = data # data is a list of [inputs, labels]\n",
        "          optimizer.zero_grad() # zero the parameter gradients\n",
        "          outputs = net(inputs) # forward\n",
        "          loss = criterion(outputs, labels)\n",
        "          loss.backward() # backward \n",
        "          optimizer.step() # optimize"
      ],
      "metadata": {
        "id": "yFu-zMtBhf6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "И вызывать ее с параметрами"
      ],
      "metadata": {
        "id": "hZipKgpHUTdy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment #2\n",
        "model = resnet18(False)\n",
        "optimizer = torch.optim.SGD(model.parameters(),lr=0.005)\n",
        "train(model,trainloader,epochs = 5,optimizer = optimizer,criterion = nn.CrossEntropyLoss())"
      ],
      "metadata": {
        "id": "R4VdPvw2Tayp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Однако обычно в процессе совершенствования модели появляется желание усовершенствовать процесс обучения. Например  добавить логгирование и le_scheduler\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mKe_SSF0iWIA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(net,trainloader,epochs,  optimizer, criterion , log, scheduler):\n",
        "  for epoch in range(epochs):  # loop over the dataset multiple times    \n",
        "      for i, data in enumerate(trainloader, 0):\n",
        "          inputs, labels = data # data is a list of [inputs, labels]\n",
        "          optimizer.zero_grad() # zero the parameter gradients\n",
        "          outputs = net(inputs)# forward\n",
        "          loss = criterion(outputs, labels)\n",
        "          loss.backward() #backward \n",
        "          optimizer.step() # optimize\n",
        "          \"\"\" new code \"\"\"\n",
        "          log.append(loss.item) \n",
        "          scheduler.step(loss)"
      ],
      "metadata": {
        "id": "xWVnihA6jjVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "model = resnet18(False)\n",
        "optimizer = torch.optim.SGD(model.parameters(),lr=0.005)\n",
        "log = [] \n",
        "scheduler = ReduceLROnPlateau(optimizer) \n",
        "criterion = nn.CrossEntropyLoss()\n",
        "train(model,trainloader,epochs = 5,optimizer = optimizer,criterion = criterion, log = log, scheduler = scheduler)"
      ],
      "metadata": {
        "id": "im-Gfh6OtYPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Такой подход ведет к дублированию кода и разрастанию сигнатуры функции. То есть нарушает принцип Don't repeat yourself [DRY](https://en.wikipedia.org/wiki/Don%27t_repeat_yourself)\n",
        "Почему дублирование кода это плохо?\n",
        "\n",
        "\n",
        "1.   Если нужно внести исправление приходится делать это в нескольких местах. Соответвенно риск ошибиться так же возрастает\n",
        "2.   Что бы понять что поменяпоменялось надо просмотреть весь код целиком\n",
        "\n",
        "Одним из способов решения этой проблемы является помещение кода в класс\n"
      ],
      "metadata": {
        "id": "IPOeG1Jtj3t9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Trainer:\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "        self.optimizer = torch.optim.SGD(\n",
        "            self.model.parameters(), lr=0.03\n",
        "        )  # Weight update\n",
        "        self.criterion = nn.CrossEntropyLoss()  # Loss function\n",
        "        self.epochs = 3\n",
        "\n",
        "    def __call__(self, train_loader):\n",
        "        self.model.train()\n",
        "        for epoch in range(self.epochs):\n",
        "            for imgs, labels in train_loader:\n",
        "                self.process_batch(imgs, labels)\n",
        "\n",
        "    def process_batch(self, imgs, labels):\n",
        "        self.optimizer.zero_grad()\n",
        "        out = self.model(imgs)\n",
        "        loss = self.criterion(out, labels)\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        return loss"
      ],
      "metadata": {
        "id": "JLGf94f1_3bI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = resnet18(False)\n",
        "trainer = Trainer(model)\n",
        "trainer(trainloader)"
      ],
      "metadata": {
        "id": "lNCYiyE5BG6s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теперь если нам понадобиться поменять алгоритм обучения или добавить новый параметр, код можно не переписывать. Например мы хотим логировать и выводить значение лосс:"
      ],
      "metadata": {
        "id": "F0RLaJ-xBmfi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EnhancedTrainer(Trainer):\n",
        "  def __init__(self, model):\n",
        "    super().__init__(model)\n",
        "\n",
        "  def __call__(self, train_loader):\n",
        "        self.model.train()\n",
        "        for epoch in range(self.epochs):\n",
        "            loss_history = []\n",
        "            for imgs, labels in train_loader:\n",
        "                loss = self.process_batch(imgs, labels)\n",
        "                loss_history.append(loss.item())\n",
        "            print(f\"Average loss: {torch.mean(loss).item():.4f}\")"
      ],
      "metadata": {
        "id": "9VGfLl-yBzea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = EnhancedTrainer(model)\n",
        "trainer.epochs = 4\n",
        "trainer(trainloader)"
      ],
      "metadata": {
        "id": "D91XWlV6DUvC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Или добавить LR_Scheduler"
      ],
      "metadata": {
        "id": "zmNJ8WfxCPQ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TrainerWithLRScheduler(EnhancedTrainer):\n",
        "  def __init__(self, model):\n",
        "    super().__init__(model)\n",
        "    self.lr_scheduler = ReduceLROnPlateau(self.optimizer) \n",
        "\n",
        "  def process_batch(self, imgs, labels):\n",
        "        loss = super().process_batch(imgs, labels)\n",
        "        self.lr_scheduler.step(loss)\n",
        "        return loss  "
      ],
      "metadata": {
        "id": "pqAO5P67DdVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = TrainerWithLRScheduler(model)\n",
        "trainer.epochs = 4\n",
        "trainer(trainloader)"
      ],
      "metadata": {
        "id": "axDD0ioIxLA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Подобный принцип лежит в основе фреймворка [Lightning](https://www.pytorchlightning.ai/) который мы рекомендуем использовать для работы с реальными данными. Так же как и [Tensorboard](https://pytorch.org/docs/stable/tensorboard.html)\n",
        "\n",
        "Однако для работы с учебными блокнотами можно использовать заготовку подобного класса в связке с блоком кода для визуализации."
      ],
      "metadata": {
        "id": "4xC1lk0yEAQn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trainer - класс для обучения"
      ],
      "metadata": {
        "id": "QUjhr7WYaAYP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(self, model, plotter=None, lr=0.03):\n",
        "        self.model = model\n",
        "        self.optimizer = torch.optim.SGD(\n",
        "            self.model.parameters(), lr=0.03\n",
        "        )  # Weight update\n",
        "        self.criterion = nn.CrossEntropyLoss()  # Loss function\n",
        "        # Create new plotter if need\n",
        "        self.plotter = ProgressPlotter() if plotter is None else plotter\n",
        "        self.epochs = 10\n",
        "        self.loss_hist = []\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    def __call__(self, train_loader, val_loader):\n",
        "        self.model.to(self.device)\n",
        "        self.model.train() # Enable train mode\n",
        "        for epoch in tqdm(range(self.epochs)):\n",
        "            self.loss_hist = []\n",
        "            correct, total = 0, 0 # for metric calculation\n",
        "            for imgs, labels in train_loader:\n",
        "                correct += self.process_batch(imgs, labels)\n",
        "                total += len(labels)\n",
        "            \n",
        "            # Logging\n",
        "            self.plotter.add_scalar(\"Loss/train\",\n",
        "                                    torch.tensor(self.loss_hist).mean().item())\n",
        "            self.plotter.add_scalar(\n",
        "                \"Accuracy/val\", self.validate(val_loader)\n",
        "            )\n",
        "            self.plotter.add_scalar(\"Accuracy/train\", correct / total)\n",
        "            self.plotter.display([\"Loss/train\", \"Accuracy/val\"])\n",
        "\n",
        "    def process_batch(self, imgs, labels):\n",
        "        self.optimizer.zero_grad()\n",
        "        out = self.model(imgs.to(self.device))\n",
        "        loss = self.criterion(out, labels.to(self.device))\n",
        "        loss.backward()\n",
        "        self.loss_hist.append(loss.item())\n",
        "        self.optimizer.step()\n",
        "        return Trainer.get_correct_count(out.cpu(), labels)\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def get_correct_count(pred, labels):\n",
        "      _, predicted = torch.max(pred.data, 1) #shape = batch_size, class_count\n",
        "      correct_predictions =  predicted.cpu() == labels.cpu() #shape = batch_size\n",
        "      return correct_predictions.sum().item() # correct_predictions is binary\n",
        "\n",
        "    \"\"\"\n",
        "      Calculate accuracy on val or test dataset\n",
        "    \"\"\"\n",
        "    @torch.inference_mode()  # this annotation disable grad computation\n",
        "    def validate(self, test_loader):\n",
        "        correct, total = 0, 0\n",
        "        for imgs, labels in test_loader:\n",
        "            pred = self.model(imgs.to(self.device))\n",
        "            total += labels.size(0)\n",
        "            correct += Trainer.get_correct_count(pred, labels)\n",
        "        return correct / total\n"
      ],
      "metadata": {
        "id": "SDShx7wUlHMr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ProgressPlotter класс для визуализации процесса обучения\n",
        "\n",
        "Для тех у кого не работает [Tensorboard](https://pytorch.org/docs/stable/tensorboard.html)\n"
      ],
      "metadata": {
        "id": "nzaetfJxkY-3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output\n",
        "from collections import defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class ProgressPlotter:\n",
        "    \"\"\"\n",
        "      title is experiment name e.g. ResNet_SGD\n",
        "      groups is loging paramener like loss or accuracy, can be set later\n",
        "    \"\"\"\n",
        "    def __init__(self, title=\"default\", groups=None) -> None:\n",
        "        self._history_dict = defaultdict(dict)\n",
        "        self.set_title(title)\n",
        "        self.groups = self.get_groups(groups)\n",
        "\n",
        "    def get_groups(self, groups):\n",
        "        if groups is not None:\n",
        "            return self._history_dict.keys()\n",
        "        if type(groups) is str:\n",
        "            groups = [groups]\n",
        "        return groups\n",
        "\n",
        "    def set_title(self, title):\n",
        "        \"\"\" Add new experiment to plotter \n",
        "        all existing data with same title will be removed\"\"\"\n",
        "        for g in self._history_dict.keys():\n",
        "            self._history_dict[g][title] = []  # reset data\n",
        "        self.title = title\n",
        "\n",
        "    # group e.g. \"loss_val\" tag e.g. \"experiment_1\"\n",
        "    def add_scalar(self, group: str, value, tag=None) -> None:\n",
        "        tag = self.title if tag is None else tag\n",
        "\n",
        "        if not tag in self._history_dict[group]:\n",
        "            self._history_dict[group][tag] = []\n",
        "        self._history_dict[group][tag].append(value)\n",
        "\n",
        "    def add_row(self, group: str, value, tag=None) -> None:\n",
        "        tag = self.title if tag is None else tag\n",
        "        self._history_dict[group][tag] = value\n",
        "\n",
        "    def display_keys(self, ax, data):\n",
        "        # display particular chart\n",
        "        history_len = 0\n",
        "        ax.grid()\n",
        "        for key in data:\n",
        "            ax.plot(data[key], label=key)\n",
        "            history_len = max(history_len, len(data[key]))\n",
        "        if len(data) > 1:\n",
        "            ax.legend(loc=\"upper right\")\n",
        "        if history_len < 50:\n",
        "            ax.set_xlabel(\"step\")\n",
        "            ax.set_xticks(np.arange(history_len))\n",
        "            ax.set_xticklabels(np.arange(history_len))\n",
        "\n",
        "    \"\"\"\n",
        "     groups list of keys like [['loss_train','loss_val'],['accuracy']]\n",
        "     All charts within a group will be plot in the same axis\n",
        "    \"\"\"\n",
        "    def display(self, groups=None):\n",
        "        clear_output()\n",
        "        if groups is None:\n",
        "            groups = self.groups\n",
        "        n_groups = len(groups)\n",
        "        fig, ax = plt.subplots(1, n_groups, figsize=(48 // n_groups, 3))\n",
        "        if n_groups == 1:\n",
        "            ax = [ax]\n",
        "        for i, g in enumerate(groups):\n",
        "            ax[i].set_ylabel(g)\n",
        "            self.display_keys(ax[i], self.history_dict[g])\n",
        "        fig.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    @property # can be accessed without braces e.g. my_plotter.history_dict\n",
        "    def history_dict(self):\n",
        "        # store data in format like {\"experiment1\":{\"loss\":[0.5,.0.44, ...]}}\n",
        "        return dict(self._history_dict)"
      ],
      "metadata": {
        "id": "hN3dPeHgkZjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Пример запуска"
      ],
      "metadata": {
        "id": "UX2m4vm4x4o2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = resnet18(False)\n",
        "trainer = Trainer(model)\n",
        "trainer.plotter.set_title(\"Experiment_1\")\n",
        "trainer(trainloader,valloader)"
      ],
      "metadata": {
        "id": "47t75COsx697"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}