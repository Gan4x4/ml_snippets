{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Эвристика для поиска ROI\n",
    "\n",
    "Вместо того, чтобы применять классификатор \"наобум\", можно для начала выбрать те области изображения, в которых вероятность нахождения объекта наиболее высока и запускать классификатор лишь для них."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/heuristics_way_object_detection_multiple_object.png\" width=\"700\">\n",
    "\n",
    "<center><p><em>Source: <a href=\"http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture11.pdf\">Detection and Segmentation</a></p> </em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Такие области называются **regions of interest**, сокращённо — **ROI**. \n",
    "\n",
    "Для поиска таких областей можно использовать какой-либо эвристический алгоритм, например [Selective search](https://ivi.fnwi.uva.nl/isis/publications/bibtexbrowser.php?key=UijlingsIJCV2013&bib=all.bib)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selective search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selective search — известный алгоритмический метод поиска **ROI**.\n",
    "\n",
    "Идея алгоритма состоит в разбиении изображения на небольшие области и последующем их итеративном объединении.\n",
    "\n",
    "Объединение происходит на основании сходства, которое вычисляется как сумма 4-х метрик (см. иллюстрацию)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/selective_search.png\" width=\"800\">\n",
    "\n",
    "<center><p><em>Source: <a href=\"http://www.huppelen.nl/publications/selectiveSearchDraft.pdf\"> Selective Search for Object Recognition </a></p></em></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возвращает порядка 2000 прямоугольников для изображения, отрабатывает за несколько секунд на CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R-CNN (Region CNN)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Первая известная модель, построенная по описанному принципу:\n",
    "\n",
    "- на изображении ищутся ROI,\n",
    "- для каждого делается resize,\n",
    "- каждый ROI обрабатывается сверточной сетью, которая предсказывает класс объекта, который в него попал."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/region_of_interest_cnn.png\" width=\"700\">\n",
    "\n",
    "<center><p><em>Source: <a href=\"http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture11.pdf\">Detection and Segmentation</a></p> </em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме класса модель предсказывает смещения для каждого bounding box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/r_cnn_predict_bounded_box_shift.png\" width=\"700\">\n",
    "\n",
    "<center><p><em>Source: <a href=\"http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture11.pdf\">Detection and Segmentation</a></p> </em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NMS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь возникает другая проблема: в районе объекта алгоритм генерирует множество ограничивающих прямоугольников (bounding box), которые частично перекрывают друг друга."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L12/out/non_max_suppression.png\" width=\"650\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы избавиться от них используется алгоритм\n",
    "NMS (Non maxima suppression). Его задача — избавиться от bounding boxes, которые накладаваются на истинный:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/non_max_suppression_pseudo_code.png\" width=\"1000\">\n",
    "\n",
    "<center><p><em>Source: <a href=\"https://towardsdatascience.com/non-maximum-suppression-nms-93ce178e177c\"> Non-maximum Suppression (NMS)</a></p></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "здесь $B$ — это массив всех bounding box,  $C$ — массив предсказаний модели относительно наличия объекта в соответствующем bounding box\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для оценки схожести обычно используется метрика IoU(same == IoU), а значение IoU ($\\lambda_{nms}$), при котором bounding boxes считаются принадлежащими одному объекту, является гиперпараметром (часто 0.5)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "В PyTorch алгоритм NMS доступен в модуле torchvision.ops\n",
    "\n",
    "`torchvision.ops.nms(boxes, scores, iou_threshold)`,\n",
    "где:\n",
    "* `boxes` — массив bounding box,\n",
    "* `scores` — предсказанная оценка,\n",
    "* `iou_threshold` — порог IoU, NMS отбрасывает все перекрывающиеся поля с $IoU> iou\\_threshold$\n",
    "\n",
    "[[doc] torchvision.ops.nms](https://pytorch.org/vision/stable/generated/torchvision.ops.nms.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Soft NMS](https://github.com/Gan4x4/ml_snippets/blob/main/CV/SoftNMS.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fast R-CNN\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проблемой R-CNN являлась скорость, так как мы вынуждены применять CNN порядка 2000 раз (в зависимости от эвристики, которая генерирует ROI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/r_cnn_scheme.png\" width=\"800\">\n",
    "\n",
    "<center><p><em>Source: <a href=\"http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture11.pdf\">Detection and Segmentation</a></p> </em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И решением, которое предложили авторы Fast R-CNN, является поиск ROI не на самом изображении, а на карте признаков. В таком случае большая часть сверток выполняется только один раз."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/fast_r_cnn_scheme.png\" width=\"700\">\n",
    "\n",
    "<center><p><em>Source: <a href=\"http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture11.pdf\">Detection and Segmentation</a></p> </em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это радикально ускоряет процесс"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROI Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь появляется новая задача — получить карты признаков одинакового размера для всех ROI.\n",
    "\n",
    "Для этого границы ROI проецируются на карту признаков.\n",
    "\n",
    "Затем к полученным фрагментам карты признаков применяется операция max pooling, и выходы получаются фиксированного размера. Теперь их можно подать на вход полносвязного слоя."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L12/out/roi_pooling.png\" width=\"650\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROI pooling в PyTorch\n",
    "\n",
    "\n",
    "```\n",
    "torchvision.ops.roi_pool(input, boxes, output_size,...)\n",
    "```\n",
    "где:\n",
    "* `input` —  тензор с входными картами признаков,\n",
    "* `boxes` —  массив bounding box,\n",
    "* `output_size` — размер вывода после ROI pooling.\n",
    "\n",
    "\n",
    "[Документация Roi Pooling](https://pytorch.org/vision/stable/generated/torchvision.ops.roi_pool.html)\n",
    "\n",
    "Статья: [Region of Interest Pooling](https://towardsdatascience.com/region-of-interest-pooling-f7c637f409af)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROI Align\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Операция ROI pooling применялась в оригинальной модели Fast-RCNN. В дальнейшем она была заменена на Roi Align. Здесь признаки не отбрасываются, как это происходит при  max pooling, а их значения интерполируются на новую карту признаков."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/roi_align.png\" width=\"750\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы избежать квантования границ, RoIAlign использует билинейную интерполяцию для вычисления  значений входных признаков."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "torchvision.ops.roi_align(input, boxes, output_size, ...)\n",
    "```\n",
    "где:\n",
    "* `input` —  тензор с входными картами признаков,\n",
    "* `boxes` —  массив bounding box,\n",
    "* `output_size` — размер вывода после Roi Align.\n",
    "\n",
    "[[doc] torchvision.ops.roi_align](https://pytorch.org/vision/stable/generated/torchvision.ops.roi_align.html)\n",
    "\n",
    "[Understanding Region of Interest](https://towardsdatascience.com/understanding-region-of-interest-part-2-roi-align-and-roi-warp-f795196fc193)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Faster R-CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Идея: пусть сеть сама предсказывает ROI по карте признаков**\n",
    "\n",
    "После того, как в Fast-RCNN большая часть сверток стала запускаться единожды,\n",
    "скорость работы нейросетевой части существенно возросла.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/compare_training_time_r_cnn_and_fast_r_cnn.png\" width=\"900\">\n",
    "\n",
    "<center><p><em>Source: <a href=\"https://aman.ai/cs231n/detection/\">Detection and Segmentation</a></p> </em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь \"узким местом\" стала эвристика для поиска ROI.\n",
    "\n",
    "Поэтому в следующей версии детектора (Faster R-CNN) от эвристики решено было избавиться, а  ROI искать при помощи дополнительной нейросети Region Proposal Network (RPN).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/faster_r_cnn_scheme.png\" width=\"650\">\n",
    "\n",
    "<center><p><em>Source: <a href=\"https://tjmachinelearning.com/lectures/1718/obj/\">Object Detection</a></p> </em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для обучения такой модели требуется посчитать четыре loss.\n",
    "\n",
    "\n",
    "1. RPN классифицирует объект/не объект (классификация).\n",
    "2. Координаты ROI предсказанные RPN (регрессия).\n",
    "3. Класс объекта для каждого bounding box (классификация).\n",
    "4. Координаты bounding boxes (регрессия)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/faster_r_cnn_train_time.png\" width=\"700\">\n",
    "\n",
    "<center><p><em>Source: <a href=\"http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture11.pdf\">Detection and Segmentation</a></p> </em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В результате скорость увеличивается ещё почти в 10 раз, но для задач реального времени все равно остаётся неприемлемо низкой.\n",
    "\n",
    "[Faster R-CNN на PyTorch](https://pytorch.org/vision/stable/models.html#faster-r-cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "# load model\n",
    "fr_rcnn_model = torchvision.models.detection.fasterrcnn_resnet50_fpn(\n",
    "    weights=\"FasterRCNN_ResNet50_FPN_Weights.DEFAULT\",\n",
    "    progress=True,\n",
    "    num_classes=91,\n",
    "    weights_backbone=\"ResNet50_Weights.DEFAULT\",\n",
    ")\n",
    "fr_rcnn_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим данные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocotools.coco import COCO\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "\n",
    "# load data\n",
    "r = requests.get(\n",
    "    \"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\"\n",
    ")\n",
    "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "z.extractall()\n",
    "coco = COCO(\"annotations/instances_val2017.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def coco2pil(url):\n",
    "    print(url)\n",
    "    response = requests.get(url)\n",
    "    return Image.open(BytesIO(response.content))\n",
    "\n",
    "\n",
    "catIds = coco.getCatIds(catNms=[\"person\", \"bicycle\"])  # get category IDs\n",
    "# person and bicycle\n",
    "imgIds = coco.getImgIds(catIds=catIds)\n",
    "img_list = coco.loadImgs(\n",
    "    imgIds[12]\n",
    ")  # http://images.cocodataset.org/val2017/000000370208.jpg\n",
    "img = img_list[0]\n",
    "\n",
    "# plot image\n",
    "plt.figure(figsize=(10, 10))\n",
    "pil_img = coco2pil(img[\"coco_url\"])\n",
    "plt.imshow(pil_img)\n",
    "\n",
    "# plot boundy boxes\n",
    "annIds = coco.getAnnIds(imgIds=img[\"id\"])\n",
    "anns = coco.loadAnns(annIds)\n",
    "coco.showAnns(anns, draw_bbox=True)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "print(\"Image data: \")\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms.functional as TF\n",
    "from PIL import ImageDraw\n",
    "\n",
    "# lets predict objects by resnet50\n",
    "with torch.no_grad():\n",
    "    tensor = TF.pil_to_tensor(pil_img) / 255  # Normalize\n",
    "    output = fr_rcnn_model(tensor.unsqueeze(0))\n",
    "    draw = ImageDraw.Draw(pil_img)\n",
    "\n",
    "    # plot rectangles\n",
    "    for i, bbox in enumerate(output[0][\"boxes\"]):\n",
    "        if output[0][\"scores\"][i] > 0.5:\n",
    "            draw.rectangle((tuple(bbox[:2].numpy()), tuple(bbox[2:].numpy())), width=2)\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(pil_img)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Region proposal network (RPN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как устроена сеть, предсказывающая ROI?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L12/out/roi_pooling.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Карта признаков имеет фиксированные и относительно небольшие пространственные размеры (например 20x15). Поэтому можно вернуться к идее скользящего окна, которая была отвергнута в самом начале из-за большого размера изображения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/rpn_base.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для карты признаков размером 20x15 количество ROI получится равным 3000, что сравнимо с количеством предсказаний, производимых SelectiveSearch.\n",
    "\n",
    "Далеко не всегда объект хорошо вписывается в квадрат:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L12/out/rpn_aspect_ratio.png\" width=\"900\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поэтому для каждой точки на карте признаков (anchor) можно использовать окна нескольких форм:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/rpn_base_anchor.png\" width=\"700\">\n",
    "\n",
    "<center><p><em>Source: <a href=\"https://aman.ai/cs231n/detection/\">Detection and Segmentation</a></p> </em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это позволит минимизировать корректировку* и лучше предсказывать ROI  для вытянутых объектов.\n",
    "\n",
    "\\* *помним, что для каждого прямоугольника предсказываются 4 числа, обозначающих сдвиг его вершин относительно начального положения.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для каждого окна предсказываются два значения:\n",
    "\n",
    "* вероятность того, что в ROI находится объект (одно число),\n",
    "* смещения (4 числа)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сама сеть при этом может быть очень простой:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L12/out/simple_nn_predict_objectness_and_boundary_box.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Two stage detector\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если присмотреться к схеме, можно заметить, что на среднем и верхнем уровнях выполняются схожие операции.\n",
    "\n",
    "\n",
    "Разница в том, что на последнем слое предсказывается класс объекта, а на промежуточном только вероятность его присутствия (objectness). \n",
    "\n",
    "Корректировки для вершин bounding box предсказываются и в обоих случаях."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/two_stage_detector.png\" width=\"750\">\n",
    "\n",
    "<center><p><em>Source: <a href=\"http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture11.pdf\">Detection and Segmentation</a></p> </em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно сказать, что детектирование происходит в две стадии. \n",
    "Соответственно Faster RCNN == Two stage detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Stage detector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если сразу предсказывать класс, то можно избавиться от второй стадии.\n",
    "В этом случае к списку классов нужно добавить еще один элемент, который заменит objectness либо будет предсказанием класса \"фон\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L12/out/one_stage_detector.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Детекторы, которые работают \"за один проход\", быстрее, но потенциально менее точные.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/yolo_ssd_retinanet.png\" width=\"1000\">\n",
    "\n",
    "<center><p><em>Source: <a href=\"https://aman.ai/cs231n/detection/\">Detection and Segmentation</a></p> </em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим несколько моделей, построенных по этому принципу:\n",
    "YOLO, SSD, RetinaNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Сравнение скорости моделей](https://pytorch.org/vision/stable/models.html#runtime-characteristics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SSD: Single Shot MultiBox Detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/EduNet-web_dependencies/L12/ssd_default_boxes.png\" width=\"700\">\n",
    "\n",
    "<center><p><em>Source: <a href=\"https://ternak.github.io/cnn_detection.html\">Object Detection With Convolution Neural Nets</a></p> </em></center>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Кандидаты в ROI (default box) выбираются на нескольких слоях (4,7, 8, 9,11)\n",
    "* Количество форм окон (default box) на картах признаков зависит от слоя: от 4 до 6\n",
    "\n",
    "[SSD: Single Shot MultiBox Detector (Liu et al., 2015)](https://arxiv.org/abs/1512.02325)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/EduNet-web_dependencies/L12/single_shot_multibox_detector_scheme.png\" width=\"1500\">\n",
    "\n",
    "<center><p><em>Source: <a href=\"https://arxiv.org/pdf/1512.02325.pdf\">Single Shot MultiBox Detector</a></p> </em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* В качестве backbone используется VGG-16, предобученная на ImageNet\n",
    "* Добавлен класс для \"background\"\n",
    "\n",
    "\n",
    "В общей сложности делается 8732 предсказаний, каждое содержит 4 + (N + 1) чисел. \n",
    "\n",
    "N — это количество классов без фона,\n",
    "4 — смещения.\n",
    "\n",
    "\n",
    "[Подробнее.](https://towardsdatascience.com/review-ssd-single-shot-detector-object-detection-851a94607d11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss для детектора"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как подсчитать loss для детектора. Loss должн включать в себя две части: ошибку локализации и ошибку классификации.\n",
    "\n",
    "И для SSD loss function так и выглядит:\n",
    "\n",
    "$$L(x,c,l,g) = \\frac{1}{N}(L_{conf}(x,c) + \\alpha L_{loc}(x,l,g))$$\n",
    "\n",
    "\n",
    "Однако если мы будем считать ошибку локализации для всех default box, то она никогда не будет нулевой.\n",
    "Default box очень плотно перекрывает все изображение и в большинство из них объект не попадет, особенно если объект один и небольшой."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/EduNet-web_dependencies/L12/default_boxes.png\" width=\"700\">\n",
    "\n",
    "<center><p><em>Source: <a href=\"https://ternak.github.io/cnn_detection.html\">Object Detection With Convolution Neural Nets</a></p> </em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поэтому часть default box при подсчете loss игнорируются. Используются только те, у которых  большая площадь пересечения с одним из истинным bounding box больше порога (IoU > 0.5)\n",
    "\n",
    "\n",
    "\n",
    "$$L(x,l,g)_{loc} = \\sum_{i \\in Pos}^{N} x_{i,j}^{k}smooth_{L1}(l_i, g_j)$$\n",
    "\n",
    "\n",
    "Здесь:\n",
    "\n",
    "$l$ — финальные координаты предсказанного bounding box с учетом смещений,\n",
    "\n",
    "$g$ — координаты истинного bounding box,\n",
    "\n",
    "$M$ — количество истинных (ground true) bounding box-ов,\n",
    "\n",
    "$Pos$ — список отобранных default box пересекающихся с истинными,\n",
    "\n",
    "$x_{i,j}^{k} = \\{1,0\\}$ — индикатор того, что комбинация default и box валидна.\n",
    "\n",
    "\n",
    "\n",
    "> i — индекс default box,\n",
    "> j — индекс истинного (ground true) bounding box,\n",
    "> p — номер класса, к которому относится ground true bounding box (не степень).\n",
    "\n",
    "$smooth_{L1}$ — [Комбинация L1 и L2](https://pytorch.org/docs/stable/generated/torch.nn.SmoothL1Loss.html) \n",
    "\n",
    "Компонент, отвечающий за классификацию:\n",
    "\n",
    "$$L(x,с)_{conf} = -\\sum_{i \\in Pos} x_{i,j}^{k} log(softmax(c_{i}^{p})) -\\sum_{i \\in Neg} log(softmax(c_{i}^{0}))$$\n",
    "\n",
    "$c_{i}^{p}$ — вектор score для i-того default box, p — номер истинного класса, соответствующего bounding box из разметки\n",
    "\n",
    "$Pos$ — список отобранных default box, не пересекающихся с истинными (IoU < treshold)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\\* *Формулы для loss function осознанно упрощены. Например, мы опустили расчет L1 для смещений, что является технической деталью.*\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FocalLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Следующий заслуживающий внимания one-stage детектор это Retina Net — [Focal Loss for Dense Object Detection (Lin et al., 2017)](https://arxiv.org/abs/1708.02002)\n",
    "\n",
    "Собственно, авторы придумали новую функцию потерь (Focal Loss) и опубликовали модель, чтобы продемонстрировать её эффективность.\n",
    "\n",
    "Чтобы понять, какую проблему решает Focal Loss, давайте посмотрим на второй компонент Loss классификации для SSD:\n",
    "\n",
    "$$L_{conf} =  \\ ...\\  -\\sum_{i \\in Neg} log(softmax(c_{i}^{0}))$$\n",
    "\n",
    "Это кросс-энтропия для bounding boxes, содержащих фон. Тут нет ошибки: когда модель обучится правильно предсказывать класс фона (background), каждая из этих компонент будет небольшой.\n",
    "\n",
    "Проблема в том, что таких компонент очень много. Детектор предсказывает несколько тысяч, или десятков тысяч bounding boxes. Подавляющая часть из них приходится на фон. Cумма большого количества этих небольших потерь (loss) становится заметным числом и мешает учиться классифицировать реальные объекты."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как решается эта проблема в Focal Loss?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/focal_loss_vs_ce.png\" width=\"700\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://arxiv.org/abs/1708.02002\">Focal Loss for Dense Object Detection (Lin et al., 2018)</a></em></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Фактически loss для уверенно классифицированных объектов дополнительно занижается. Это похоже на взвешивание при дисбалансе классов.\n",
    "\n",
    "Достигается этот эффект путем домножения на коэффициент: $ (1-p_{t})^\\gamma$\n",
    "\n",
    "Здесь:\n",
    "\n",
    "$ p_{t} $ — вероятность истинного класса,\n",
    "\n",
    "$ \\gamma $ — число большее 1 и являющееся гиперпараметром.\n",
    "\n",
    "\n",
    "Пока модель ошибается, $p_{t}$ — мало, и значение выражения в скобках соответственно близко к 1. \n",
    "\n",
    "Когда модель обучилась, значение $p_{t}$ становится близким к 1, а разность в скобках становится маленьким числом, которое возводится в степень $ \\gamma $ > 1. Таким образом, домножение на это небольшое число нивелирует вклад верно классифицированных объектов.\n",
    "\n",
    "Это позволяет модели сосредоточиться на изучении сложных объектов (hard example )\n",
    "\n",
    "[Подробнее про FocalLoss](https://github.com/Gan4x4/ml_snippets/blob/main/FocalLoss.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Нard Example Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При обучении модели, мы можем обнаружить, что средняя ошибка на всех данных достаточно маленькая, однако ошибка на редких нетипичных данных довольно высока. При этом нетипичные данные необязательно являются выбросами.\n",
    "\n",
    "Разберемся, почему так происходит.\n",
    "К примеру, рассмотрим задачу обнаружения автомобилей на потоках данных с камер наружного видеонаблюдения. Если в обучающем наборе большая часть данных — снимки, сделанные днём, то качество работы модели ночью будет низким. В данном случае, \"нетипичными\" данными будут ночные снимки. Но, на самом-то деле, \"нетипичных\" случаев может быть довольно много, и некоторые из них могут происходить даже днём. Например: \n",
    "* изменение погоды (изменение яркости, резкости, помехи на изображении),\n",
    "* смена сезона (снег либо листья могут покрыть дорогу — изменение фона),\n",
    "* машины с экзотическими узорами на кузове."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Довольно простым и эффективным решением проблемы является сбор \"сложных\" случаев (**hard example mining**) и дообучение модели на них. При этом, поскольку модель уже довольно хорошо работает на большей части данных, можно дополнительно удалить часть данных из обучающей выборки — таким образом мы сосредотачиваем модель на обучении на сложных примерах."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L12/out/hard_example_mining.png\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Online hard example mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В некоторых случаях hard exapmle mining можно выполнять прямо во время формирования батча, \"налету\". В таких случаях говорят про **online hard example mining**.\n",
    "\n",
    "Один из вариантов может быть реализован в two-stage детекторах.\n",
    "Напоминаю: первая часть детектора отвечает за обнаружение regions of interest (RoI), затем выполняется (как правило, сравнительно вычислительно дешёвая) классификация. Одним из вариантов реализации идеи может быть выполнение forward pass классификатора по всем предложенным RoI и затем формирование батча, в котором будет выделено определённое количество \"мест\" под RoI, предсказания на которых выполняются наихудшим образом."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/online_hard_example_mining.png\" width=\"700\"></center>\n",
    "\n",
    "<center><p><em>Source: <a href=\"https://sklin93.github.io/hog.html\"> HoG Face Detection with a Sliding Window </a></p> </em></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature pyramid network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вторым полезным нововведением  в RetinaNet стало использование пирамиды признаков.\n",
    "\n",
    "[Feature Pyramid Networks for Object Detection (Tsung-Yi Lin et al., 2017)](https://arxiv.org/abs/1612.03144)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/retinanet_use_outputs_fpn.png\" width=\"900\">\n",
    "\n",
    "<center><p><em>Source: <a href=\"https://link.springer.com/article/10.1007/s11042-022-13153-y?error=cookies_not_supported&code=d283d48a-d725-4d7e-a568-7955a14a0550\">Tools, techniques, datasets and application areas for object detection in an image: a review</a></p> </em></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RetinaNet использует выходы FPN для предсказаний и класса, и bbox. Мы уже обсуждали пирамиды признаков применительно к сетям для сегментации, в частности, FCN. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На каждом сверточном слое извлекаются карты признаков. \n",
    "\n",
    "Их пространственное разрешение постепенно уменьшается, а глубина (количество каналов) увеличивается.\n",
    "\n",
    "\n",
    "Но первые слои содержат мало семантической информации (только низкоуровневые признаки).  А карты признаков с глубоких слоев имеют низкое пространственное разрешение, что не позволяет качественно определить границы объектов.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L12/out/semantic_information.png\" width=\"650\">\n",
    "\n",
    "<left><p><em>Source: <a href=\"https://arxiv.org/pdf/1612.03144.pdf\">Feature Pyramid Networks for Object Detection</a></p> </em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так же, как и в случае с сегментацией, точность повышается, если делать предсказания на картах, содержащих признаки для разных масштабов.\n",
    "\n",
    "\n",
    "При этом можно получать карты с большим пространственным разрешением не просто сохраняя их в памяти, а еще и прибавляя к ним значения признаков с более глубоких слоев, предварительно интерполировав их (Upsample)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идея состоит в том, чтобы делать предсказание с учетом семантической информации, полученной на более глубоких слоях. Здесь признаки   суммируются, а не конкатенируются."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Затем к новым картам признаков может применяться дополнительная свертка.\n",
    "\n",
    "На выходе получаем карты признаков P2—P5, на которых уже предсказываются bounding boxes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L12/out/resnet_prediction_head_scheme.png\" width=\"850\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В случае 2-stage детектора (RCNN) новые карты признаков подаются на вход RPN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/EduNet-web_dependencies/L12/features_from_blackbone.jpeg\" width=\"1100\">\n",
    "\n",
    "<center><p><em>Source: <a href=\"https://jonathan-hui.medium.com/understanding-feature-pyramid-networks-for-object-detection-fpn-45b227b9106c\">Understanding Feature Pyramid Networks for object detection (FPN)</a></p> </em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А признаки для предсказаний используются из backbone.\n",
    "\n",
    "Дополнительно: [Блог-пост про FPN](https://jonathan-hui.medium.com/understanding-feature-pyramid-networks-for-object-detection-fpn-45b227b9106c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [You Only Look Once: Unified, Real-Time Object Detection (Redmon et. al., 2015)](https://arxiv.org/abs/1506.02640) \n",
    "* [YOLO9000: Better, Faster, Stronger (Redmon et. al., 2015)](https://arxiv.org/abs/1612.08242)\n",
    "* [YOLOv3: An Incremental Improvement (Redmon et. al., 2018)](https://arxiv.org/abs/1804.02767)\n",
    "* [YOLOv4: Optimal Speed and Accuracy of Object Detection (Bochkovskiy et al., 2020)](https://arxiv.org/abs/2004.10934)\n",
    "* [YOLOv5 (Glenn Jocher Ultralytics,  June 2020)](https://github.com/ultralytics/yolov5)\n",
    "* [YOLOX: Exceeding YOLO Series in 2021 (Ge et al., June 2021)](https://arxiv.org/abs/2107.08430)\n",
    "\n",
    "* [YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors(Chien-Yao Wang et. al. July 2022)](https://arxiv.org/abs/2207.02696)\n",
    "\n",
    "*  [YOLOv6: A Single-Stage Object Detection Framework for Industrial Applications(Chuyi Li et. al., Sept 2022)](https://arxiv.org/abs/2209.02976)\n",
    "\n",
    "*  \t[YOLOv8,\tUltralytics, Dec. 2022](\n",
    "https://github.com/ultralytics/ultralytics)\n",
    "[(documentation !)](https://docs.ultralytics.com/)\n",
    "\n",
    "* [YOLO6v3\tYOLOv6 v3.0: A Full-Scale Reloading Chuyi Li et. al., Dec. 2023](https://arxiv.org/abs/2301.05586)\n",
    "\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/yolov6_performance.png\" width=\"1000\">\n",
    "\n",
    "<center><p><em>Source: <a href=\"https://arxiv.org/abs/2301.05586\">YOLOv6 v3.0: A Full-Scale Reloading\n",
    "</a></p> </em></center>\n",
    "\n",
    "\n",
    "Первая версия YOLO вышла в том же году что и SSD. На тот момент детектор несколько проигрывал SSD в точности.\n",
    "\n",
    "Однако благодаря усилиям Joseph Redmon проект поддерживался и развивался в течение нескольких лет.\n",
    "\n",
    " 3-я версия детектора оказалась настолько удачной, что даже в 2021 можно было прочесть:  \"YOLOv3, one of the most widely used detectors in industry\" [2021](https://arxiv.org/abs/2107.08430)\n",
    "\n",
    "Последующие версии создавались разными авторами. Полагаю что правильно считать их разными форками YOLOv3 а не новыми версиями. Даже нумерация условна, например статья про v7 датируется более ранней датой чем v6.\n",
    "\n",
    "В прикладных задачах я бы рекомендовал использовать YOLOv8 так как авторы выложили на свой сайт [документацию](https://docs.ultralytics.com/), а точность при скорости порядка 100 fps у всех современных моделей почти одинакова.\n",
    "\n",
    "В настоящий момент можно сказать, что YOLO — это оптимальный детектор по соотношению качества распознавания к скорости.\n",
    "\n",
    "Далее мы выборочно рассмотрим некоторые версии что бы разобраться какие именно техники позволили так существенно улучшить качество и скорость детектирования.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### YOLOv3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/yolov3.png\" width=\"1200\">\n",
    "\n",
    "<center><p><em>Source: <a href=\"https://towardsdatascience.com/yolo-v3-object-detection-53fb7d3bfe6b#:~:text=YOLO%20v2%20used%20a%20custom,more%20layers%20for%20object%20detection.&text=First%2C%20YOLO%20v3%20uses%20a,layer%20network%20trained%20on%20Imagenet.\">What’s new in YOLO v3?</a></p> </em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "В качестве backbone используется оригинальная сверточная сеть DarkNet53, задействующая слои Batch Norm и Skip connection. Но есть реализации с ResNet в качестве backbone.\n",
    "\n",
    "Детектор использует большинство техник, которые мы обсудили:\n",
    "\n",
    "* Default boxes извлекаются на трех слоях различной глубины. Для каждой ячейки  предсказывается З окна\n",
    "* FPN: признаки конкатенируются, а не складываются\n",
    "* Resolution augmentation: При обучении разрешение входных изображений менялось от 384x384 до 672x672\n",
    "* В качестве loss function для классификации используется бинарная кросс-энтропия, позволяющая предсказывать несколько объектов в одном bounding box. Что позволяет использовать детектор с multilabel датасетами, где один объект может иметь несколько меток (person & woman)\n",
    "* Предсказывается дополнительный параметр objectness score. Он не связан с классификацией. Его задача — предсказать, насколько вероятно, что в предсказанном default boх действительно есть объект и он будет учитываться при подсчете loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Формат предсказаний\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/yolov3_prediction.png\" width=\"400\">\n",
    "\n",
    "<left><p><em>Source: <a href=\"https://blog.paperspace.com/how-to-implement-a-yolo-object-detector-in-pytorch/\">How to implement a YOLO (v3) object detector from scratch in PyTorch</a></p> </em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Соответственно, для каждого anchor box предсказывается вот такой вектор значений:\n",
    "\n",
    "* смещения;\n",
    "* objectness — вероятность наличия объекта;\n",
    "* scores — уверенность того, что bbox содержит объект определенного класса. Для моделей тренированных на СOCO классов 80.\n",
    "\n",
    "Для 80-ти классов получается 85 значений на один default box."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"orange\"> Предсказание смещений</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вот так преобразуются предсказания YOLOv3 для получения финальных координат bounding box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/yolov3_coordinates_prediction.png\" width=\"500\">\n",
    "\n",
    "<left><p><em>Source: <a href=\"https://arxiv.org/pdf/1612.08242.pdf\">YOLO9000:\n",
    "Better, Faster, Stronger</a></p> </em></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "$c_{x} , c_{y}$ — это координаты верхнего левого угла default box\n",
    "$p_{w} , p_{h}$ — это ширина и высота default box\n",
    "$t_{x} , t_{y}$ — предсказанные смещения  для центра\n",
    "$t_{w} , t_{h}$ — предсказанные корректировки  для ширины и высоты\n",
    "\n",
    "$b_{x} , b_{y}, b_{w}, b_{h}$ — координаты центра, ширина и высота финального предсказанного bouning box. Значения в процентах от ширины и длины исходного изображения\n",
    "\n",
    "$\\sigma(x) $ — сигмоида\n",
    "\n",
    "$e$ — число Эйлера\n",
    "\n",
    "сторона каждой клетки равна 1-це так как это просто порядковый номер элемента в строке и столбце."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <font color=\"orange\">Почему не предсказывть абсолютные значения?</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно по схеме не предсказываются абсолютные значения смещений. Почему вместо того что бы в понятной регрессионной задаче не предсказать 4 числа предсказываются  коеффициенты, с которыми затем происходят неочевидные преобразования?\n",
    "\n",
    "Что бы ответить на этот вопрос вспомним про нормализацию данных. Мы нормализуем входные данные, и центрируем их вокруг нуля."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = torch.randn((512, 100))  # Fake normalized data\n",
    "plt.hist(x.mean(dim=0), bins=10)\n",
    "plt.show()\n",
    "print(f\"Mean: {x.mean().item():.2f} Variance: {x.var().item():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Кроме того мы определенным образом инициализируем веса и добавляем слои ализации что бы распределение входов очередного слоя (они же выходы предидущего) более-менее сохранялось.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(100, 50),  # weights randomly sampled from some random distribution\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm1d(50),\n",
    "    nn.Linear(50, 1),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Веса инициализированны равномерно расперделены вокруг нуля:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = net[0].weight.data.numpy()\n",
    "plt.hist(weights.flatten(), bins=20)\n",
    "plt.show()\n",
    "print(weights.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И на выходе последнего слоя будет та же ситуация: необученная сеть будет чаще всего предсказывать около нулевые значения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = net(x)\n",
    "plt.hist(out.detach().numpy(), bins=20)\n",
    "plt.show()\n",
    "print(f\"Mean: {out.mean().item():.2f} Variance: {out.var().item():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если мы будем пытаться предсказывать большие по модулю значения, например абсолютные координаты:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = torch.randint(0, 224, (512, 1))  # fake coordinate in range [0 .. 255]\n",
    "print(targets[:10].flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "То получим огромную ошибку, которая повлечет большое обновление весов приведет к нестабильному обучению, большим абсолютным значениям весов и.т.п."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "loss = criterion(out, targets)\n",
    "print(\"Loss\", loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если мы стандартизуем координаты(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = targets.float().mean()\n",
    "std = targets.float().std()\n",
    "\n",
    "transformed_targets = (targets - mean) / std\n",
    "\n",
    "print(transformed_targets.flatten()[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "То получим ошибку на 4 порядка меньше:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = criterion(out, transformed_targets)\n",
    "print(\"Loss\", loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Фактически теперь мы предсказываем смещение от среднего значения. Что бы его затем использовать надо его денормализовать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_out = out * std + mean\n",
    "print(real_out.int().flatten()[:10])  # values like a coords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Иногда такую постобработку удобно включить в модель, что бы она учитывалась при подсчете loss. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Центр**\n",
    "\n",
    "В случае когда смещение не нужно, сеть предсказывает нулевые значения для $t_{x} и t_{y}$ \n",
    "\n",
    "$\\sigma(0) = 0.5$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot sigmoid function\n",
    "import numpy as np\n",
    "\n",
    "x = np.linspace(-5, 5)\n",
    "y = 1 / (1 + np.exp(-x))\n",
    "plt.plot(x, y)\n",
    "plt.axvline(0, color=\"black\", alpha=0.4)\n",
    "plt.title(\"Sigmoid\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сответственно центр предсказанного bbox окажется в центре клетки и совпадет с центром default box\n",
    "\n",
    "Если смешение t большОе положительное to $\\sigma(t)$ будет близко к +1 и центр сместиться к правой границе ячейки. Однако никода не выйдет за ее пределы т.к. $\\sigma(t) < 1$\n",
    "\n",
    "Аналогично для отрицательных значений. \n",
    "\n",
    "Это ровно то что нам нужно, так как за пердсказания bbox объектов, центры которых лежат в других ячейках должны отвечать default box соответствующих ячеек.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Щирина и высота *\n",
    "\n",
    "Корректировоку для ширины и высоты нет смысла ограничивать единицей, так как объект может занимать несколько ячеек и даже выходить за границу изображения.\n",
    "\n",
    "Здесь ширина и высота  default box ($p_{w} , p_{h}$) масштабируются путем умножения на $e^t$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot exponent\n",
    "x = np.linspace(-2, 2)\n",
    "y = np.exp(x)\n",
    "plt.plot(x, y)\n",
    "plt.axvline(0, color=\"black\", alpha=0.4)\n",
    "plt.title(\"Exp\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При $t~= 0$ когда сеть необучена или корректировка не нужна $e^0 ~=1$ и $b =p*e^t ~= p$\n",
    "\n",
    "Чем заметном отклонеии t от нуля ширина с высотой будут либо стремительно расти (при t>>0) либо стремительно уменьшаться (t<<0)\n",
    "\n",
    "P.S. Этот прием часто используется для предсказания сетью значений имеющих большой разброс."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### YOLOv4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOLOv3 на момент выхода стал одним из самых быстрых детекторов и последней версией за авторством Джозефа Редмона.\n",
    "\n",
    "YOLOv4 — это детище других авторов. Модель не стала быстрее, но стала намного более точной."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/yolov4.jpeg\" width=\"700\"></center>\n",
    "\n",
    "<center><p><em>Source: <a href=\"https://medium.com/visionwizard/yolov4-version-3-proposed-workflow-e4fa175b902\"> YOLOv4 — Version 3: Proposed Workflow</a></p> </em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что же добавили авторы:\n",
    "\n",
    "1. Поменялся Backbone:\n",
    "* Увеличилось количество слоев,\n",
    "* Добавился SPP block,\n",
    "* Добавились Dense блоки (дополнительные skip connection как в [DenseNet](https://arxiv.org/abs/1608.06993))\n",
    "* Появились PAN блоки (Path Aggregation Network Module)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Path Aggregation Network Module(PAN)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/pan.png\" width=\"600\"></center>\n",
    "\n",
    "<center><p>\n",
    "[Path Aggregation Network for Instance Segmentation](https://openaccess.thecvf.com/content_cvpr_2018/papers/Liu_Path_Aggregation_Network_CVPR_2018_paper.pdf)\n",
    "</center></p>\n",
    "\n",
    "\n",
    "а) Строится пирамида признаков, при этом некоторые карты признаков дополняются  признакими с более ранних слоев (красный пунктир)\n",
    "\n",
    "b) Затем на основе последнего слоя FPN строится еще одна, (Bottom-up) и опять новые  карты признаков дополняются признакими полученными на первом уровне  (зеленый пунктир)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. *Поменялся [процесс обучения](https://blog.roboflow.com/yolov4-data-augmentation/)*. В частности использовались такие техники аугментации как :\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mosaic augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/mosaic_augmentation.jpg\" width=\"900\"></center>\n",
    "\n",
    "<center><p><em>Source: <a href=\"https://medium.com/visionwizard/yolov4-version-3-proposed-workflow-e4fa175b902\"> YOLOv4 — Version 3: Proposed Workflow </a></p></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Изображение, которое подается на вход сети, склеивается из нескольких (4) фрагментов разных изображений. При этом статистика для Batch norm считается по 4-м полным изображениям.\n",
    "Такая стратегия позволяет уменьшить размер batch, что важно при работе с изображениями, имеющими большое разрешение."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Self-Adversarial Training (SAT)\n",
    "\n",
    "Это [Adversarial attack](https://en.wikipedia.org/wiki/Adversarial_machine_learning) непосредственно в процессе обучения. Для каждого batch выполняется 2 forward и 2 backward прохода:\n",
    "*  Выполняется прямой проход и подсчитывается loss\n",
    "*  После первого прохода обновляются изображения а не веса сети\n",
    "*  Далее сеть учится на измененных изображениях\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### YOLOv5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/yolov5.png\" width=\"900\"></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Статья не публиковалась.\n",
    "Точность сравнима  с v4, но модель определенно хорошо упакована и была доступна на [torch.hub](https://pytorch.org/hub/ultralytics_yolov5/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/yolo5_ver.png\" width=\"700\"></center>\n",
    "\n",
    " В репозитории содержиться несколько предобученных моделей. Чем больше параметров у модели тем она точнее и одновременно медленне.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"orange\">YOLOX</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/yolox_speed.png\" width=\"500\"></center>\n",
    "\n",
    "<center><p><em>Source: <a href=\"https://arxiv.org/pdf/2107.08430.pdf\"> YOLOX: Exceeding YOLO Series in 2021</a></p> </em></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Авторы использовали в качестве baseline модели YOLOv3, и, убедившись на ней в эффективности усовершенствований, применили некоторые из них к YOLOv4 и YOLOv5.\n",
    "\n",
    "Далее рассмотрим список нововведений."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Decoupled head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/yolox_decoupled_head.png\" width=\"700\">\n",
    "\n",
    "<center><p><em>Source: <a href=\"https://arxiv.org/pdf/2107.08430.pdf\">YOLOX: Exceeding YOLO Series in 2021\n",
    "</a></p> </em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так же, как в RetinaNet, для регрессии и классификации используются различные головы (подсети)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Anchor-free"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все детекторы, которые рассматривались ранее, использовали несколько предопределенных default box (anchor) для каждой точки на карте признаков.\n",
    "Количество и размер этих якорных окон являются гиперпараметрами модели.\n",
    "\n",
    "В 2019 г вышла статья [FCOS: Fully Convolutional One-Stage](https://towardsdatascience.com/forget-the-hassles-of-anchor-boxes-with-fcos-fully-convolutional-one-stage-object-detection-fc0e25622e1c), где авторы отказываются от такого подхода. Для каждой точки на карте признаков сразу предсказывают один bounding box.\n",
    "\n",
    "Если пиксель, соответствующий центру предсказанного bounding box, попадает в истинный (ground true) bounding box, то он используется при подсчете loss.\n",
    "\n",
    "Это подход был применен и в YOLOX."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/yolox_techniques.png\" width=\"1000\">\n",
    "\n",
    "<center><p><em>Source: <a href=\"https://arxiv.org/pdf/2107.08430.pdf\">YOLOX: Exceeding YOLO Series in 2021\n",
    "</a></p> </em></center>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* and advanced label assigning strategy (SimOTA)\n",
    "* MultiPositives\n",
    "* OTA/SimOTA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### YOLOv6v3\n",
    "\n",
    "\n",
    " Судя по результатам опубликованным в статье, это модель самая точная из семейства YOLO на начало 2023г.\n",
    "\n",
    "В статье описанны следующие модификации архитектуры и техники обучения:\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Усовершениствована PAN подсеть (\"шея\")\n",
    "\n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/yolov6_neck.png\" width=\"1000\"></center>\n",
    "\n",
    "\n",
    "<center><p><em>Source: <a href=\"https://arxiv.org/abs/2301.05586\">YOLOv6 v3.0: A Full-Scale Reloading\n",
    "</a></p> </em></center>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ##### Anchor-Aided Training\n",
    "\n",
    " <center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/yolov6_head.png\" width=\"600\"></center>\n",
    "\n",
    "\n",
    "<center><p><em>Source: <a href=\"https://arxiv.org/abs/2301.05586\">YOLOv6 v3.0: A Full-Scale Reloading\n",
    "</a></p> </em></center>\n",
    "\n",
    " В режиме вывода детектор работает в anchor-free режиме. То есть предсказывает только один boundong box на элемент карты признаков.\n",
    "\n",
    " Но при обучении использовался дополнительный (auxilliary) anchor-based блок, выходы которого же учитывались при подсчете loss. В режиме вывода он не задействуется и соответственно не оказывает влияние на скорость."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Self-distillation\n",
    "\n",
    "При обучении легких моделей использовались тяжелые. \n",
    "\n",
    "При дистилляции к loss полученному при помощи разметки ($L_{det}$) добавляется 2й компонент, который сравнивает выходы большой (teacher) и малой (student) моделей. \n",
    "\n",
    "$ L = L_{det} + \\alpha L_{KD}$ \n",
    "\n",
    "Сравнение выходов (распределений) можно провести при помощи KL дивергенции. \n",
    "\n",
    "$L_{KD} = KL(p_t^{cls} || p_s^{cls} ) + KL(p_t^{reg}||p_s^{reg})$\n",
    "\n",
    "$p_t^{cls}$ - предсказания вероятностей классов учителем\n",
    "\n",
    "$p_s^{cls}$ - предсказания вероятностей классов обучаемой модели (student)\n",
    "\n",
    "$p_t^{reg}$ - предсказания смещений учителем\n",
    "\n",
    "$p_s^{reg}$ - предсказания смещений учеником\n",
    "\n",
    "\n",
    "Подробнее про такой подход можно прочесть в статье: [Generalized Focal Loss: Learning Qualified and Distributed Bounding Boxes for Dense Object Detection](https://paperswithcode.com/paper/generalized-focal-loss-learning-qualified-and)\n",
    "\n",
    "\n",
    "В случае с YOLOv6v3 при дистиляции для второй компоненты loss ($L_{KL}$) использовал дополнительный выход, который не задействуется в режиме вывода.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "К сожалению [документация к API](https://yolov6-docs.readthedocs.io/zh_CN/latest/) модели выложена только на китайском языке."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### YOLOv8\n",
    "\n",
    "По этой модели не публиковалась статья, зато есть [документация](https://docs.ultralytics.com/quickstart/)\n",
    "\n",
    "Попробуем попробуем запустить ее."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ultralytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Инстанцируем модель по названию. Полный [список предобученных моделей](https://github.com/ultralytics/ultralytics#models) ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "yolo8 = YOLO(\"yolov8n.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скачаем тестовое изображение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget https://edunet.kea.su/repo/EduNet-web_dependencies/L01/gentelmens.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим и детектируем на нем объекты.\n",
    "\n",
    "Из коробки работает с изображениями в разных форматах и даже url, автоматически меняет размер входного изображения, возвращает объект с результатами..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# pil_img = Image.open(\"gentelmens.jpg\")\n",
    "results = yolo8(\"gentelmens.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве результата возвращается список объектов которые содержат информацию полную информацию о детектировании.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(results))  # contains detections for one image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У него есть методы для получения списка координат предсказанных bounding box после NMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.ultralytics.com/reference/results/\n",
    "print(results[0].boxes.data)  # x1,y2,x2,y2,conf,class_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Может даже создать картинку с нарисоваными bounding box. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 6))\n",
    "pil_with_bbox = results[0].plot()\n",
    "plt.imshow(pil_with_bbox)  # BGR?\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Похоже объект results хранит картинку в BGR формате, переведем в RGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(pil_with_bbox[..., ::-1])  # BGR->RGB\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
