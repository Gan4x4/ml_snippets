{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Focal Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Focal Loss — это функция потерь, используемая в нейронных сетях для решения проблемы классификации *сложных* объектов. Идея состоит в том, что если мы имеем дело с сильным дисбалансом классов, то модели *просто* верно классифицировать объекты преобладаюшего класса (easy examples), а объекты минорного класса для нее являются *сложными* (hard examples). При этом, в силу дисбаланса, сумма большого количества малых потерь на *простых* объектах может перевешивать сумму малого количества больших потерь на *сложных* объектах, и тем самым модель будет плохо учиться верно классифицировать объекты минорного класса.\n",
    "\n",
    "Focal Loss была предложена в статье [Focal Loss for Dense Object Detection (Lin et al., 2017)](https://arxiv.org/abs/1708.02002) изначально для задачи детектирования объектов на изображениях. Она определяется так:\n",
    "\n",
    "$$\\text{FL}(p_t) = -(1 - p_t)^\\gamma\\text{log}(p_t)$$\n",
    "\n",
    "Здесь $p_t$ — предсказанная вероятность истинного класса, а $\\gamma$ — настраиваемый параметр. Focal Loss уменьшает потери на уверенно классифицируемых примерах (где $p_t>0.5$), и больше фокусируется на сложных примерах, которые классифицированы неправильно. Параметр $\\gamma$ управляет относительной важностью неправильно классифицируемых примеров. Более высокое значение $\\gamma$ увеличивает важность неправильно классифицированных примеров. В экспериментах авторы показали, что параметр $\\gamma=2$ показывал себя наилучшим образом в их задаче.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L11/focal_loss_vs_ce.png\" width=\"700\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://arxiv.org/abs/1708.02002\">Focal Loss for Dense Object Detection (Lin et al., 2018)</a></em></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При $\\gamma=0$ Focal Loss становится равной Cross-Entropy Loss, которая выражается как обратный логарифм вероятности истинного класса:\n",
    "\n",
    "$$\\text{CE}(p_t)=-\\text{log}(p_t)$$\n",
    "\n",
    "Фактически, потери для уверенно классифицированных объектов дополнительно занижаются. Это похоже на взвешивание при дисбалансе классов.\n",
    "\n",
    "Достигается этот эффект путем домножения на коэффициент: $ (1-p_{t})^\\gamma$\n",
    "\n",
    "Пока модель ошибается, $p_{t}$ — мала, и значение выражения в скобках соответственно близко к 1.\n",
    "\n",
    "Когда модель обучилась, значение $p_{t}$ становится близким к 1, а разность в скобках становится маленьким числом, которое возводится в степень $ \\gamma \\ge 0 $. Таким образом, домножение на это небольшое число нивелирует вклад верно классифицированных объектов.\n",
    "\n",
    "Это позволяет модели сосредоточиться (сфокусироваться, отсюда и название) на изучении сложных объектов (hard examples).\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разберем на примере. Пусть мы имеем дело с задачей бинарной классификации, где модель должна отличать яблоки и груши. Пусть набор данных несбалансирован: на 20 яблок приходится одна груша. Модель может хорошо обучиться классифицировать яблоки: вероятность истинного класса велика и равна $0.9$ для каждого яблока. При этом модель не научилась хорошо классифицировать груши: вероятность истинного класса для груши мала и равна $0.2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://edunet.kea.su/repo/EduNet-content/L11/out/unbalanced_apples_pear.png' width=600></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\large{CE = \\overbrace{\\sum^{20}-\\text{log}(0.9)}^{\\large\\color{#3C8031}{\\text{loss(apples)=2.11}}} + \\overbrace{(-\\text{log}(0.2))}^{\\large\\color{#F26035}{\\text{loss(pear)=1.61}}} \\approx 3.72}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\large{FL(\\gamma=2) = \\overbrace{\\sum^{20}-\\color{#AF3235}{\\underbrace{(1-0.9)^2}_{0.01}}\\text{log}(0.9)}^{\\large\\color{#3C8031}{\\text{loss(apples)=0.02}}} + \\overbrace{(-\\color{#AF3235}{\\underbrace{(1-0.2)^2}_{0.64}}\\text{log}(0.2))}^{\\large\\color{#F26035}{\\text{loss(pear)=1.03}}} \\approx 1.05}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В случае Focal Loss коэффициент $(1-p_t)^\\gamma$ в 100 раз занизил потери при уверенной классификации яблок и потери при неверной классификации груши стали преобладать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте посчитаем для различных значений $γ$, сколько понадобится примеров с небольшой ошибкой (высокой вероятностью истинного класса, равной $0.9$), чтобы получить суммарный **Focal Loss** примерно такой же, как у одного примера с большой ошибкой (низкой вероятностью истинного класса, равной $0.2$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cross_entropy(prob_true):\n",
    "    return -np.log(prob_true)\n",
    "\n",
    "\n",
    "def focal_loss(prob_true, gamma=2):\n",
    "    return (1 - prob_true) ** gamma * cross_entropy(prob_true)\n",
    "\n",
    "\n",
    "p1 = 0.9  # probability of easy examples predictions\n",
    "p2 = 0.2  # probability of hard examples predictions\n",
    "gammas = [0, 0.5, 1, 2, 5, 10, 15]\n",
    "\n",
    "print(\n",
    "    f\"For probability of easy examples predictions {p1} and probability of hard examples predictions {p2}\\n\"\n",
    ")\n",
    "\n",
    "for gamma in gammas:\n",
    "    fl1 = focal_loss(p1, gamma)\n",
    "    fl2 = focal_loss(p2, gamma)\n",
    "\n",
    "    print(\n",
    "        f\"gamma = {gamma},\".ljust(15),\n",
    "        f\"for an equal loss with a problematic prediction, almost correct ones are required {int(fl2 / fl1)}\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно, при увеличении значения $\\gamma$ можно достичь значительного роста \"важности\" примеров с высокой ошибкой, что, по сути, позволяет модели обращать внимание на \"hard examples\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В Focal Loss также могут быть добавлены веса для классов. Тогда формула будет выглядеть так:\n",
    "\n",
    "$$\\text{FL}(p_t) = -\\alpha_t(1 - p_t)^\\gamma\\text{log}(p_t)$$\n",
    "\n",
    "Здесь $\\alpha_t$ — вес для истинного класса, имеющий такой же смысл, как параметр `weight` в Cross-Entropy Loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Focal Loss не реализована в PyTorch нативно, но существуют сторонние совместимые реализации. Посмотрим, как воспользоваться [одной из них](https://github.com/AdeelH/pytorch-multi-class-focal-loss)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def set_random_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "!wget https://raw.githubusercontent.com/AdeelH/pytorch-multi-class-focal-loss/master/focal_loss.py\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from focal_loss import FocalLoss\n",
    "\n",
    "\n",
    "criterion = FocalLoss(alpha=None, gamma=2.)\n",
    "\n",
    "model_output = torch.rand(3, 3)  # model output is logits, as in CELoss\n",
    "print(f\"model_output:\\n {model_output}\")\n",
    "\n",
    "target = torch.empty(3, dtype=torch.long).random_(3)\n",
    "print(f\"target: {target}\")\n",
    "\n",
    "loss_fl = criterion(model_output, target)\n",
    "print(f\"loss_fl: {loss_fl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Убедимся, что сторонняя реализация вычисляет то, что нужно. Во-первых, переведем `model_output` из логитов в вероятности с помощью softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = torch.nn.functional.softmax(model_output, dim=1)\n",
    "\n",
    "print(f\"probabilities after softmax:\\n {probs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(prob_true):\n",
    "    return -np.log(prob_true)\n",
    "\n",
    "\n",
    "def focal_loss(prob_true, gamma=2):\n",
    "    return (1 - prob_true) ** gamma * cross_entropy(prob_true)\n",
    "\n",
    "hand_calculated_loss = 0\n",
    "\n",
    "for i in range(3):\n",
    "    hand_calculated_loss += focal_loss(probs[i, target[i]])\n",
    "\n",
    "hand_calculated_loss /= 3  # average by number of samples\n",
    "print(f\"hand-calculated focal loss: {hand_calculated_loss.item()}\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
