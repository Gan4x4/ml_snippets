{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOFJP7Q967nu2kx8stW/GeB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"ObOJr4ZA5Nsh"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"wU3OPAisQfrp"},"source":["#### Focal Loss"]},{"cell_type":"markdown","source":["Focal Loss — это функция потерь, используемая в нейронных сетях для решения проблемы классификации *сложных* объектов. Идея состоит в том, что если мы имеем дело с сильным дисбалансом классов, то модели *просто* верно классифицировать объекты преобладаюшего класса (easy examples), а объекты минорного класса для нее являются *сложными* (hard examples). При этом, в силу дисбаланса, сумма большого количества малых потерь на *простых* объектах может перевешивать сумму малого количества больших потерь на *сложных* объектах, и тем самым модель будет плохо учиться верно классифицировать объекты минорного класса.\n","\n","Focal Loss была предложена в статье [Focal Loss for Dense Object Detection (Lin et al., 2017)](https://arxiv.org/abs/1708.02002) изначально для задачи детектирования объектов на изображениях. Она определяется так:\n","\n","$$\\text{FL}(p_t) = -(1 - p_t)^\\gamma\\text{log}(p_t)$$\n","\n","Здесь $p_t$ — предсказанная вероятность истинного класса, а $\\gamma$ — настраиваемый параметр. Focal Loss уменьшает потери на уверенно классифицируемых примерах (где $p_t>0.5$), и больше фокусируется на сложных примерах, которые классифицированы неправильно. Параметр $\\gamma$ управляет относительной важностью неправильно классифицируемых примеров. Более высокое значение $\\gamma$ увеличивает важность неправильно классифицированных примеров. В экспериментах авторы показали, что параметр $\\gamma=2$ показывал себя наилучшим образом в их задаче.\n"],"metadata":{"id":"0PJXwvgk_dDM"}},{"cell_type":"markdown","metadata":{"id":"xjx7bjfpQfrp"},"source":["<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L11/focal_loss_vs_ce.png\" width=\"700\"></center>\n","\n","<center><em>Source: <a href=\"https://arxiv.org/abs/1708.02002\">Focal Loss for Dense Object Detection (Lin et al., 2018)</a></em></center>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"WtazhKsGQfrp"},"source":["При $\\gamma=0$ Focal Loss становится равной Cross-Entropy Loss, которая выражается как обратный логарифм вероятности истинного класса:\n","\n","$$\\text{CE}(p_t)=-\\text{log}(p_t)$$\n","\n","Фактически, потери для уверенно классифицированных объектов дополнительно занижаются. Это похоже на взвешивание при дисбалансе классов.\n","\n","Достигается этот эффект путем домножения на коэффициент: $ (1-p_{t})^\\gamma$\n","\n","Пока модель ошибается, $p_{t}$ — мала, и значение выражения в скобках соответственно близко к 1.\n","\n","Когда модель обучилась, значение $p_{t}$ становится близким к 1, а разность в скобках становится маленьким числом, которое возводится в степень $ \\gamma \\ge 0 $. Таким образом, домножение на это небольшое число нивелирует вклад верно классифицированных объектов.\n","\n","Это позволяет модели сосредоточиться (сфокусироваться, отсюда и название) на изучении сложных объектов (hard examples).\n","\n","\n","\n"]},{"cell_type":"markdown","source":["Разберем на примере. Пусть мы имеем дело с задачей бинарной классификации, где модель должна отличать яблоки и груши. Пусть набор данных несбалансирован: на 20 яблок приходится одна груша. Модель может хорошо обучиться классифицировать яблоки: вероятность истинного класса велика и равна $0.9$ для каждого яблока. При этом модель не научилась хорошо классифицировать груши: вероятность истинного класса для груши мала и равна $0.2$."],"metadata":{"id":"tFWb0fPS74YR"}},{"cell_type":"markdown","source":["<img src='https://edunet.kea.su/repo/EduNet-content/L11/out/unbalanced_apples_pear.png' width=600></img>"],"metadata":{"id":"TjWavUrD6yBu"}},{"cell_type":"markdown","source":["$\\large{CE = \\overbrace{\\sum^{20}-\\text{log}(0.9)}^{\\large\\color{#3C8031}{\\text{loss(apples)=2.11}}} + \\overbrace{(-\\text{log}(0.2))}^{\\large\\color{#F26035}{\\text{loss(pear)=1.61}}} \\approx 3.72}$"],"metadata":{"id":"oDDsWMFLpft1"}},{"cell_type":"markdown","source":["$\\large{FL(\\gamma=2) = \\overbrace{\\sum^{20}-\\color{#AF3235}{\\underbrace{(1-0.9)^2}_{0.01}}\\text{log}(0.9)}^{\\large\\color{#3C8031}{\\text{loss(apples)=0.02}}} + \\overbrace{(-\\color{#AF3235}{\\underbrace{(1-0.2)^2}_{0.64}}\\text{log}(0.2))}^{\\large\\color{#F26035}{\\text{loss(pear)=1.03}}} \\approx 1.05}$"],"metadata":{"id":"ZQoSA6VHtrS-"}},{"cell_type":"markdown","source":["В случае Focal Loss коэффициент $(1-p_t)^\\gamma$ в 100 раз занизил потери при уверенной классификации яблок и потери при неверной классификации груши стали преобладать."],"metadata":{"id":"tvLqd59N_hjk"}},{"cell_type":"markdown","metadata":{"id":"v9BjAPqkRVVP"},"source":["Давайте посчитаем для различных значений $γ$, сколько понадобится примеров с небольшой ошибкой (высокой вероятностью истинного класса, равной $0.9$), чтобы получить суммарный **Focal Loss** примерно такой же, как у одного примера с большой ошибкой (низкой вероятностью истинного класса, равной $0.2$)."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cMp3Ly81RVVP","executionInfo":{"status":"ok","timestamp":1683208640350,"user_tz":-180,"elapsed":18,"user":{"displayName":"Сергей Колпинский","userId":"04950950781799274860"}},"outputId":"de0d942d-f287-40ff-ab74-6d076af9ce6d"},"outputs":[{"output_type":"stream","name":"stdout","text":["For probability of easy examples predictions 0.9 and probability of hard examples predictions 0.2\n","\n","gamma = 0,      for an equal loss with a problematic prediction, almost correct ones are required 15\n","gamma = 0.5,    for an equal loss with a problematic prediction, almost correct ones are required 43\n","gamma = 1,      for an equal loss with a problematic prediction, almost correct ones are required 122\n","gamma = 2,      for an equal loss with a problematic prediction, almost correct ones are required 977\n","gamma = 5,      for an equal loss with a problematic prediction, almost correct ones are required 500548\n","gamma = 10,     for an equal loss with a problematic prediction, almost correct ones are required 16401977428\n","gamma = 15,     for an equal loss with a problematic prediction, almost correct ones are required 537459996388583\n"]}],"source":["import numpy as np\n","\n","def cross_entropy(prob_true):\n","    return -np.log(prob_true)\n","\n","\n","def focal_loss(prob_true, gamma=2):\n","    return (1 - prob_true) ** gamma * cross_entropy(prob_true)\n","\n","\n","p1 = 0.9  # probability of easy examples predictions\n","p2 = 0.2  # probability of hard examples predictions\n","gammas = [0, 0.5, 1, 2, 5, 10, 15]\n","\n","print(\n","    f\"For probability of easy examples predictions {p1} and probability of hard examples predictions {p2}\\n\"\n",")\n","\n","for gamma in gammas:\n","    fl1 = focal_loss(p1, gamma)\n","    fl2 = focal_loss(p2, gamma)\n","\n","    print(\n","        f\"gamma = {gamma},\".ljust(15),\n","        f\"for an equal loss with a problematic prediction, almost correct ones are required {int(fl2 / fl1)}\",\n","    )"]},{"cell_type":"markdown","metadata":{"id":"rYabilxIRVVQ"},"source":["Как видно, при увеличении значения $\\gamma$ можно достичь значительного роста \"важности\" примеров с высокой ошибкой, что, по сути, позволяет модели обращать внимание на \"hard examples\"."]},{"cell_type":"markdown","source":["В Focal Loss также могут быть добавлены веса для классов. Тогда формула будет выглядеть так:\n","\n","$$\\text{FL}(p_t) = -\\alpha_t(1 - p_t)^\\gamma\\text{log}(p_t)$$\n","\n","Здесь $\\alpha_t$ — вес для истинного класса, имеющий такой же смысл, как параметр `weight` в Cross-Entropy Loss."],"metadata":{"id":"pFi4XMxON24W"}},{"cell_type":"markdown","source":["Focal Loss не реализована в PyTorch нативно, но существуют сторонние совместимые реализации. Посмотрим, как воспользоваться [одной из них](https://github.com/AdeelH/pytorch-multi-class-focal-loss)."],"metadata":{"id":"oi424qAwHFl5"}},{"cell_type":"code","source":["import random\n","\n","def set_random_seed(seed):\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    np.random.seed(seed)\n","    random.seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","\n","\n","set_random_seed(42)"],"metadata":{"id":"ODsx1BG8TtGQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from IPython.display import clear_output\n","\n","!wget https://raw.githubusercontent.com/AdeelH/pytorch-multi-class-focal-loss/master/focal_loss.py\n","clear_output()"],"metadata":{"id":"RryeDBdVHrWM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","from torch import nn\n","from focal_loss import FocalLoss\n","\n","\n","criterion = FocalLoss(alpha=None, gamma=2.)\n","\n","model_output = torch.rand(3, 3)  # model output is logits, as in CELoss\n","print(f\"model_output:\\n {model_output}\")\n","\n","target = torch.empty(3, dtype=torch.long).random_(3)\n","print(f\"target: {target}\")\n","\n","loss_fl = criterion(model_output, target)\n","print(f\"loss_fl: {loss_fl}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jec1JP6jHwlY","executionInfo":{"status":"ok","timestamp":1683208640835,"user_tz":-180,"elapsed":491,"user":{"displayName":"Сергей Колпинский","userId":"04950950781799274860"}},"outputId":"e46af443-d391-406a-f25e-955828059e8a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["model_output:\n"," tensor([[0.8823, 0.9150, 0.3829],\n","        [0.9593, 0.3904, 0.6009],\n","        [0.2566, 0.7936, 0.9408]])\n","target: tensor([2, 1, 1])\n","loss_fl: 0.6864498257637024\n"]}]},{"cell_type":"markdown","source":["Убедимся, что сторонняя реализация вычисляет то, что нужно. Во-первых, переведем `model_output` из логитов в вероятности с помощью softmax."],"metadata":{"id":"lwvX0UMzXACU"}},{"cell_type":"code","source":["probs = torch.nn.functional.softmax(model_output, dim=1)\n","\n","print(f\"probabilities after softmax:\\n {probs}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N2jW6iK2LrvF","executionInfo":{"status":"ok","timestamp":1683208640836,"user_tz":-180,"elapsed":10,"user":{"displayName":"Сергей Колпинский","userId":"04950950781799274860"}},"outputId":"203b18e8-49c4-4d65-977e-2b19678043e5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["probabilities after softmax:\n"," tensor([[0.3788, 0.3914, 0.2299],\n","        [0.4415, 0.2500, 0.3085],\n","        [0.2131, 0.3646, 0.4224]])\n"]}]},{"cell_type":"code","source":["def cross_entropy(prob_true):\n","    return -np.log(prob_true)\n","\n","\n","def focal_loss(prob_true, gamma=2):\n","    return (1 - prob_true) ** gamma * cross_entropy(prob_true)\n","\n","hand_calculated_loss = 0\n","\n","for i in range(3):\n","    hand_calculated_loss += focal_loss(probs[i, target[i]])\n","\n","hand_calculated_loss /= 3  # average by number of samples\n","print(f\"hand-calculated focal loss: {hand_calculated_loss.item()}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Te0ltp87L-XK","executionInfo":{"status":"ok","timestamp":1683208640836,"user_tz":-180,"elapsed":6,"user":{"displayName":"Сергей Колпинский","userId":"04950950781799274860"}},"outputId":"52e20178-189a-4861-a533-d3d1a843613b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["hand-calculated focal loss: 0.6864497661590576\n"]}]}]}